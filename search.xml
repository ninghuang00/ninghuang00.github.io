<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Java开发知识框架]]></title>
    <url>%2F2118%2F07%2F07%2FJava%E5%BC%80%E5%8F%91%E7%9F%A5%E8%AF%86%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[Java开发知识框架 阿里面试官(qingfeng82)的建议: 识别和定义问题的能力 解决问题的能力 思辨能力 学习能力 体系化的思考和总结能力 自我认知能力 认知扩展的能力 基础知识Java基础知识 java基础 +++ 面向对象 集合 异常 IO/NIO Jdk8特性 +++ lambda表达式 函数式编程 stream FP 多线程 +++ ThreadPoolExecutor(线程池) J.U.C Atomic* fork/join 线程的几种状态 设计模式设计模式 单例 工厂 代理 装饰 Servlet容器(部署配置使用) ++ tomcat jetty JVM相关Java虚拟机 类加载过程 class编译 加载 执行原理过程 内存管理 垃圾回收(GC) 调优技巧 ++ 实践积累 开源框架开源框架 Spring +++ 依赖注入 控制反转 代理 事务管理 Bean实例化 MyBatis +++ SQL MAPS 数据关联 动态映射 事务管理 数据库数据库 sql基础语法 group by 索引 加速原理 使用原则 读写分离 概念 脚本及Linux知识脚本及Linux知识 常用shell命令 +++ ps top curl scp ssh netstat vim快捷键 计算机网络计算机网络 http tcp netty(http权威指南 说是神书) servlet4.0,jdk9,http2了解一下 开发工具及环境部署开发工具及环境部署 集成开发环境(IDE) IDEA Eclipse 编译工具(build tool) Maven Gradle 版本控制工具(VCS) git 其他脚本工具 模板引擎(juicer-一个javascript模板引擎的实现和优化) 项目实践积累项目实践积累 网上书店 天气预报APP 艺术学校管理系统 基于selenium的自动化测试平台 数据交换平台 基本知识框架]]></content>
      <categories>
        <category>大纲</category>
      </categories>
      <tags>
        <tag>目录</tag>
        <tag>知识框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内存泄漏]]></title>
    <url>%2F2018%2F09%2F26%2F%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%2F</url>
    <content type="text"><![CDATA[这是摘要 内存泄漏指的就是不会再被使用的对象的内存却不能被回收 可能出现内存泄漏的情况 长生命周期的对象持有短生命周期对象的引用12345678public class Simple &#123; Object object; public void method1()&#123; object = new Object(); //...其他代码 //object = null; &#125;&#125; 比如说这种情况就可能会出现内存泄漏,如果object对象只在method1()方法中使用,其实应该写成局部变量,或者在方法结束的地方释放掉. 移动数组中的指针来实现移除元素,但是没有解除引用 123456public E pop()&#123; if(size == 0) return null; else return (E) elementData[--size];//此时elementData[size]元素还保持引用,无法被回收&#125; 可以调用close()方法关闭资源的类 123456try&#123; session=sessionFactory.openSession();/ /...其他操作&#125;finally&#123; session.close();//如果不close的话,session对象只有在sessionFactory对象被回收,才能被回收&#125; 静态集合类引起 123456static Vector v = new Vector(10); for (int i = 1; i&lt;100; i++) &#123; Object o = new Object(); v.add(o); o = null; //此时v中的数组依然持有对象的引用&#125; 集合中元素的属性被修改,如果重写了hashCode()方法,可能会引起泄漏 12345678910111213public static void main(String[] args) &#123; Set&lt;Person&gt; set = new HashSet&lt;Person&gt;(); Person p1 = new Person("唐僧","pwd1",25); Person p2 = new Person("孙悟空","pwd2",26); Person p3 = new Person("猪八戒","pwd3",27); set.add(p1); set.add(p2); set.add(p3); p3.setAge(2); //修改p3的年龄,此时p3元素对应的hashcode值发生改变 set.remove(p3); //此时remove不掉，造成内存泄漏 set.add(p3); //重新添加，居然添加成功 &#125; 内存泄漏的排查方法 主要工具Optimizeit Profiler，JProbe Profiler，JinSight , Rational 公司的Purify等 方法在运行过程中，我们可以随时观察内存的使用情况，通过这种方式，我们可以很快找到那些长期不被释放，并且不再使用的对象。我们通过检查这些对象的生存周期，确认其是否为内存泄露。 总结在实践当中，寻找内存泄露是一件非常麻烦的事情，它需要程序员对整个程序的代码比较清楚，并且需要丰富的调试经验，但是这个过程对于很多关键的Java程序都是十分重要的。 实践案例 案例一 参考地址:https://my.oschina.net/jenwang/blog/833207 步骤 使用jstat -gcutil pid time查看gc状态(pid:线程号,time:多少时间打印一次) 可以观察到fullgc频繁 使用jmap -histo:live pid查看内存使用情况,打印存活对象状态 可以看出HashTable中的元素有5000多万，占用内存大约1.5G的样子 定位代码接下来自然要看看是什么代码往HashTable里疯狂的put数据，于是用神器btrace跟踪Hashtable.put调用的堆栈。编写btrace脚本TracingHashTable.java,然后运行bin/btrace -cp build 4947 TracingHashTable.java123456789101112131415import com.sun.btrace.annotations.*;import static com.sun.btrace.BTraceUtils.*;@BTracepublic class TracingHashTable &#123; /*指明要查看的方法，类*/ @OnMethod( clazz="java.util.Hashtable", method="put", location=@Location(Kind.RETURN)) public static void traceExecute(@Self java.util.Hashtable object)&#123; println("调用堆栈！！"); jstack(); &#125;&#125; 可以看出是在接收到消息后查询入库的代码造成的，业务方法调用ibatis再到mysql jdbc驱动执行statement时put了大量的属性到HashTable中。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>内存泄漏</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker入门]]></title>
    <url>%2F2018%2F09%2F25%2Fdocker%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[这是摘要 docker和虚拟机的区别 参考地址:https://www.cnblogs.com/pangguoping/articles/5515286.html]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis默认配置文件]]></title>
    <url>%2F2018%2F09%2F19%2Fredis%E9%BB%98%E8%AE%A4%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[这是摘要 redis默认配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413513613713813914014114214314414514614714814915015115215315415515615715815916016116216316416516616716816917017117217317417517617717817918018118218318418518618718818919019119219319419519619719819920020120220320420520620720820921021121221321421521621721821922022122222322422522622722822923023123223323423523623723823924024124224324424524624724824925025125225325425525625725825926026126226326426526626726826927027127227327427527627727827928028128228328428528628728828929029129229329429529629729829930030130230330430530630730830931031131231331431531631731831932032132232332432532632732832933033133233333433533633733833934034134234334434534634734834935035135235335435535635735835936036136236336436536636736836937037137237337437537637737837938038138238338438538638738838939039139239339439539639739839940040140240340440540640740840941041141241341441541641741841942042142242342442542642742842943043143243343443543643743843944044144244344444544644744844945045145245345445545645745845946046146246346446546646746846947047147247347447547647747847948048148248348448548648748848949049149249349449549649749849950050150250350450550650750850951051151251351451551651751851952052152252352452552652752852953053153253353453553653753853954054154254354454554654754854955055155255355455555655755855956056156256356456556656756856957057157257357457557657757857958058158258358458558658758858959059159259359459559659759859960060160260360460560660760860961061161261361461561661761861962062162262362462562662762862963063163263363463563663763863964064164264364464564664764864965065165265365465565665765865966066166266366466566666766866967067167267367467567667767867968068168268368468568668768868969069169269369469569669769869970070170270370470570670770870971071171271371471571671771871972072172272372472572672772872973073173273373473573673773873974074174274374474574674774874975075175275375475575675775875976076176276376476576676776876977077177277377477577677777877978078178278378478578678778878979079179279379479579679779879980080180280380480580680780880981081181281381481581681781881982082182282382482582682782882983083183283383483583683783883984084184284384484584684784884985085185285385485585685785885986086186286386486586686786886987087187287387487587687787887988088188288388488588688788888989089189289389489589689789889990090190290390490590690790890991091191291391491591691791891992092192292392492592692792892993093193293393493593693793893994094194294394494594694794894995095195295395495595695795895996096196296396496596696796896997097197297397497597697797897998098198298398498598698798898999099199299399499599699799899910001001100210031004100510061007100810091010101110121013101410151016101710181019102010211022102310241025102610271028102910301031103210331034103510361037103810391040104110421043104410451046104710481049105010511052105310541055105610571058105910601061106210631064106510661067106810691070107110721073107410751076107710781079108010811082108310841085108610871088108910901091109210931094109510961097109810991100110111021103110411051106110711081109111011111112111311141115111611171118111911201121112211231124112511261127112811291130113111321133113411351136113711381139114011411142114311441145114611471148114911501151115211531154115511561157115811591160116111621163116411651166116711681169117011711172117311741175117611771178117911801181118211831184118511861187118811891190119111921193119411951196119711981199120012011202120312041205120612071208120912101211121212131214121512161217121812191220122112221223122412251226122712281229123012311232123312341235123612371238123912401241124212431244124512461247124812491250125112521253125412551256125712581259126012611262126312641265126612671268126912701271127212731274127512761277127812791280128112821283128412851286128712881289129012911292129312941295129612971298129913001301130213031304130513061307130813091310131113121313131413151316# Redis configuration file example.## Note that in order to read the configuration file, Redis must be# started with the file path as first argument:## ./redis-server /path/to/redis.conf# Note on units: when memory size is needed, it is possible to specify# it in the usual form of 1k 5GB 4M and so forth:## 1k =&gt; 1000 bytes# 1kb =&gt; 1024 bytes# 1m =&gt; 1000000 bytes# 1mb =&gt; 1024*1024 bytes# 1g =&gt; 1000000000 bytes# 1gb =&gt; 1024*1024*1024 bytes## units are case insensitive so 1GB 1Gb 1gB are all the same.################################## INCLUDES #################################### Include one or more other config files here. This is useful if you# have a standard template that goes to all Redis servers but also need# to customize a few per-server settings. Include files can include# other files, so use this wisely.## Notice option "include" won't be rewritten by command "CONFIG REWRITE"# from admin or Redis Sentinel. Since Redis always uses the last processed# line as value of a configuration directive, you'd better put includes# at the beginning of this file to avoid overwriting config change at runtime.## If instead you are interested in using includes to override configuration# options, it is better to use include as the last line.## include /path/to/local.conf# include /path/to/other.conf################################## MODULES ###################################### Load modules at startup. If the server is not able to load modules# it will abort. It is possible to use multiple loadmodule directives.## loadmodule /path/to/my_module.so# loadmodule /path/to/other_module.so################################## NETWORK ###################################### By default, if no "bind" configuration directive is specified, Redis listens# for connections from all the network interfaces available on the server.# It is possible to listen to just one or multiple selected interfaces using# the "bind" configuration directive, followed by one or more IP addresses.## Examples:## bind 192.168.1.100 10.0.0.1# bind 127.0.0.1 ::1## ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the# internet, binding to all the interfaces is dangerous and will expose the# instance to everybody on the internet. So by default we uncomment the# following bind directive, that will force Redis to listen only into# the IPv4 lookback interface address (this means Redis will be able to# accept connections only from clients running into the same computer it# is running).## IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES# JUST COMMENT THE FOLLOWING LINE.# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bind 127.0.0.1# Protected mode is a layer of security protection, in order to avoid that# Redis instances left open on the internet are accessed and exploited.## When protected mode is on and if:## 1) The server is not binding explicitly to a set of addresses using the# "bind" directive.# 2) No password is configured.## The server only accepts connections from clients connecting from the# IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain# sockets.## By default protected mode is enabled. You should disable it only if# you are sure you want clients from other hosts to connect to Redis# even if no authentication is configured, nor a specific set of interfaces# are explicitly listed using the "bind" directive.protected-mode yes# Accept connections on the specified port, default is 6379 (IANA #815344).# If port 0 is specified Redis will not listen on a TCP socket.port 6379# TCP listen() backlog.## In high requests-per-second environments you need an high backlog in order# to avoid slow clients connections issues. Note that the Linux kernel# will silently truncate it to the value of /proc/sys/net/core/somaxconn so# make sure to raise both the value of somaxconn and tcp_max_syn_backlog# in order to get the desired effect.tcp-backlog 511# Unix socket.## Specify the path for the Unix socket that will be used to listen for# incoming connections. There is no default, so Redis will not listen# on a unix socket when not specified.## unixsocket /tmp/redis.sock# unixsocketperm 700# Close the connection after a client is idle for N seconds (0 to disable)timeout 0# TCP keepalive.## If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence# of communication. This is useful for two reasons:## 1) Detect dead peers.# 2) Take the connection alive from the point of view of network# equipment in the middle.## On Linux, the specified value (in seconds) is the period used to send ACKs.# Note that to close the connection the double of the time is needed.# On other kernels the period depends on the kernel configuration.## A reasonable value for this option is 300 seconds, which is the new# Redis default starting with Redis 3.2.1.tcp-keepalive 300################################# GENERAL ###################################### By default Redis does not run as a daemon. Use 'yes' if you need it.# Note that Redis will write a pid file in /var/run/redis.pid when daemonized.daemonize no# If you run Redis from upstart or systemd, Redis can interact with your# supervision tree. Options:# supervised no - no supervision interaction# supervised upstart - signal upstart by putting Redis into SIGSTOP mode# supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET# supervised auto - detect upstart or systemd method based on# UPSTART_JOB or NOTIFY_SOCKET environment variables# Note: these supervision methods only signal "process is ready."# They do not enable continuous liveness pings back to your supervisor.supervised no# If a pid file is specified, Redis writes it where specified at startup# and removes it at exit.## When the server runs non daemonized, no pid file is created if none is# specified in the configuration. When the server is daemonized, the pid file# is used even if not specified, defaulting to "/var/run/redis.pid".## Creating a pid file is best effort: if Redis is not able to create it# nothing bad happens, the server will start and run normally.pidfile /var/run/redis_6379.pid# Specify the server verbosity level.# This can be one of:# debug (a lot of information, useful for development/testing)# verbose (many rarely useful info, but not a mess like the debug level)# notice (moderately verbose, what you want in production probably)# warning (only very important / critical messages are logged)loglevel notice# Specify the log file name. Also the empty string can be used to force# Redis to log on the standard output. Note that if you use standard# output for logging but daemonize, logs will be sent to /dev/nulllogfile ""# To enable logging to the system logger, just set 'syslog-enabled' to yes,# and optionally update the other syslog parameters to suit your needs.# syslog-enabled no# Specify the syslog identity.# syslog-ident redis# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.# syslog-facility local0# Set the number of databases. The default database is DB 0, you can select# a different one on a per-connection basis using SELECT &lt;dbid&gt; where# dbid is a number between 0 and 'databases'-1databases 16# By default Redis shows an ASCII art logo only when started to log to the# standard output and if the standard output is a TTY. Basically this means# that normally a logo is displayed only in interactive sessions.## However it is possible to force the pre-4.0 behavior and always show a# ASCII art logo in startup logs by setting the following option to yes.always-show-logo yes################################ SNAPSHOTTING ################################## Save the DB on disk:## save &lt;seconds&gt; &lt;changes&gt;## Will save the DB if both the given number of seconds and the given# number of write operations against the DB occurred.## In the example below the behaviour will be to save:# after 900 sec (15 min) if at least 1 key changed# after 300 sec (5 min) if at least 10 keys changed# after 60 sec if at least 10000 keys changed## Note: you can disable saving completely by commenting out all "save" lines.## It is also possible to remove all the previously configured save# points by adding a save directive with a single empty string argument# like in the following example:## save ""save 900 1save 300 10save 60 10000# By default Redis will stop accepting writes if RDB snapshots are enabled# (at least one save point) and the latest background save failed.# This will make the user aware (in a hard way) that data is not persisting# on disk properly, otherwise chances are that no one will notice and some# disaster will happen.## If the background saving process will start working again Redis will# automatically allow writes again.## However if you have setup your proper monitoring of the Redis server# and persistence, you may want to disable this feature so that Redis will# continue to work as usual even if there are problems with disk,# permissions, and so forth.stop-writes-on-bgsave-error yes# Compress string objects using LZF when dump .rdb databases?# For default that's set to 'yes' as it's almost always a win.# If you want to save some CPU in the saving child set it to 'no' but# the dataset will likely be bigger if you have compressible values or keys.rdbcompression yes# Since version 5 of RDB a CRC64 checksum is placed at the end of the file.# This makes the format more resistant to corruption but there is a performance# hit to pay (around 10%) when saving and loading RDB files, so you can disable it# for maximum performances.## RDB files created with checksum disabled have a checksum of zero that will# tell the loading code to skip the check.rdbchecksum yes# The filename where to dump the DBdbfilename dump.rdb# The working directory.## The DB will be written inside this directory, with the filename specified# above using the 'dbfilename' configuration directive.## The Append Only File will also be created inside this directory.## Note that you must specify a directory here, not a file name.dir ./################################# REPLICATION ################################## Master-Slave replication. Use slaveof to make a Redis instance a copy of# another Redis server. A few things to understand ASAP about Redis replication.## 1) Redis replication is asynchronous, but you can configure a master to# stop accepting writes if it appears to be not connected with at least# a given number of slaves.# 2) Redis slaves are able to perform a partial resynchronization with the# master if the replication link is lost for a relatively small amount of# time. You may want to configure the replication backlog size (see the next# sections of this file) with a sensible value depending on your needs.# 3) Replication is automatic and does not need user intervention. After a# network partition slaves automatically try to reconnect to masters# and resynchronize with them.## slaveof &lt;masterip&gt; &lt;masterport&gt;# If the master is password protected (using the "requirepass" configuration# directive below) it is possible to tell the slave to authenticate before# starting the replication synchronization process, otherwise the master will# refuse the slave request.## masterauth &lt;master-password&gt;# When a slave loses its connection with the master, or when the replication# is still in progress, the slave can act in two different ways:## 1) if slave-serve-stale-data is set to 'yes' (the default) the slave will# still reply to client requests, possibly with out of date data, or the# data set may just be empty if this is the first synchronization.## 2) if slave-serve-stale-data is set to 'no' the slave will reply with# an error "SYNC with master in progress" to all the kind of commands# but to INFO and SLAVEOF.#slave-serve-stale-data yes# You can configure a slave instance to accept writes or not. Writing against# a slave instance may be useful to store some ephemeral data (because data# written on a slave will be easily deleted after resync with the master) but# may also cause problems if clients are writing to it because of a# misconfiguration.## Since Redis 2.6 by default slaves are read-only.## Note: read only slaves are not designed to be exposed to untrusted clients# on the internet. It's just a protection layer against misuse of the instance.# Still a read only slave exports by default all the administrative commands# such as CONFIG, DEBUG, and so forth. To a limited extent you can improve# security of read only slaves using 'rename-command' to shadow all the# administrative / dangerous commands.slave-read-only yes# Replication SYNC strategy: disk or socket.## -------------------------------------------------------# WARNING: DISKLESS REPLICATION IS EXPERIMENTAL CURRENTLY# -------------------------------------------------------## New slaves and reconnecting slaves that are not able to continue the replication# process just receiving differences, need to do what is called a "full# synchronization". An RDB file is transmitted from the master to the slaves.# The transmission can happen in two different ways:## 1) Disk-backed: The Redis master creates a new process that writes the RDB# file on disk. Later the file is transferred by the parent# process to the slaves incrementally.# 2) Diskless: The Redis master creates a new process that directly writes the# RDB file to slave sockets, without touching the disk at all.## With disk-backed replication, while the RDB file is generated, more slaves# can be queued and served with the RDB file as soon as the current child producing# the RDB file finishes its work. With diskless replication instead once# the transfer starts, new slaves arriving will be queued and a new transfer# will start when the current one terminates.## When diskless replication is used, the master waits a configurable amount of# time (in seconds) before starting the transfer in the hope that multiple slaves# will arrive and the transfer can be parallelized.## With slow disks and fast (large bandwidth) networks, diskless replication# works better.repl-diskless-sync no# When diskless replication is enabled, it is possible to configure the delay# the server waits in order to spawn the child that transfers the RDB via socket# to the slaves.## This is important since once the transfer starts, it is not possible to serve# new slaves arriving, that will be queued for the next RDB transfer, so the server# waits a delay in order to let more slaves arrive.## The delay is specified in seconds, and by default is 5 seconds. To disable# it entirely just set it to 0 seconds and the transfer will start ASAP.repl-diskless-sync-delay 5# Slaves send PINGs to server in a predefined interval. It's possible to change# this interval with the repl_ping_slave_period option. The default value is 10# seconds.## repl-ping-slave-period 10# The following option sets the replication timeout for:## 1) Bulk transfer I/O during SYNC, from the point of view of slave.# 2) Master timeout from the point of view of slaves (data, pings).# 3) Slave timeout from the point of view of masters (REPLCONF ACK pings).## It is important to make sure that this value is greater than the value# specified for repl-ping-slave-period otherwise a timeout will be detected# every time there is low traffic between the master and the slave.## repl-timeout 60# Disable TCP_NODELAY on the slave socket after SYNC?## If you select "yes" Redis will use a smaller number of TCP packets and# less bandwidth to send data to slaves. But this can add a delay for# the data to appear on the slave side, up to 40 milliseconds with# Linux kernels using a default configuration.## If you select "no" the delay for data to appear on the slave side will# be reduced but more bandwidth will be used for replication.## By default we optimize for low latency, but in very high traffic conditions# or when the master and slaves are many hops away, turning this to "yes" may# be a good idea.repl-disable-tcp-nodelay no# Set the replication backlog size. The backlog is a buffer that accumulates# slave data when slaves are disconnected for some time, so that when a slave# wants to reconnect again, often a full resync is not needed, but a partial# resync is enough, just passing the portion of data the slave missed while# disconnected.## The bigger the replication backlog, the longer the time the slave can be# disconnected and later be able to perform a partial resynchronization.## The backlog is only allocated once there is at least a slave connected.## repl-backlog-size 1mb# After a master has no longer connected slaves for some time, the backlog# will be freed. The following option configures the amount of seconds that# need to elapse, starting from the time the last slave disconnected, for# the backlog buffer to be freed.## Note that slaves never free the backlog for timeout, since they may be# promoted to masters later, and should be able to correctly "partially# resynchronize" with the slaves: hence they should always accumulate backlog.## A value of 0 means to never release the backlog.## repl-backlog-ttl 3600# The slave priority is an integer number published by Redis in the INFO output.# It is used by Redis Sentinel in order to select a slave to promote into a# master if the master is no longer working correctly.## A slave with a low priority number is considered better for promotion, so# for instance if there are three slaves with priority 10, 100, 25 Sentinel will# pick the one with priority 10, that is the lowest.## However a special priority of 0 marks the slave as not able to perform the# role of master, so a slave with priority of 0 will never be selected by# Redis Sentinel for promotion.## By default the priority is 100.slave-priority 100# It is possible for a master to stop accepting writes if there are less than# N slaves connected, having a lag less or equal than M seconds.## The N slaves need to be in "online" state.## The lag in seconds, that must be &lt;= the specified value, is calculated from# the last ping received from the slave, that is usually sent every second.## This option does not GUARANTEE that N replicas will accept the write, but# will limit the window of exposure for lost writes in case not enough slaves# are available, to the specified number of seconds.## For example to require at least 3 slaves with a lag &lt;= 10 seconds use:## min-slaves-to-write 3# min-slaves-max-lag 10## Setting one or the other to 0 disables the feature.## By default min-slaves-to-write is set to 0 (feature disabled) and# min-slaves-max-lag is set to 10.# A Redis master is able to list the address and port of the attached# slaves in different ways. For example the "INFO replication" section# offers this information, which is used, among other tools, by# Redis Sentinel in order to discover slave instances.# Another place where this info is available is in the output of the# "ROLE" command of a master.## The listed IP and address normally reported by a slave is obtained# in the following way:## IP: The address is auto detected by checking the peer address# of the socket used by the slave to connect with the master.## Port: The port is communicated by the slave during the replication# handshake, and is normally the port that the slave is using to# list for connections.## However when port forwarding or Network Address Translation (NAT) is# used, the slave may be actually reachable via different IP and port# pairs. The following two options can be used by a slave in order to# report to its master a specific set of IP and port, so that both INFO# and ROLE will report those values.## There is no need to use both the options if you need to override just# the port or the IP address.## slave-announce-ip 5.5.5.5# slave-announce-port 1234################################## SECURITY #################################### Require clients to issue AUTH &lt;PASSWORD&gt; before processing any other# commands. This might be useful in environments in which you do not trust# others with access to the host running redis-server.## This should stay commented out for backward compatibility and because most# people do not need auth (e.g. they run their own servers).## Warning: since Redis is pretty fast an outside user can try up to# 150k passwords per second against a good box. This means that you should# use a very strong password otherwise it will be very easy to break.## requirepass foobared# Command renaming.## It is possible to change the name of dangerous commands in a shared# environment. For instance the CONFIG command may be renamed into something# hard to guess so that it will still be available for internal-use tools# but not available for general clients.## Example:## rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52## It is also possible to completely kill a command by renaming it into# an empty string:## rename-command CONFIG ""## Please note that changing the name of commands that are logged into the# AOF file or transmitted to slaves may cause problems.################################### CLIENTS ##################################### Set the max number of connected clients at the same time. By default# this limit is set to 10000 clients, however if the Redis server is not# able to configure the process file limit to allow for the specified limit# the max number of allowed clients is set to the current file limit# minus 32 (as Redis reserves a few file descriptors for internal uses).## Once the limit is reached Redis will close all the new connections sending# an error 'max number of clients reached'.## maxclients 10000############################## MEMORY MANAGEMENT ################################# Set a memory usage limit to the specified amount of bytes.# When the memory limit is reached Redis will try to remove keys# according to the eviction policy selected (see maxmemory-policy).## If Redis can't remove keys according to the policy, or if the policy is# set to 'noeviction', Redis will start to reply with errors to commands# that would use more memory, like SET, LPUSH, and so on, and will continue# to reply to read-only commands like GET.## This option is usually useful when using Redis as an LRU or LFU cache, or to# set a hard memory limit for an instance (using the 'noeviction' policy).## WARNING: If you have slaves attached to an instance with maxmemory on,# the size of the output buffers needed to feed the slaves are subtracted# from the used memory count, so that network problems / resyncs will# not trigger a loop where keys are evicted, and in turn the output# buffer of slaves is full with DELs of keys evicted triggering the deletion# of more keys, and so forth until the database is completely emptied.## In short... if you have slaves attached it is suggested that you set a lower# limit for maxmemory so that there is some free RAM on the system for slave# output buffers (but this is not needed if the policy is 'noeviction').## maxmemory &lt;bytes&gt;# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory# is reached. You can select among five behaviors:## volatile-lru -&gt; Evict using approximated LRU among the keys with an expire set.# allkeys-lru -&gt; Evict any key using approximated LRU.# volatile-lfu -&gt; Evict using approximated LFU among the keys with an expire set.# allkeys-lfu -&gt; Evict any key using approximated LFU.# volatile-random -&gt; Remove a random key among the ones with an expire set.# allkeys-random -&gt; Remove a random key, any key.# volatile-ttl -&gt; Remove the key with the nearest expire time (minor TTL)# noeviction -&gt; Don't evict anything, just return an error on write operations.## LRU means Least Recently Used# LFU means Least Frequently Used## Both LRU, LFU and volatile-ttl are implemented using approximated# randomized algorithms.## Note: with any of the above policies, Redis will return an error on write# operations, when there are no suitable keys for eviction.## At the date of writing these commands are: set setnx setex append# incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd# sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby# zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby# getset mset msetnx exec sort## The default is:## maxmemory-policy noeviction# LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated# algorithms (in order to save memory), so you can tune it for speed or# accuracy. For default Redis will check five keys and pick the one that was# used less recently, you can change the sample size using the following# configuration directive.## The default of 5 produces good enough results. 10 Approximates very closely# true LRU but costs more CPU. 3 is faster but not very accurate.## maxmemory-samples 5############################# LAZY FREEING ##################################### Redis has two primitives to delete keys. One is called DEL and is a blocking# deletion of the object. It means that the server stops processing new commands# in order to reclaim all the memory associated with an object in a synchronous# way. If the key deleted is associated with a small object, the time needed# in order to execute the DEL command is very small and comparable to most other# O(1) or O(log_N) commands in Redis. However if the key is associated with an# aggregated value containing millions of elements, the server can block for# a long time (even seconds) in order to complete the operation.## For the above reasons Redis also offers non blocking deletion primitives# such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and# FLUSHDB commands, in order to reclaim memory in background. Those commands# are executed in constant time. Another thread will incrementally free the# object in the background as fast as possible.## DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled.# It's up to the design of the application to understand when it is a good# idea to use one or the other. However the Redis server sometimes has to# delete keys or flush the whole database as a side effect of other operations.# Specifically Redis deletes objects independently of a user call in the# following scenarios:## 1) On eviction, because of the maxmemory and maxmemory policy configurations,# in order to make room for new data, without going over the specified# memory limit.# 2) Because of expire: when a key with an associated time to live (see the# EXPIRE command) must be deleted from memory.# 3) Because of a side effect of a command that stores data on a key that may# already exist. For example the RENAME command may delete the old key# content when it is replaced with another one. Similarly SUNIONSTORE# or SORT with STORE option may delete existing keys. The SET command# itself removes any old content of the specified key in order to replace# it with the specified string.# 4) During replication, when a slave performs a full resynchronization with# its master, the content of the whole database is removed in order to# load the RDB file just transfered.## In all the above cases the default is to delete objects in a blocking way,# like if DEL was called. However you can configure each case specifically# in order to instead release memory in a non-blocking way like if UNLINK# was called, using the following configuration directives:lazyfree-lazy-eviction nolazyfree-lazy-expire nolazyfree-lazy-server-del noslave-lazy-flush no############################## APPEND ONLY MODE ################################ By default Redis asynchronously dumps the dataset on disk. This mode is# good enough in many applications, but an issue with the Redis process or# a power outage may result into a few minutes of writes lost (depending on# the configured save points).## The Append Only File is an alternative persistence mode that provides# much better durability. For instance using the default data fsync policy# (see later in the config file) Redis can lose just one second of writes in a# dramatic event like a server power outage, or a single write if something# wrong with the Redis process itself happens, but the operating system is# still running correctly.## AOF and RDB persistence can be enabled at the same time without problems.# If the AOF is enabled on startup Redis will load the AOF, that is the file# with the better durability guarantees.## Please check http://redis.io/topics/persistence for more information.appendonly no# The name of the append only file (default: "appendonly.aof")appendfilename "appendonly.aof"# The fsync() call tells the Operating System to actually write data on disk# instead of waiting for more data in the output buffer. Some OS will really flush# data on disk, some other OS will just try to do it ASAP.## Redis supports three different modes:## no: don't fsync, just let the OS flush the data when it wants. Faster.# always: fsync after every write to the append only log. Slow, Safest.# everysec: fsync only one time every second. Compromise.## The default is "everysec", as that's usually the right compromise between# speed and data safety. It's up to you to understand if you can relax this to# "no" that will let the operating system flush the output buffer when# it wants, for better performances (but if you can live with the idea of# some data loss consider the default persistence mode that's snapshotting),# or on the contrary, use "always" that's very slow but a bit safer than# everysec.## More details please check the following article:# http://antirez.com/post/redis-persistence-demystified.html## If unsure, use "everysec".# appendfsync alwaysappendfsync everysec# appendfsync no# When the AOF fsync policy is set to always or everysec, and a background# saving process (a background save or AOF log background rewriting) is# performing a lot of I/O against the disk, in some Linux configurations# Redis may block too long on the fsync() call. Note that there is no fix for# this currently, as even performing fsync in a different thread will block# our synchronous write(2) call.## In order to mitigate this problem it's possible to use the following option# that will prevent fsync() from being called in the main process while a# BGSAVE or BGREWRITEAOF is in progress.## This means that while another child is saving, the durability of Redis is# the same as "appendfsync none". In practical terms, this means that it is# possible to lose up to 30 seconds of log in the worst scenario (with the# default Linux settings).## If you have latency problems turn this to "yes". Otherwise leave it as# "no" that is the safest pick from the point of view of durability.no-appendfsync-on-rewrite no# Automatic rewrite of the append only file.# Redis is able to automatically rewrite the log file implicitly calling# BGREWRITEAOF when the AOF log size grows by the specified percentage.## This is how it works: Redis remembers the size of the AOF file after the# latest rewrite (if no rewrite has happened since the restart, the size of# the AOF at startup is used).## This base size is compared to the current size. If the current size is# bigger than the specified percentage, the rewrite is triggered. Also# you need to specify a minimal size for the AOF file to be rewritten, this# is useful to avoid rewriting the AOF file even if the percentage increase# is reached but it is still pretty small.## Specify a percentage of zero in order to disable the automatic AOF# rewrite feature.auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# An AOF file may be found to be truncated at the end during the Redis# startup process, when the AOF data gets loaded back into memory.# This may happen when the system where Redis is running# crashes, especially when an ext4 filesystem is mounted without the# data=ordered option (however this can't happen when Redis itself# crashes or aborts but the operating system still works correctly).## Redis can either exit with an error when this happens, or load as much# data as possible (the default now) and start if the AOF file is found# to be truncated at the end. The following option controls this behavior.## If aof-load-truncated is set to yes, a truncated AOF file is loaded and# the Redis server starts emitting a log to inform the user of the event.# Otherwise if the option is set to no, the server aborts with an error# and refuses to start. When the option is set to no, the user requires# to fix the AOF file using the "redis-check-aof" utility before to restart# the server.## Note that if the AOF file will be found to be corrupted in the middle# the server will still exit with an error. This option only applies when# Redis will try to read more data from the AOF file but not enough bytes# will be found.aof-load-truncated yes# When rewriting the AOF file, Redis is able to use an RDB preamble in the# AOF file for faster rewrites and recoveries. When this option is turned# on the rewritten AOF file is composed of two different stanzas:## [RDB file][AOF tail]## When loading Redis recognizes that the AOF file starts with the "REDIS"# string and loads the prefixed RDB file, and continues loading the AOF# tail.## This is currently turned off by default in order to avoid the surprise# of a format change, but will at some point be used as the default.aof-use-rdb-preamble no################################ LUA SCRIPTING ################################ Max execution time of a Lua script in milliseconds.## If the maximum execution time is reached Redis will log that a script is# still in execution after the maximum allowed time and will start to# reply to queries with an error.## When a long running script exceeds the maximum execution time only the# SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be# used to stop a script that did not yet called write commands. The second# is the only way to shut down the server in the case a write command was# already issued by the script but the user doesn't want to wait for the natural# termination of the script.## Set it to 0 or a negative value for unlimited execution without warnings.lua-time-limit 5000################################ REDIS CLUSTER ################################# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++# WARNING EXPERIMENTAL: Redis Cluster is considered to be stable code, however# in order to mark it as "mature" we need to wait for a non trivial percentage# of users to deploy it in production.# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++## Normal Redis instances can't be part of a Redis Cluster; only nodes that are# started as cluster nodes can. In order to start a Redis instance as a# cluster node enable the cluster support uncommenting the following:## cluster-enabled yes# Every cluster node has a cluster configuration file. This file is not# intended to be edited by hand. It is created and updated by Redis nodes.# Every Redis Cluster node requires a different cluster configuration file.# Make sure that instances running in the same system do not have# overlapping cluster configuration file names.## cluster-config-file nodes-6379.conf# Cluster node timeout is the amount of milliseconds a node must be unreachable# for it to be considered in failure state.# Most other internal time limits are multiple of the node timeout.## cluster-node-timeout 15000# A slave of a failing master will avoid to start a failover if its data# looks too old.## There is no simple way for a slave to actually have an exact measure of# its "data age", so the following two checks are performed:## 1) If there are multiple slaves able to failover, they exchange messages# in order to try to give an advantage to the slave with the best# replication offset (more data from the master processed).# Slaves will try to get their rank by offset, and apply to the start# of the failover a delay proportional to their rank.## 2) Every single slave computes the time of the last interaction with# its master. This can be the last ping or command received (if the master# is still in the "connected" state), or the time that elapsed since the# disconnection with the master (if the replication link is currently down).# If the last interaction is too old, the slave will not try to failover# at all.## The point "2" can be tuned by user. Specifically a slave will not perform# the failover if, since the last interaction with the master, the time# elapsed is greater than:## (node-timeout * slave-validity-factor) + repl-ping-slave-period## So for example if node-timeout is 30 seconds, and the slave-validity-factor# is 10, and assuming a default repl-ping-slave-period of 10 seconds, the# slave will not try to failover if it was not able to talk with the master# for longer than 310 seconds.## A large slave-validity-factor may allow slaves with too old data to failover# a master, while a too small value may prevent the cluster from being able to# elect a slave at all.## For maximum availability, it is possible to set the slave-validity-factor# to a value of 0, which means, that slaves will always try to failover the# master regardless of the last time they interacted with the master.# (However they'll always try to apply a delay proportional to their# offset rank).## Zero is the only value able to guarantee that when all the partitions heal# the cluster will always be able to continue.## cluster-slave-validity-factor 10# Cluster slaves are able to migrate to orphaned masters, that are masters# that are left without working slaves. This improves the cluster ability# to resist to failures as otherwise an orphaned master can't be failed over# in case of failure if it has no working slaves.## Slaves migrate to orphaned masters only if there are still at least a# given number of other working slaves for their old master. This number# is the "migration barrier". A migration barrier of 1 means that a slave# will migrate only if there is at least 1 other working slave for its master# and so forth. It usually reflects the number of slaves you want for every# master in your cluster.## Default is 1 (slaves migrate only if their masters remain with at least# one slave). To disable migration just set it to a very large value.# A value of 0 can be set but is useful only for debugging and dangerous# in production.## cluster-migration-barrier 1# By default Redis Cluster nodes stop accepting queries if they detect there# is at least an hash slot uncovered (no available node is serving it).# This way if the cluster is partially down (for example a range of hash slots# are no longer covered) all the cluster becomes, eventually, unavailable.# It automatically returns available as soon as all the slots are covered again.## However sometimes you want the subset of the cluster which is working,# to continue to accept queries for the part of the key space that is still# covered. In order to do so, just set the cluster-require-full-coverage# option to no.## cluster-require-full-coverage yes# This option, when set to yes, prevents slaves from trying to failover its# master during master failures. However the master can still perform a# manual failover, if forced to do so.## This is useful in different scenarios, especially in the case of multiple# data center operations, where we want one side to never be promoted if not# in the case of a total DC failure.## cluster-slave-no-failover no# In order to setup your cluster make sure to read the documentation# available at http://redis.io web site.########################## CLUSTER DOCKER/NAT support ######################### In certain deployments, Redis Cluster nodes address discovery fails, because# addresses are NAT-ted or because ports are forwarded (the typical case is# Docker and other containers).## In order to make Redis Cluster working in such environments, a static# configuration where each node knows its public address is needed. The# following two options are used for this scope, and are:## * cluster-announce-ip# * cluster-announce-port# * cluster-announce-bus-port## Each instruct the node about its address, client port, and cluster message# bus port. The information is then published in the header of the bus packets# so that other nodes will be able to correctly map the address of the node# publishing the information.## If the above options are not used, the normal Redis Cluster auto-detection# will be used instead.## Note that when remapped, the bus port may not be at the fixed offset of# clients port + 10000, so you can specify any port and bus-port depending# on how they get remapped. If the bus-port is not set, a fixed offset of# 10000 will be used as usually.## Example:## cluster-announce-ip 10.1.1.5# cluster-announce-port 6379# cluster-announce-bus-port 6380################################## SLOW LOG #################################### The Redis Slow Log is a system to log queries that exceeded a specified# execution time. The execution time does not include the I/O operations# like talking with the client, sending the reply and so forth,# but just the time needed to actually execute the command (this is the only# stage of command execution where the thread is blocked and can not serve# other requests in the meantime).## You can configure the slow log with two parameters: one tells Redis# what is the execution time, in microseconds, to exceed in order for the# command to get logged, and the other parameter is the length of the# slow log. When a new command is logged the oldest one is removed from the# queue of logged commands.# The following time is expressed in microseconds, so 1000000 is equivalent# to one second. Note that a negative number disables the slow log, while# a value of zero forces the logging of every command.slowlog-log-slower-than 10000# There is no limit to this length. Just be aware that it will consume memory.# You can reclaim memory used by the slow log with SLOWLOG RESET.slowlog-max-len 128################################ LATENCY MONITOR ############################### The Redis latency monitoring subsystem samples different operations# at runtime in order to collect data related to possible sources of# latency of a Redis instance.## Via the LATENCY command this information is available to the user that can# print graphs and obtain reports.## The system only logs operations that were performed in a time equal or# greater than the amount of milliseconds specified via the# latency-monitor-threshold configuration directive. When its value is set# to zero, the latency monitor is turned off.## By default latency monitoring is disabled since it is mostly not needed# if you don't have latency issues, and collecting data has a performance# impact, that while very small, can be measured under big load. Latency# monitoring can easily be enabled at runtime using the command# "CONFIG SET latency-monitor-threshold &lt;milliseconds&gt;" if needed.latency-monitor-threshold 0############################# EVENT NOTIFICATION ############################### Redis can notify Pub/Sub clients about events happening in the key space.# This feature is documented at http://redis.io/topics/notifications## For instance if keyspace events notification is enabled, and a client# performs a DEL operation on key "foo" stored in the Database 0, two# messages will be published via Pub/Sub:## PUBLISH __keyspace@0__:foo del# PUBLISH __keyevent@0__:del foo## It is possible to select the events that Redis will notify among a set# of classes. Every class is identified by a single character:## K Keyspace events, published with __keyspace@&lt;db&gt;__ prefix.# E Keyevent events, published with __keyevent@&lt;db&gt;__ prefix.# g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...# $ String commands# l List commands# s Set commands# h Hash commands# z Sorted set commands# x Expired events (events generated every time a key expires)# e Evicted events (events generated when a key is evicted for maxmemory)# A Alias for g$lshzxe, so that the "AKE" string means all the events.## The "notify-keyspace-events" takes as argument a string that is composed# of zero or multiple characters. The empty string means that notifications# are disabled.## Example: to enable list and generic events, from the point of view of the# event name, use:## notify-keyspace-events Elg## Example 2: to get the stream of the expired keys subscribing to channel# name __keyevent@0__:expired use:## notify-keyspace-events Ex## By default all notifications are disabled because most users don't need# this feature and the feature has some overhead. Note that if you don't# specify at least one of K or E, no events will be delivered.notify-keyspace-events ""############################### ADVANCED CONFIG ################################ Hashes are encoded using a memory efficient data structure when they have a# small number of entries, and the biggest entry does not exceed a given# threshold. These thresholds can be configured using the following directives.hash-max-ziplist-entries 512hash-max-ziplist-value 64# Lists are also encoded in a special way to save a lot of space.# The number of entries allowed per internal list node can be specified# as a fixed maximum size or a maximum number of elements.# For a fixed maximum size, use -5 through -1, meaning:# -5: max size: 64 Kb &lt;-- not recommended for normal workloads# -4: max size: 32 Kb &lt;-- not recommended# -3: max size: 16 Kb &lt;-- probably not recommended# -2: max size: 8 Kb &lt;-- good# -1: max size: 4 Kb &lt;-- good# Positive numbers mean store up to _exactly_ that number of elements# per list node.# The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),# but if your use case is unique, adjust the settings as necessary.list-max-ziplist-size -2# Lists may also be compressed.# Compress depth is the number of quicklist ziplist nodes from *each* side of# the list to *exclude* from compression. The head and tail of the list# are always uncompressed for fast push/pop operations. Settings are:# 0: disable all list compression# 1: depth 1 means "don't start compressing until after 1 node into the list,# going from either the head or tail"# So: [head]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[tail]# [head], [tail] will always be uncompressed; inner nodes will compress.# 2: [head]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[tail]# 2 here means: don't compress head or head-&gt;next or tail-&gt;prev or tail,# but compress all nodes between them.# 3: [head]-&gt;[next]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[prev]-&gt;[tail]# etc.list-compress-depth 0# Sets have a special encoding in just one case: when a set is composed# of just strings that happen to be integers in radix 10 in the range# of 64 bit signed integers.# The following configuration setting sets the limit in the size of the# set in order to use this special memory saving encoding.set-max-intset-entries 512# Similarly to hashes and lists, sorted sets are also specially encoded in# order to save a lot of space. This encoding is only used when the length and# elements of a sorted set are below the following limits:zset-max-ziplist-entries 128zset-max-ziplist-value 64# HyperLogLog sparse representation bytes limit. The limit includes the# 16 bytes header. When an HyperLogLog using the sparse representation crosses# this limit, it is converted into the dense representation.## A value greater than 16000 is totally useless, since at that point the# dense representation is more memory efficient.## The suggested value is ~ 3000 in order to have the benefits of# the space efficient encoding without slowing down too much PFADD,# which is O(N) with the sparse encoding. The value can be raised to# ~ 10000 when CPU is not a concern, but space is, and the data set is# composed of many HyperLogLogs with cardinality in the 0 - 15000 range.hll-sparse-max-bytes 3000# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in# order to help rehashing the main Redis hash table (the one mapping top-level# keys to values). The hash table implementation Redis uses (see dict.c)# performs a lazy rehashing: the more operation you run into a hash table# that is rehashing, the more rehashing "steps" are performed, so if the# server is idle the rehashing is never complete and some more memory is used# by the hash table.## The default is to use this millisecond 10 times every second in order to# actively rehash the main dictionaries, freeing memory when possible.## If unsure:# use "activerehashing no" if you have hard latency requirements and it is# not a good thing in your environment that Redis can reply from time to time# to queries with 2 milliseconds delay.## use "activerehashing yes" if you don't have such hard requirements but# want to free memory asap when possible.activerehashing yes# The client output buffer limits can be used to force disconnection of clients# that are not reading data from the server fast enough for some reason (a# common reason is that a Pub/Sub client can't consume messages as fast as the# publisher can produce them).## The limit can be set differently for the three different classes of clients:## normal -&gt; normal clients including MONITOR clients# slave -&gt; slave clients# pubsub -&gt; clients subscribed to at least one pubsub channel or pattern## The syntax of every client-output-buffer-limit directive is the following:## client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;## A client is immediately disconnected once the hard limit is reached, or if# the soft limit is reached and remains reached for the specified number of# seconds (continuously).# So for instance if the hard limit is 32 megabytes and the soft limit is# 16 megabytes / 10 seconds, the client will get disconnected immediately# if the size of the output buffers reach 32 megabytes, but will also get# disconnected if the client reaches 16 megabytes and continuously overcomes# the limit for 10 seconds.## By default normal clients are not limited because they don't receive data# without asking (in a push way), but just after a request, so only# asynchronous clients may create a scenario where data is requested faster# than it can read.## Instead there is a default limit for pubsub and slave clients, since# subscribers and slaves receive data in a push fashion.## Both the hard or the soft limit can be disabled by setting them to zero.client-output-buffer-limit normal 0 0 0client-output-buffer-limit slave 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60# Client query buffers accumulate new commands. They are limited to a fixed# amount by default in order to avoid that a protocol desynchronization (for# instance due to a bug in the client) will lead to unbound memory usage in# the query buffer. However you can configure it here if you have very special# needs, such us huge multi/exec requests or alike.## client-query-buffer-limit 1gb# In the Redis protocol, bulk requests, that are, elements representing single# strings, are normally limited ot 512 mb. However you can change this limit# here.## proto-max-bulk-len 512mb# Redis calls an internal function to perform many background tasks, like# closing connections of clients in timeout, purging expired keys that are# never requested, and so forth.## Not all tasks are performed with the same frequency, but Redis checks for# tasks to perform according to the specified "hz" value.## By default "hz" is set to 10. Raising the value will use more CPU when# Redis is idle, but at the same time will make Redis more responsive when# there are many keys expiring at the same time, and timeouts may be# handled with more precision.## The range is between 1 and 500, however a value over 100 is usually not# a good idea. Most users should use the default of 10 and raise this up to# 100 only in environments where very low latency is required.hz 10# When a child rewrites the AOF file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.aof-rewrite-incremental-fsync yes# Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good# idea to start with the default settings and only change them after investigating# how to improve the performances and how the keys LFU change over time, which# is possible to inspect via the OBJECT FREQ command.## There are two tunable parameters in the Redis LFU implementation: the# counter logarithm factor and the counter decay time. It is important to# understand what the two parameters mean before changing them.## The LFU counter is just 8 bits per key, it's maximum value is 255, so Redis# uses a probabilistic increment with logarithmic behavior. Given the value# of the old counter, when a key is accessed, the counter is incremented in# this way:## 1. A random number R between 0 and 1 is extracted.# 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).# 3. The counter is incremented only if R &lt; P.## The default lfu-log-factor is 10. This is a table of how the frequency# counter changes with a different number of accesses with different# logarithmic factors:## +--------+------------+------------+------------+------------+------------+# | factor | 100 hits | 1000 hits | 100K hits | 1M hits | 10M hits |# +--------+------------+------------+------------+------------+------------+# | 0 | 104 | 255 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 1 | 18 | 49 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 10 | 10 | 18 | 142 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 100 | 8 | 11 | 49 | 143 | 255 |# +--------+------------+------------+------------+------------+------------+## NOTE: The above table was obtained by running the following commands:## redis-benchmark -n 1000000 incr foo# redis-cli object freq foo## NOTE 2: The counter initial value is 5 in order to give new objects a chance# to accumulate hits.## The counter decay time is the time, in minutes, that must elapse in order# for the key counter to be divided by two (or decremented if it has a value# less &lt;= 10).## The default value for the lfu-decay-time is 1. A Special value of 0 means to# decay the counter every time it happens to be scanned.## lfu-log-factor 10# lfu-decay-time 1########################### ACTIVE DEFRAGMENTATION ######################### WARNING THIS FEATURE IS EXPERIMENTAL. However it was stress tested# even in production and manually tested by multiple engineers for some# time.## What is active defragmentation?# -------------------------------## Active (online) defragmentation allows a Redis server to compact the# spaces left between small allocations and deallocations of data in memory,# thus allowing to reclaim back memory.## Fragmentation is a natural process that happens with every allocator (but# less so with Jemalloc, fortunately) and certain workloads. Normally a server# restart is needed in order to lower the fragmentation, or at least to flush# away all the data and create it again. However thanks to this feature# implemented by Oran Agra for Redis 4.0 this process can happen at runtime# in an "hot" way, while the server is running.## Basically when the fragmentation is over a certain level (see the# configuration options below) Redis will start to create new copies of the# values in contiguous memory regions by exploiting certain specific Jemalloc# features (in order to understand if an allocation is causing fragmentation# and to allocate it in a better place), and at the same time, will release the# old copies of the data. This process, repeated incrementally for all the keys# will cause the fragmentation to drop back to normal values.## Important things to understand:## 1. This feature is disabled by default, and only works if you compiled Redis# to use the copy of Jemalloc we ship with the source code of Redis.# This is the default with Linux builds.## 2. You never need to enable this feature if you don't have fragmentation# issues.## 3. Once you experience fragmentation, you can enable this feature when# needed with the command "CONFIG SET activedefrag yes".## The configuration parameters are able to fine tune the behavior of the# defragmentation process. If you are not sure about what they mean it is# a good idea to leave the defaults untouched.# Enabled active defragmentation# activedefrag yes# Minimum amount of fragmentation waste to start active defrag# active-defrag-ignore-bytes 100mb# Minimum percentage of fragmentation to start active defrag# active-defrag-threshold-lower 10# Maximum percentage of fragmentation at which we use maximum effort# active-defrag-threshold-upper 100# Minimal effort for defrag in CPU percentage# active-defrag-cycle-min 25# Maximal effort for defrag in CPU percentage# active-defrag-cycle-max 75 sentinel默认配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195# Example sentinel.conf# *** IMPORTANT ***## By default Sentinel will not be reachable from interfaces different than# localhost, either use the 'bind' directive to bind to a list of network# interfaces, or disable protected mode with "protected-mode no" by# adding it to this configuration file.## Before doing that MAKE SURE the instance is protected from the outside# world via firewalling or other means.## For example you may use one of the following:## bind 127.0.0.1 192.168.1.1## protected-mode no# port &lt;sentinel-port&gt;# The port that this sentinel instance will run onport 26379# sentinel announce-ip &lt;ip&gt;# sentinel announce-port &lt;port&gt;## The above two configuration directives are useful in environments where,# because of NAT, Sentinel is reachable from outside via a non-local address.## When announce-ip is provided, the Sentinel will claim the specified IP address# in HELLO messages used to gossip its presence, instead of auto-detecting the# local address as it usually does.## Similarly when announce-port is provided and is valid and non-zero, Sentinel# will announce the specified TCP port.## The two options don't need to be used together, if only announce-ip is# provided, the Sentinel will announce the specified IP and the server port# as specified by the "port" option. If only announce-port is provided, the# Sentinel will announce the auto-detected local IP and the specified port.## Example:## sentinel announce-ip 1.2.3.4# dir &lt;working-directory&gt;# Every long running process should have a well-defined working directory.# For Redis Sentinel to chdir to /tmp at startup is the simplest thing# for the process to don't interfere with administrative tasks such as# unmounting filesystems.dir /tmp# sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt;## Tells Sentinel to monitor this master, and to consider it in O_DOWN# (Objectively Down) state only if at least &lt;quorum&gt; sentinels agree.## Note that whatever is the ODOWN quorum, a Sentinel will require to# be elected by the majority of the known Sentinels in order to# start a failover, so no failover can be performed in minority.## Slaves are auto-discovered, so you don't need to specify slaves in# any way. Sentinel itself will rewrite this configuration file adding# the slaves using additional configuration options.# Also note that the configuration file is rewritten when a# slave is promoted to master.## Note: master name should not include special characters or spaces.# The valid charset is A-z 0-9 and the three characters ".-_".sentinel monitor mymaster 127.0.0.1 6379 2# sentinel auth-pass &lt;master-name&gt; &lt;password&gt;## Set the password to use to authenticate with the master and slaves.# Useful if there is a password set in the Redis instances to monitor.## Note that the master password is also used for slaves, so it is not# possible to set a different password in masters and slaves instances# if you want to be able to monitor these instances with Sentinel.## However you can have Redis instances without the authentication enabled# mixed with Redis instances requiring the authentication (as long as the# password set is the same for all the instances requiring the password) as# the AUTH command will have no effect in Redis instances with authentication# switched off.## Example:## sentinel auth-pass mymaster MySUPER--secret-0123passw0rd# sentinel down-after-milliseconds &lt;master-name&gt; &lt;milliseconds&gt;## Number of milliseconds the master (or any attached slave or sentinel) should# be unreachable (as in, not acceptable reply to PING, continuously, for the# specified period) in order to consider it in S_DOWN state (Subjectively# Down).## Default is 30 seconds.sentinel down-after-milliseconds mymaster 30000# sentinel parallel-syncs &lt;master-name&gt; &lt;numslaves&gt;## How many slaves we can reconfigure to point to the new slave simultaneously# during the failover. Use a low number if you use the slaves to serve query# to avoid that all the slaves will be unreachable at about the same# time while performing the synchronization with the master.sentinel parallel-syncs mymaster 1# sentinel failover-timeout &lt;master-name&gt; &lt;milliseconds&gt;## Specifies the failover timeout in milliseconds. It is used in many ways:## - The time needed to re-start a failover after a previous failover was# already tried against the same master by a given Sentinel, is two# times the failover timeout.## - The time needed for a slave replicating to a wrong master according# to a Sentinel current configuration, to be forced to replicate# with the right master, is exactly the failover timeout (counting since# the moment a Sentinel detected the misconfiguration).## - The time needed to cancel a failover that is already in progress but# did not produced any configuration change (SLAVEOF NO ONE yet not# acknowledged by the promoted slave).## - The maximum time a failover in progress waits for all the slaves to be# reconfigured as slaves of the new master. However even after this time# the slaves will be reconfigured by the Sentinels anyway, but not with# the exact parallel-syncs progression as specified.## Default is 3 minutes.sentinel failover-timeout mymaster 180000# SCRIPTS EXECUTION## sentinel notification-script and sentinel reconfig-script are used in order# to configure scripts that are called to notify the system administrator# or to reconfigure clients after a failover. The scripts are executed# with the following rules for error handling:## If script exits with "1" the execution is retried later (up to a maximum# number of times currently set to 10).## If script exits with "2" (or an higher value) the script execution is# not retried.## If script terminates because it receives a signal the behavior is the same# as exit code 1.## A script has a maximum running time of 60 seconds. After this limit is# reached the script is terminated with a SIGKILL and the execution retried.# NOTIFICATION SCRIPT## sentinel notification-script &lt;master-name&gt; &lt;script-path&gt;# # Call the specified notification script for any sentinel event that is# generated in the WARNING level (for instance -sdown, -odown, and so forth).# This script should notify the system administrator via email, SMS, or any# other messaging system, that there is something wrong with the monitored# Redis systems.## The script is called with just two arguments: the first is the event type# and the second the event description.## The script must exist and be executable in order for sentinel to start if# this option is provided.## Example:## sentinel notification-script mymaster /var/redis/notify.sh# CLIENTS RECONFIGURATION SCRIPT## sentinel client-reconfig-script &lt;master-name&gt; &lt;script-path&gt;## When the master changed because of a failover a script can be called in# order to perform application-specific tasks to notify the clients that the# configuration has changed and the master is at a different address.# # The following arguments are passed to the script:## &lt;master-name&gt; &lt;role&gt; &lt;state&gt; &lt;from-ip&gt; &lt;from-port&gt; &lt;to-ip&gt; &lt;to-port&gt;## &lt;state&gt; is currently always "failover"# &lt;role&gt; is either "leader" or "observer"# # The arguments from-ip, from-port, to-ip, to-port are used to communicate# the old address of the master and the new address of the elected slave# (now a master).## This script should be resistant to multiple invocations.## Example:## sentinel client-reconfig-script mymaster /var/redis/reconfig.sh]]></content>
      <categories>
        <category>配置文件</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用redis实现tomcat会话缓存实验]]></title>
    <url>%2F2018%2F09%2F19%2F%E4%BD%BF%E7%94%A8redis%E5%AE%9E%E7%8E%B0tomcat%E4%BC%9A%E8%AF%9D%E7%BC%93%E5%AD%98%E5%AE%9E%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[这是摘要 nginx配置 修改nginx.conf实现负载均衡 修改tomcat配置 修改server.xml设置默认虚拟主机 123456789101112&lt;Engine name="Catalina" defaultHost="localhost" jvmRoute="tomcat-1"&gt; ... &lt;Host name="localhost" appBase="webapps" unpackWARs="true" autoDeploy="true"&gt; &lt;Context docBase="/web/webapp1" path="" reloadable="true"/&gt; ... &lt;/Host&gt; ...&lt;/Engine&gt; /web/webapp1/中添加index.jsp文件,方便查看效果 1234567891011121314151617181920&lt;%@page language="java" import="java.util.*" pageEncoding="UTF-8"%&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;tomcat-1&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;&lt;font color="red"&gt;Session serviced by tomcat&lt;/font&gt;&lt;/h1&gt; &lt;table aligh="center" border="1"&gt; &lt;tr&gt; &lt;td&gt;Session ID&lt;/td&gt; &lt;td&gt;&lt;%=session.getId() %&gt;&lt;/td&gt; &lt;% session.setAttribute("abc","abc");%&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Created on&lt;/td&gt; &lt;td&gt;&lt;%= session.getCreationTime() %&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/body&gt;&lt;html&gt; 在tomcat/lib/中添加以下jar包 123tomcat-redis-session-manage-tomcat7.jarjedis-2.5.2.jarcommons-pool2-2.2.jar 修改context.xml文件 1234567891011121314151617181920&lt;Context&gt;&lt;!-- Default set of monitored resources --&gt;&lt;WatchedResource&gt;WEB-INF/web.xml&lt;/WatchedResource&gt;&lt;!-- Uncomment this to disable session persistence across Tomcat restarts --&gt;&lt;!--&lt;Manager pathname="" /&gt; --&gt;&lt;!-- Uncomment this to enable Comet connection tacking (provides eventson session expiration as well as webapp lifecycle) --&gt;&lt;!--&lt;Valve className="org.apache.catalina.valves.CometConnectionManagerValve" /&gt; --&gt; &lt;Valve className="com.s.tomcat.redissessions.RedisSessionHandlerValve"/&gt; &lt;Manager className="com.s.tomcat.redissessions.RedisSessionManager" host="120.79.202.146" port="6380" database="0" password="password" maxInactiveInterval="60" /&gt; &lt;/Context&gt; 可以尝试新的方案:https://github.com/redisson/redisson/tree/master/redisson-tomcat 启动redis]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>session</tag>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx配置详解]]></title>
    <url>%2F2018%2F09%2F10%2Fnginx%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[这是摘要 配置文件结构events块http块server块 配置https1234567891011121314151617server &#123; listen 8443; #nginx对外提供服务的https端口 include /etc/nginx/default.d/*.conf; ssl on; #配置ssl ssl_certificate edu.crt; #ssl 密钥， edu.crt放到/etc/nginx目录 ssl_certificate_key edu.key; #ssl 密钥， edu.key放到/etc/nginx目录 ssl_session_timeout 5m; #ssl_verify_client off; ssl_protocols SSLv3 TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers &quot;HIGH:!aNULL:!MD5 or HIGH:!aNULL:!MD5:!3DES&quot;; ssl_prefer_server_ciphers on; location / &#123; root /usr/share/nginx/html; index index.html index.htm; &#125;&#125; location块 获取用户真实ip 参考地址:https://blog.csdn.net/bao19901210/article/details/52537279 12345678910location /edu-ui &#123; # 携带用户真实ip proxy_set_header X-Real-IP $remote_addr; # 携带用户真实ip proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 携带域名 proxy_set_header Host $host; # proxy_set_header X-NginX-Proxy true;说明开启Nginx代理&#125; upstream块 基本配置 1234567upstream bakend&#123; #定义负载均衡设备的Ip及设备状态 ip_hash; server 10.0.0.11:9090 down; server 10.0.0.11:8080 weight=2 max_fails=3 fail_timeout=10s; server 10.0.0.11:6060 weight=1 max_fails=3 fail_timeout=10s; server 10.0.0.11:7070 backup; &#125; upstream还能够为每一个设备设置状态值，这些状态值的含义分别例如以下： down表示单前的server临时不參与负载. weightweight越大，负载的权重就越大。 max_fails同意请求失败的次数默觉得1.当超过最大次数时，返回proxy_next_upstream 模块定义的错误. fail_timeoutmax_fails次失败后。暂停的时间。 backup其他全部的非backup机器down或者忙的时候，请求backup机器。所以这台机器压力会最轻。 配置文件模板123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139#user nobody;worker_processes 1;#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; #log_format main '$remote_addr - $remote_user [$time_local] "$request" ' # '$status $body_bytes_sent "$http_referer" ' # '"$http_user_agent" "$http_x_forwarded_for"'; #access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server &#123; # 声明服务器监听端口 listen 80; # 就是http://后面的域名 server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; # 静态页面目录 root /Users/huangning/Desktop/IJProject/iBase4J/iBase4J-UI; # 默认首页 index index.html # 相当于http://127.0.0.1:8090/education/xxxx.jpg location ~ \.(html|js|css|png|jpg|gif) &#123; # 静态页面目录 root /usr/local/tomcat/webapps/education; &#125; # 相当于http://127.0.0.1:8090/education location /education &#123; # 动态页面,交给tomcat处理 proxy_pass http://127.0.0.1:8090; &#125; # 相当于http://127.0.0.1:8090/education/login.action location ~ \.(jsp|do|action) &#123; # 动态页面,交给tomcat处理 proxy_pass http://127.0.0.1:8090; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # # 如果被代理服务器返回的状态码为400或者大于400，设置的error_page配置起作用。默认为off。 # proxy_intercept_errors on; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \.php$ &#123; # proxy_pass http://127.0.0.1; #&#125; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \.php$ &#123; # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #&#125; # deny access to .htaccess files, if Apaches document root # concurs with nginxs one # #location ~ /\.ht &#123; # deny all; #&#125; &#125; # another virtual host using mix of IP-, name-, and port-based configuration # #server &#123; # listen 8000; # listen somename:8080; # server_name somename alias another.alias; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125; # HTTPS server # #server &#123; # listen 443 ssl; # server_name localhost; # ssl_certificate cert.pem; # ssl_certificate_key cert.key; # ssl_session_cache shared:SSL:1m; # ssl_session_timeout 5m; # ssl_ciphers HIGH:!aNULL:!MD5; # ssl_prefer_server_ciphers on; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125;&#125;]]></content>
      <categories>
        <category>笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CDN入门]]></title>
    <url>%2F2018%2F09%2F09%2FCDN%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[这是摘要]]></content>
      <categories>
        <category>笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[redis入门]]></title>
    <url>%2F2018%2F09%2F09%2Fredis%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[这是摘要 redis-cli使用验证密码auth password之后就可以进行各种操作了 查看信息 查看所有信息info 查看主从信息info replication 查看配置 config get *获取所有参数配置信息 config get 参数名获取对应参数名的配置信息 参数热修改 config set 参数名 参数值重启之后失效 config rewrite将当前配置写入配置文件,重启之后还有效 获取匹配正则的所有keykeys pattern redis应用场景 参考地址:http://blog.51cto.com/yw666/1910451 负载均衡中,缓存session实现session共享当使用负载均衡技术将用户请求分发到不同的服务器上时,就可能引起session一致性问题.解决方案: 使用ip_hash技术将请求精确定位到确定的服务器,缺点就是服务器宕机,则会话丢失 会话共享复制:在服务器之间复制共享会话,缺点就是只能在相同的中间件之间进行(比如tomcat-tomcat) 使用cacheDB存取session信息，应用服务器接受新请求将session信息保存在cacheDB中，当应用服务器发生故障时，调度器会遍历寻找可用节点，分发请求，当应用服务器发现session不在本机内存时，则去cacheDB中查找，如果找到则复制到本机，这样实现session共享和高可用。 配置redis主从,配置哨兵如果master掉线了,sentinel会从slave中vote一个变成master,原来的master重启后,会变成slave 配置主redis,docker run --name redis-master -p 6381:6381 -v $PWD/redis-master.conf:/data/redis-master.conf -d redis redis-server /data/redis-master.conf 配置从redis,docker run --name redis-slave1 -p 6382:6382 -v $PWD/redis-slave1.conf:/data/redis-slave1.conf -d redis redis-server /data/redis-slave1.conf 1slaveof master_ip port 配置哨兵docker run --name sentinel -p 26379:26379 -v $PWD/sentinel/sentinel.conf:/data/sentinel.conf -d redis redis-sentinel /data/sentinel.conf 1234567daemonize noprotected-mode noport 26379dir "/tmp"sentinel deny-scripts-reconfig yes#最后的1表示有几个哨兵觉得master掉线了sentinel monitor mymaster 120.79.202.146 6383 1 注意事项如果主从redis都没有配置bind,并且protected-mode no,那么redis-sentinel.conf也要如此配置;如果主从redis配置了bind ip,并且protected-mode yes,那么redis-sentinel.conf也要如此配置 redis内存模型 参考地址:https://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&amp;mid=2650768913&amp;idx=1&amp;sn=df5d7dbe8122b832d3b398f94989c61f&amp;scene=21#wechat_redirect 下图是执行set hello word涉及的数据模型 dictEntry(24字节)Redis是Key-Value数据库，因此对每个键值对都会有一个dictEntry，里面存储了指向Key和Value的指针；next指向下一个dictEntry，与本Key-Value无关。 Key(即SDS len+free+9 字节)图中右上角可见，Key（“hello”）并不是直接以字符串存储，而是存储在SDS结构中。 SDS(简单动态字符串)Redis没有直接使用C字符串(即以空字符‘\0’结尾的字符数组)作为默认的字符串表示，而是使用了SDS。SDS是简单动态字符串(Simple Dynamic String)的缩写。结构如下: 12345struct sdshdr &#123; int len; //记录buf中已经使用的长度 int free; //记录buf中未使用的长度 char buf[]; //buf数组中使用'\0'作为结尾,'\0'不计算在len和free中&#125;; redisObject(16字节)Value(“world”)既不是直接以字符串存储，也不是像Key一样直接存储在SDS中，而是存储在redisObject中。实际上，不论Value是5种类型的哪一种，都是通过RedisObject来存储的；而RedisObject中的type字段指明了Value对象的类型，ptr字段则指向对象所在的地址。不过可以看出，字符串对象虽然经过了RedisObject的包装，但仍然需要通过SDS存储。RedisObject的定义: 1234567typedef struct redisObject &#123; unsigned type:4; //表示对象的类型,目前包括REDIS_STRING等五种 unsigned encoding:4; //表示对象的内部编码 unsigned lru:REDIS_LRU_BITS; //记录的是对象最后一次被访问的时间 int refcount; //记录对象被引用的次数 void *ptr; //指向具体数据的指针&#125; robj; jemalloc无论是DictEntry对象，还是RedisObject、SDS对象，都需要内存分配器（如jemalloc）分配内存进行存储。以DictEntry对象为例，有3个指针组成，在64位机器下占24个字节，jemalloc会为它分配32字节大小的内存单元。 Redis的5种对象类型 字符串(REDIS_STRING) 简介字符串是最基础的类型，因为所有的键都是字符串类型，且字符串之外的其他几种复杂类型的元素也是字符串。字符串长度不能超过512MB。 内部编码: int:8个字节的长整型。字符串值是整型时，这个值使用long整型表示。 embstr:字符串小于等于39字节时使用,由于使用时只分配一次内存(所以redisObject和SDS连续),在修改值的时候就需要重新分配两者内存,因此实现为只读,在修改时直接转换成raw再修改,无论是否超过39字节 raw:大于39字节的字符串,会分别为SDS和RedisObject分配内存 哈希(REDIS_HASH) 简介哈希作为一种数据结构，不仅与字符串、列表、集合、有序结合并列，是Redis对外提供的5种对象类型的一种，也是Redis作为Key-Value数据库所使用的数据结构。为了说明的方便，在本文后面当使用“内层的哈希”时，代表的是Redis对外提供的5种对象类型的一种；使用“外层的哈希”代指Redis作为Key-Value数据库所使用的数据结构。 内部编码内层的哈希使用的内部编码可以是压缩列表（ziplist）和哈希表（hashtable）两种；Redis的外层的哈希则只使用了hashtable。使用压缩列表的条件和REDIS_LIST相同,不满足则使用哈希表 列表(REDIS_LIST) 简介列表（list）用来存储多个有序的字符串，每个字符串称为元素；一个列表可以存储2^32-1个元素。Redis中的列表支持两端插入和弹出，并可以获得指定位置（或范围）的元素，可以充当数组、队列、栈等。 内部编码 压缩列表(ziplist)使用条件 列表中元素个数小于512 所有元素字符串对象大小小于64字节只能从压缩列表转换成双端列表,不能反向.其中，单个字符串不能超过64字节，是为了便于统一分配每个节点的长度。这里的64字节是指字符串的长度，不包括SDS结构，因为压缩列表使用连续、定长内存块存储字符串，不需要SDS结构指明长度。 双端列表(linkedlist)压缩列表是Redis为了节约内存而开发的，是由一系列特殊编码的连续内存块（而不是像双端链表一样每个节点是指针）组成的顺序型数据结构；与双端链表相比，压缩列表可以节省内存空间，但是进行修改或增删操作时，复杂度较高，因此当节点数量较少时，可以使用压缩列表。但是节点数量多时，还是使用双端链表划算。压缩列表不仅用于实现列表，也用于实现哈希、有序列表，使用非常广泛。 集合(REDIS_SET) 简介集合（set）与列表类似，都是用来保存多个字符串，但集合与列表有两点不同：集合中的元素是无序的，因此不能通过索引来操作元素；集合中的元素不能有重复。一个集合中最多可以存储2^32-1个元素，除了支持常规的增删改查，Redis还支持多个集合取交集、并集、差集。 内部编码 哈希表 整数集合整数集合适用于集合所有元素都是整数且集合元素数量较小的时候，与哈希表相比，整数集合的优势在于集中存储，节省空间；同时，虽然对于元素的操作复杂度也由O(n)变为了O(1)，但由于集合数量较少，因此操作的时间并没有明显劣势。只有集合中元素数量小于512且都为整数值时使用结构定义: 12345typedef struct intset&#123; uint32_t encoding; //决定contents数组中的元素的类型 uint32_t length; //元素个数 int8_t contents[];&#125; intset; 有序集合(REDIS_ZSET) 简介有序集合与集合一样，元素都不能重复。但与集合不同的是，有序集合中的元素是有顺序的。与列表使用索引下标作为排序依据不同，有序集合为每个元素设置一个分数（score）作为排序依据。 内部编码 压缩列表集合中成员小于128且所有成员小于64字节时使用 跳表(skiplist)跳跃表是一种有序数据结构，通过在每个节点中维持多个指向其它节点的指针，从而达到快速访问节点的目的。除了跳跃表，实现有序数据结构的另一种典型实现是平衡树；大多数情况下，跳跃表的效率可以和平衡树媲美，且跳跃表实现比平衡树简单很多，因此Redis中选用跳跃表代替平衡树。跳跃表支持平均O(logN)、最坏O(N)的复杂点进行节点查找，并支持顺序操作。Redis的跳跃表实现由zskiplist和zskiplistNode两个结构组成：前者用于保存跳跃表信息（如头结点、尾节点、长度等），后者用于表示跳跃表节点。 估算redis内存使用量 参考地址:https://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&amp;mid=2650768913&amp;idx=1&amp;sn=df5d7dbe8122b832d3b398f94989c61f&amp;scene=21#wechat_redirect 假设有90000个键值对，每个key的长度是7个字节，每个value的长度也是7个字节（且key和value都不是整数）。下面来估算这90000个键值对所占用的空间。在估算占据空间之前，首先可以判定字符串类型使用的编码方式：embstr。90000个键值对占据的内存空间主要可以分为两部分：一部分是90000个dictEntry占据的空间；一部分是键值对所需要的bucket空间。每个dictEntry占据的空间包括： 一个dictEntry，24字节，jemalloc会分配32字节的内存块。 一个key，7字节，所以SDS(key)需要7+9=16个字节，jemalloc会分配16字节的内存块。 一个RedisObject，16字节，jemalloc会分配16字节的内存块。 一个value，7字节，所以SDS(value)需要7+9=16个字节，jemalloc会分配16字节的内存块。 综上，一个dictEntry需要32+16+16+16=80个字节。bucket空间：bucket数组的大小为大于90000的最小的2^n，是131072，每个bucket元素为8字节（因为64位系统中指针大小为8字节）。因此，可以估算出这90000个键值对占据的内存大小为：9000080 + 1310728 = 8248576。验证代码:1234567891011121314151617181920public class RedisTest &#123; public static Jedis jedis = new Jedis("localhost", 6379); public static void main(String[] args) throws Exception&#123; Long m1 = Long.valueOf(getMemory()); insertData(); Long m2 = Long.valueOf(getMemory()); System.out.println(m2 - m1); &#125; public static void insertData()&#123; for(int i = 10000; i &lt; 100000; i++)&#123; jedis.set("aa" + i, "aa" + i); //key和value长度都是7字节，且不是整数 &#125; &#125; public static String getMemory()&#123; String memoryAllLine = jedis.info("memory"); String usedMemoryLine = memoryAllLine.split("\r\n")[1]; String memory = usedMemoryLine.substring(usedMemoryLine.indexOf(':') + 1); return memory; &#125;&#125; 作为对比将key和value的长度由7字节增加到8字节，则对应的SDS变为17个字节，jemalloc会分配32个字节，因此每个dictEntry占用的字节数也由80字节变为112字节。此时估算这90000个键值对占据内存大小为：90000112 + 1310728 = 11128576。123456789public static void insertData()&#123; for(int i = 10000; i &lt; 100000; i++)&#123; jedis.set("aaa" + i, "aaa" + i); //key和value长度都是8字节，且不是整数 &#125;&#125; redis高可用 参考地址:https://mp.weixin.qq.com/s?__biz=MzI4NTA1MDEwNg==&amp;mid=2650769300&amp;idx=1&amp;sn=49a11efa1a6ee605fceaddf240a55c40&amp;scene=21#wechat_redirect 持久化最简单的高可用方案,将数据备份到硬盘,保证进程退出后数据不会丢失 RDB持久化(redis database)手动或者自动将当前进程中的数据生成快照存到硬盘中优缺点 优点: RDB文件体积小,网络传输速度快,适合做全量复制 恢复速度比AOF快很多 对redis性能影响小 缺点: 实时性低,兼容性差 AOF持久化(append only file)将redis每次执行的写命令记录到单独的日志文件(类似MySQL的binlog),实时性更好,更流行与RDB持久化相对应，AOF的优点在于支持秒级持久化、兼容性好，缺点是文件大、恢复速度慢、对性能影响大。 持久化策略选择在多数情况下，我们都会配置主从环境，slave的存在既可以实现数据的热备，也可以进行读写分离分担Redis读请求，以及在master宕掉后继续提供服务。在这种情况下，一种可行的做法是： master：完全关闭持久化（包括RDB和AOF），这样可以让master的性能达到最好； slave：关闭RDB，开启AOF（如果对数据安全要求不高，开启RDB关闭AOF也可以），并定时对持久化文件进行备份（如备份到其他文件夹，并标记好备份的时间）；然后关闭AOF的自动重写，然后添加定时任务，在每天Redis闲时（如凌晨12点）调用bgrewriteaof。 复制复制主要实现了数据的多机备份以及对于读操作的负载均衡和简单的故障恢复。哨兵和集群都是在复制的基础上实现的高可用.缺陷是: 故障恢复无法自动化 写操作无法负载均衡、存储能力受到单机的限制。 哨兵在复制的基础上，哨兵实现了自动化的故障恢复。缺陷是写操作无法负载均衡、存储能力受到单机的限制。 集群通过集群，Redis解决了写操作无法负载均衡以及存储能力受到单机限制的问题，实现了较为完善的高可用方案。 redis主从同步 参考地址:https://juejin.im/entry/5b90a96a5188255c48348e15 主从同步的作用 数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。 故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复，但实际上是一种服务的冗余。 负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务（即写Redis数据时应用连接主节点，读Redis数据时应用连接从节点），分担服务器负载；尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高Redis服务器的并发量。 高可用基石：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是Redis高可用的基础。 步骤 启动两个redis实例,此时两个都默认是master节点 选择其中之一作为slave节点执行slaveof 192.168.0.230 6379,想要断开复制只要在从节点上执行slaveof no one 原理 建立连接阶段 保存主节点信息 建立socket连接 发送ping命令,根据返回的信息判断socket连接是否可用 身份验证 发送从节点信息,主节点保存 数据同步阶段 从节点向主节点发送psync命令,开始同步.此时主从节点互为客户端,同步根据主从节点状态分为两种: 全量复制 从节点或者主节点判断出无法进行部分复制的时候,使用全量复制 主节点接收到全量复制命令后,执行bgsave,在后台生成RDB文件,并使用一个缓存区保存从现在开始的所有写命令 从节点接收到RDB文件后,将现有旧数据删除,载入RDB文件,将数据库恢复到主节点执行bgsave时的状态, 主节点将缓存区中的写命令发送给从节点执行,更新到最新状态 部分复制 复制偏移量主节点和从节点分别维护一个offset,用来表示主节点复制到从节点的数据量,通过对比offset可以得知主从节点中的数据是否一致,不一致的数据存储位置记录在复制积压缓冲区中. 复制积压缓冲区复制积压缓冲区是由主节点维护的一个默认大小1M的FIFO队列,备份主节点命令传播阶段的写命令,以及每个字节对应的复制偏移量 服务器运行IDinfo server | grep run_id节点初次复制时,主节点会将自己的run_id发给从节点,用于断线重连的时候判断之前是否有过连接,有过连接的可能可以使用部分复制,否则只能使用全量复制 命令传播阶段数据同步阶段完成后，主从节点进入命令传播阶段。在这个阶段主节点将自己执行的写命令发送给从节点，从节点接收命令并执行，从而保证主从节点数据的一致性.此时主从节点还要维持心跳机制：PING和REPLCONF ACK。心跳机制对于主从复制的超时判断、数据安全等有作用。 SpringBoot集成redisredis配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879bind 127.0.0.1 #指定监听的本机网卡?protected-mode yes #是否开启保护模式port 6379 #指定端口号tcp-backlog 511# unixsocket /tmp/redis.sock# unixsocketperm 700timeout 0 #客户端超时关闭连接时间,0代表无限时间tcp-keepalive 300 #非0表示开启长连接,并指定发送ACKs的间隔,来探测daemonize no #是否后台运行pidfile /var/run/redis_6379.pid #指定pid文件的路径,不指定的话,默认在/var/run/redis.pidloglevel notice #server日志级别,有debug(开发测试环境),verbose,notice(生产环境),warninglogfile "" #日志生成路径和文件名,设置成/dev/null可以屏蔽日志databases 16 #可以理解成数据库的数量,0~15save 900 1 # 15分钟内至少一个key发生变更,会触发snapshot,添加 save "" 屏蔽所有配置,即不进行rdb备份save 300 10save 60 10000stop-writes-on-bgsave-error yesrdbcompression yes #是否开启rdb文件压缩rdbchecksum yes #是否开启校验和dbfilename dump.rdb #备份文件的路径和文件名,默认在/var/lib/redis/dump.rdbdir ./ #指定rdb和aof文件的存放路径,只能是路径# slaveof &lt;masterip&gt; &lt;masterport&gt; #定义从redis# masterauth &lt;master-password&gt; # 连接主redis的密码slave-serve-stale-data yes #当前redis是slave时,当与主redis失去通信,是否继续提供服务slave-read-only yes #从redis只读slave-priority 100## slave-announce-ip 5.5.5.5# slave-announce-port 1234# requirepass foobared #设置密码,设置成""就是没有密码# maxclients 10000 #最大客户端连接数量# maxmemory &lt;bytes&gt; #设置最大缓存大小,超过后会先清理内存,然后还不行,接下来只接受读请求,拒绝写请求## volatile-lru -&gt; 根据lru移除设置了过期时间的key# allkeys-lru -&gt; 根据lru移除所有的# volatile-lfu -&gt; 根据lfu移除设置过期时间的# allkeys-lfu -&gt; 根据lfu移除所有的# volatile-random -&gt; 随机移除设置了过期时间的# allkeys-random -&gt; 随机移除任何的# volatile-ttl -&gt; 移除即将过期的# noeviction -&gt; 不移除,返回写错误## maxmemory-policy noevictionlazyfree-lazy-eviction nolazyfree-lazy-expire nolazyfree-lazy-server-del noslave-lazy-flush noappendonly no #是否开启aof持久化模式,默认不开启appendfilename "appendonly.aof" #aof持久化备份文件名# 三种策略# no: don't fsync, just let the OS flush the data when it wants. Faster.# always: fsync after every write to the append only log. Slow, Safest.# everysec: fsync only one time every second. Compromise.#appendfsync everysec #定义写操作备份到硬盘的频率no-appendfsync-on-rewrite no #是否在重写aof文件的时候关闭aof文件的命令追加同步,因为重写子线程和追加主线程可能会冲突引起阻塞auto-aof-rewrite-percentage 100 #当aof文件超过多少百分比(和最近一次重写后的aof文件大小比较)进行重写,设置为0可以关闭重写功能,auto-aof-rewrite-min-size 64mb #指定需要重写的最小aof文件大小aof-load-truncated yesaof-use-rdb-preamble no# cluster-enabled yes# cluster-config-file nodes-6379.conf# cluster-node-timeout 15000# cluster-slave-validity-factor 10# cluster-migration-barrier 1# cluster-require-full-coverage yes# cluster-slave-no-failover no# cluster-announce-ip 10.1.1.5# cluster-announce-port 6379# cluster-announce-bus-port 6380]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>redis</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx入门]]></title>
    <url>%2F2018%2F09%2F09%2Fnginx%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[这是摘要 配置详解nginx配置详解 反向代理简单说就是通过代理服务器,对客户端的访问隐藏了真正的服务器.真实的服务器不能直接被外部网络访问，所以需要一台代理服务器，而代理服务器能被外部网络访问的同时又跟真实服务器在同一个网络环境，当然也可能是同一台服务器，端口不同而已。 nginx简单配置12345678910server &#123; listen 80; server_name localhost; client_max_body_size 1024M; location / &#123; proxy_pass http://localhost:8080; proxy_set_header Host $host:$server_port; &#125; &#125; 以上配置,访问localhost相当于访问localhost:8080 负载均衡自带的三种策略 RR(默认)按照时间顺序(轮询)将请求分配到后端服务器,如果后端服务器down掉了,会自动剔除1234567891011121314upstream test &#123; server localhost:8080; server localhost:8081;&#125;server &#123; listen 81; server_name localhost; client_max_body_size 1024M; location / &#123; proxy_pass http://test; proxy_set_header Host $host:$server_port; &#125;&#125; 这里配置了2台服务器，当然实际上是一台，只是端口不一样而已，而8081的服务器是不存在的,也就是说访问不到，但是我们访问http://localhost 的时候,也不会有问题，会默认跳转到http://localhost:8080 具体是因为Nginx会自动判断服务器的状态，如果服务器处于不能访问（服务器挂了），就不会跳转到这台服务器，所以也避免了一台服务器挂了影响使用的情况，由于Nginx默认是RR策略，所以我们不需要其他更多的设置。 参考配置nginx实现请求的负载均衡 + keepalived实现nginx的高可用 https://www.cnblogs.com/youzhibing/p/7327342.html LVS + keepalived + nginx + tomcat 实现主从热备 + 负载均衡 http://www.cnblogs.com/youzhibing/p/5061786.html 主从热备+负载均衡（LVS + keepalived） http://www.cnblogs.com/youzhibing/p/5021224.html 权重按照分配的权重转发请求 1234upstream test &#123; server localhost:8080 weight=9; server localhost:8081 weight=1; &#125; ip_hash 12345upstream test &#123; ip_hash; server localhost:8080; server localhost:8081;&#125; 热备当localhos:8080发生故障时,才会用8081 12345upstream test &#123; ip_hash; server localhost:8080; server localhost:8081 backup;&#125; 使用默认和权重方式会有一个问题,就是应用程序有时候不是无状态的,比如使用session保存登录信息,就需要连续访问同样的服务器才行,这个时候可以是使用ip_hash转发请求 第三方策略 fair12345upstream backend &#123; fair; server localhost:8080; server localhost:8081; &#125; 根据后端服务器响应的时间选择 url_hash按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法123456upstream backend &#123; hash $request_uri; hash_method crc32; server localhost:8080; server localhost:8081;&#125; 动静分离动静分离是让动态网站里的动态网页根据一定规则把不变的资源和经常变的资源区分开来，动静资源做好了拆分以后，我们就可以根据静态资源的特点将其做缓存操作，这就是网站静态化处理的核心思路1234567891011121314151617181920212223242526272829upstream test&#123; server localhost:8080; server localhost:8081; &#125; server &#123; listen 80; server_name localhost; location / &#123; root e:\wwwroot; index index.html; &#125; # 所有静态请求都由nginx处理 location ~ \.(gif|jpg|jpeg|png|bmp|swf|css|js)$ &#123; root e:\wwwroot; &#125; # 所有动态请求都转发给tomcat处理 location ~ \.(jsp|do)$ &#123; proxy_pass http://test; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root e:\wwwroot; &#125; &#125; 这样我们就可以吧HTML以及图片和css以及js放到wwwroot目录下，而tomcat只负责处理jsp和请求，例如当我们后缀为gif的时候，Nginx默认会从wwwroot获取到当前请求的动态图文件返回，当然这里的静态文件跟Nginx是同一台服务器，我们也可以在另外一台服务器，然后通过反向代理和负载均衡配置过去就好了，只要搞清楚了最基本的流程，很多配置就很简单了，另外localtion后面其实是一个正则表达式，所以非常灵活 nginx命令1234567/etc/init.d/nginx start/restart # 启动/重启Nginx服务/etc/init.d/nginx stop # 停止Nginx服务/etc/nginx/nginx.conf # Nginx配置文件位置nginx -s reload # 重新加载配置文件]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>反向代理</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql基础]]></title>
    <url>%2F2018%2F09%2F02%2Fsql%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[这是摘要 sql执行顺序 SQL编译基本过程 直接写一堆的sql来进行数据的处理，和用一个存储过程来进行数据的处理，哪个性能更好一些。存储过程是在数据库中已经编译过的,直接写sql还要先编译,效率会差一些 sql语法嵌套查询123456#查询“001”课程比“002”课程成绩高的所有学生的学号；Select a.Sno from (select Sno,score from SC where Cno='001') a,(select Sno,score from SC where Cno='002') bWhere a.Sno=b.Sno and a.score&gt;b.score; 1参考地址:https://my.oschina.net/startphp/blog/123337 12 12 函数 group by GROUP BY 语句用于结合聚合函数，根据一个或多个列对结果集进行分组。 1234# 求学生平均成绩SELECT id, AVG(score)FROM studentGROUP BY id; 123# 统计各个site_id的访问次数SELECT site_id, SUM(access_log.count) AS numsFROM access_log GROUP BY site_id; 123456# 统计各个网站访问数量SELECT Websites.name,COUNT(access_log.aid) AS nums FROM access_logLEFT JOIN WebsitesON access_log.site_id=Websites.idGROUP BY Websites.name; having在 SQL 中增加 HAVING 子句原因是，WHERE 关键字无法与聚合函数一起使用。HAVING 子句可以让我们筛选分组后的各组数据。1234567# 查找总访问量大于 200 的网站。SELECT Websites.name, Websites.url, SUM(access_log.count) AS nums FROM (access_logINNER JOIN WebsitesON access_log.site_id=Websites.id)GROUP BY Websites.nameHAVING SUM(access_log.count) &gt; 200; 1234567# 查找总访问量大于 200 的网站，并且 alexa 排名小于 200。SELECT Websites.name, SUM(access_log.count) AS nums FROM WebsitesINNER JOIN access_logON Websites.id=access_log.site_id AND Websites.alexa &lt; 200 #WHERE Websites.alexa &lt; 200 GROUP BY Websites.nameHAVING SUM(access_log.count) &gt; 200; 基础语法 distinct 12SELECT DISTINCT column_name,column_nameFROM table_name; limit 123SELECT column_name(s)FROM table_nameLIMIT number; order by 123SELECT column_name,column_nameFROM table_nameORDER BY column_name,column_name ASC|DESC; sql通配符123456# MySQL支持% : 代表0个或者多个字符_ : 代表1个字符# MySQL不支持[abc] : a,b,c中的一个字符[!abc]或者[^abc] : 不是a,b,c中的字符 高级语法 like 123#返回G开头的 SELECT * FROM WebsitesWHERE name LIKE 'G%'; regexp 123#正则表达式,选取G或者F或者s开头的SELECT * FROM WebsitesWHERE name REGEXP '^[GFs]'; between和in 123SELECT * FROM WebsitesWHERE (alexa BETWEEN 1 AND 20)AND NOT country IN ('USA', 'IND'); 别名 123#列别名SELECT name, CONCAT(url, ', ', alexa, ', ', country) AS site_infoFROM Websites; 1234#表别名SELECT w.name, w.url, a.count, a.date FROM Websites AS w, access_log AS a WHERE a.site_id=w.id and w.name="菜鸟教程"; inner join(简写成join)INNER JOIN 关键字在表中存在至少一个匹配时返回行。如果 “Websites” 表中的行在 “access_log” 中没有匹配，则不会列出这些行。 1234SELECT Websites.id, Websites.name, access_log.count, access_log.dateFROM WebsitesINNER JOIN access_logON Websites.id=access_log.site_id; left joinLEFT JOIN 关键字从左表（table1）返回所有的行，即使右表（table2）中没有匹配。如果右表中没有匹配，则结果为 NULL。返回的结果数&gt;=左表 1234SELECT column_name(s)FROM table1LEFT JOIN table2ON table1.column_name=table2.column_name; right joinRIGHT JOIN 关键字从右表（table2）返回所有的行，即使左表（table1）中没有匹配。如果左表中没有匹配，则结果为 NULL。返回的结果数&gt;=右表 1234SELECT column_name(s)FROM table1RIGHT JOIN table2ON table1.column_name=table2.column_name; full joinFULL OUTER JOIN 关键字只要左表（table1）和右表（table2）其中一个表中存在匹配，则返回行. 1234SELECT column_name(s)FROM table1FULL OUTER JOIN table2ON table1.column_name=table2.column_name; where和on的关系 参考地址:http://www.runoob.com/w3cnote/sql-join-the-different-of-on-and-where.html union和union allUNION 操作符用于合并两个或多个 SELECT 语句的结果集。UNION 内部的每个 SELECT 语句必须拥有相同数量的列。列也必须拥有相似的数据类型。同时，每个 SELECT 语句中的列的顺序必须相同。UNION 操作符选取不同的值。如果允许重复的值，请使用 UNION ALL。 123SELECT column_name(s) FROM table1UNIONSELECT column_name(s) FROM table2; 12 12 连接的结果数 123inner join &lt;= min(left join, right join)full join &gt;= max(left join, right join)当 inner join &lt; min(left join, right join) 时， full join &gt; max(left join, right join) 约束123456CREATE TABLE Persons( Id_P int NOT NULL PRIMARY KEY, //PRIMARY KEY约束 LastName varchar(255) NOT NULL, FirstName varchar(255)) unique同一张表中可以有多个列为unique创建表时123456CREATE TABLE Persons(P_Id int NOT NULL,LastName varchar(255) NOT NULL,UNIQUE (P_Id)) 修改12345ALTER TABLE PersonsADD UNIQUE (P_Id)ALTER TABLE PersonsDROP INDEX uc_PersonID primary key同一张表中只能有一列1234567CREATE TABLE Persons(P_Id int NOT NULL,LastName varchar(255) NOT NULL,FirstName varchar(255),CONSTRAINT pk_PersonID PRIMARY KEY (P_Id,LastName)) 修改12ALTER TABLE PersonsADD PRIMARY KEY (P_Id) 联合主键12ALTER TABLE PersonsADD CONSTRAINT pk_PersonID PRIMARY KEY (P_Id,LastName) foreign key 1234567891011121314151617CREATE TABLE Orders(O_Id int NOT NULL,OrderNo int NOT NULL,P_Id int,PRIMARY KEY (O_Id),FOREIGN KEY (P_Id) REFERENCES Persons(P_Id))//修改ALTER TABLE OrdersADD FOREIGN KEY (P_Id)REFERENCES Persons(P_Id)//联合外键ALTER TABLE OrdersADD CONSTRAINT fk_PerOrdersFOREIGN KEY (P_Id)REFERENCES Persons(P_Id) check 1234567891011CREATE TABLE Persons(P_Id int NOT NULL,CHECK (P_Id&gt;0))//修改ALTER TABLE PersonsADD CHECK (P_Id&gt;0)//多个ALTER TABLE PersonsADD CONSTRAINT chk_Person CHECK (P_Id&gt;0 AND City='Sandnes') default 12345678CREATE TABLE Persons( P_Id int NOT NULL, City varchar(255) DEFAULT 'Sandnes')//修改ALTER TABLE PersonsALTER City SET DEFAULT 'SANDNES']]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统]]></title>
    <url>%2F2018%2F09%2F01%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[这是摘要 缓存什么是缓存有哪些算法 参考地址:https://www.cnblogs.com/dolphin0520/p/3749259.html FIFO 概念FIFO(First in First out)，先进先出。在操作系统的设计理念中很多地方都利用到了先进先出的思想，比如作业调度（先来先服务）,使用数据结构中的队列即可实现。在FIFO Cache设计中，核心原则就是：如果一个数据最先进入缓存中，则应该最早淘汰掉。也就是说，当缓存满的时候，应当把最先进入缓存的数据给淘汰掉。 Cache中应该支持以下操作; get(key)：如果Cache中存在该key，则返回对应的value值，否则，返回-1； set(key,value)：如果Cache中存在该key，则重置value值；如果不存在该key，则将该key插入到到Cache中，若Cache已满，则淘汰最早进入Cache的数据。 实现使用map和队列 LFU 概念LFU（Least Frequently Used）最近最少使用算法。它是基于“如果一个数据在最近一段时间内使用次数很少，那么在将来一段时间内被使用的可能性也很小”的思路。LFU是基于访问次数的。 LFU Cache应该支持的操作为： get(key)：如果Cache中存在该key，则返回对应的value值，否则，返回-1； set(key,value)：如果Cache中存在该key，则重置value值；如果不存在该key，则将该key插入到到Cache中，若Cache已满，则淘汰最少访问的数据。 实现 LRU 概念LRU全称是Least Recently Used，即最近最久未使用的意思。LRU算法的设计原则是：如果一个数据在最近一段时间没有被访问到，那么在将来它被访问的可能性也很小。也就是说，当限定的空间已存满数据时，应当把最久没有被访问到的数据淘汰。 具备的操作： set(key,value)：如果key在hashmap中存在，则先重置对应的value值，然后获取对应的节点cur，将cur节点从链表删除，并移动到链表的头部；若果key在hashmap不存在，则新建一个节点，并将节点放到链表的头部。当Cache存满的时候，将链表最后一个节点删除即可。 get(key)：如果key在hashmap中存在，则把对应的节点放到链表头部，并返回对应的value值；如果不存在，则返回-1。 实现使用双向链表和map实现]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务]]></title>
    <url>%2F2018%2F08%2F29%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[这是摘要 微服务架构简单的说，微服务架构就是将一个完整的应用从数据存储开始垂直拆分成多个不同的服务，每个服务都能独立部署、独立维护、独立扩展，服务与服务间通过诸如RESTful API的方式互相调用。涉及到的技术:负载均衡,反向代理,服务治理(发现和注册),消息总线,配置管理,断路器,数据监控等. 优点 将一个庞大的一体式应用拆分成多个服务,单个服务更容易开发,理解和维护 对于单个服务,负责开发的团队可以选择最合适的开发技术 单个服务的独立部署,可以选择最合适的部署平台,并且开发者不用协调其他服务对本服务的影响,加快部署速度. 缺点 微服务通常是分布式系统,需要额外实现代码来保证进程之间的通信 分布式数据库的一致性是个难题,(涉及到CAP理论) 整个系统(也就是多服务)测试更加麻烦 整个系统的自动化部署(服务多了以后不可能一个个手工部署),监控,调度,架构设计也是难点 springCloudSpring Cloud是一个基于Spring Boot实现的云应用开发工具，它为基于JVM的云应用开发中的配置管理、服务发现、断路器、智能路由、微代理、控制总线、全局锁、决策竞选、分布式会话和集群状态管理等操作提供了一种简单的开发方式。Spring Cloud包含了多个子项目（针对分布式系统中涉及的多个不同开源产品），比如：Spring Cloud Config、Spring Cloud Netflix、Spring Cloud CloudFoundry、Spring Cloud AWS、Spring Cloud Security、Spring Cloud Commons、Spring Cloud Zookeeper、Spring Cloud CLI等项目。 Spring Cloud组件及功能介绍 参考地址:https://blog.csdn.net/tudou201601/article/details/79123912 各个组件解决的问题 eureka用于服务的注册和发现,ribbon则是来消费服务 hystrix是优化ribbon,防止一个服务节点的故障,导致整个系统的崩溃,起到保险丝的作用 zuul是整个微服务系统的防火墙和代理,反向代理隐藏微服务节点 config是为了解决分布式微服务同意配置问题 bus是为了解决修改服务节点配置之后的刷新问题,交给bus来通知服务节点更新配置 feign是为了简化服务之间的请求代码 stream是为了简化对MQ的操作,达到MQ和应用程序的松耦合 Sleuth是因为单次请求在微服务节点中跳转无法追溯，解决任务链日志追踪问题的 spring-boot-actuator(系统健康监测)actuator是springboot中一个用来做系统健康检测的一个模块，它提供一个resetful的api接口，可以将系统运行过程中的磁盘空间、线程数、以及程序连接的数据库情况通过json返回，然后再结合预警、监控模块进行实时系统监控。 1234567//配置management.security.enabled=false//pom&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 服务发现和注册spring-cloud-eureka(云端服务的注册与发现) 参考地址:https://zhuanlan.zhihu.com/p/25141622 注册:每个服务单元向注册中心登记自己提供的服务，将主机与端口号，版本号，通讯协议等一些附加信息告知注册中心，注册中心按服务名分类组织服务清单。另外，服务中心还需要以心跳的方式去检测清单中的服务是否可用，如不可用，需要从服务清单中删除，达到删除故障服务的效果。 发现:由于在服务治理下运作，服务间的调用不再采用通过具体的实例地址来实现，而是通过向服务服务名发起请求调用实现（ribbon）,服务注册中心返回服务名对应的所有的实例清单。 1234567891011//服务注册中心&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka-server&lt;/artifactId&gt;&lt;/dependency&gt; //服务提供方&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt;&lt;/dependency&gt; 服务网关spring-cloud-zuul(边缘服务框架)Zuul 是在云平台上提供动态路由,监控,弹性,安全等边缘服务的框架。Zuul 相当于是设备和 Netflix 流应用的 Web 网站后端所有请求的前门。 过滤器 请求路由 动态过滤器 动态路由 客户端负载均衡spring-cloud-ribbon(云端负载均衡)提供云端负载均衡，有多种负载均衡策略可供选择，可配合服务发现和断路器使用。 负载均衡策略 RandomRule从实例列表随机选取。选择逻辑在一个while(server == null)循环之内，正常情况下都应该选出一个服务示例，如果出现死循环获取不到服务实例，则有可能存在并发的Bug RoundRobinRule（默认）按照线性轮循一次选择每个实例。随机次数超过十次，结束尝试 RetryRule内部还定义了一个IRule，默认RoundRobinRule。在指定时间内反复尝试，超时返回null weightedResponseTimeRule基于RoundRobinRule增加了根据实例的运行情况来计算权重，然后根据权重挑选实例。主要有三个核心内容： 定时任务每30s执行一次权重计算 权重计算记录每个实例的统计信息，累加所有实例的平均响应时间，得到总平均响应时间totalResponseTime.循环所有的实例计算其权重，weightSoFar+=totalResponseTime - 实例平均响应时间，其中weightSoFar初始化值为0。每个实例权重结果保存到ArrayList currentWeights中。 实例选择判断最后一个实例的权重是否&lt; 0.001d，若小于，采用RoundRobinRule策略生成一个[0,最大权重值)区间内的随机数。遍历权重清单currentWeights，若权重大于等于随机的得到的数值，就选择这个实例 客户端容错保护spring-cloud-hystrix(熔断器)熔断器，容错管理工具，旨在通过熔断机制控制服务和第三方库的节点,从而对延迟和故障提供更强大的容错能力。在服务单元发生故障后,向调用方返回一个错误响应,而不是长时间的等待,hystrix具备以下功能: Hystrix具备： 服务降级 服务熔断 请求总数（默认20） 错误百分比（默认50） 线程和信号隔离 请求缓存 请求合并 服务监控 集群监控（Turbine） spring-cloud-Feign(http客户端,声明式服务调用)本质上就是Ribbon+HystrixSpring Cloud Feign 在RestTemplate的基础上作了进一步的封装，由它来帮助我们定义和实现依赖服务接口的定义。我们只需要创建一个接口并用注解的方式来配置它。微服务之间的调用本质还是http请求，如果对于每个请求都需要写请求代码，增加请求参数，同时对请求结果做处理，就会存在大量重复工作，而feign非常优雅的帮助我们解决了这个问题，只需要定义一个interface，fegin就知道http请求的时候参数应该如何设置。 分布式配置管理结合config和bus可以实现动态配置 spring-cloud-config(配置管理)配置文件内容123456789#git仓库uri地址spring.cloud.config.server.git.uri=https://github.com/super-potato/spring-cloud-config.git#仓库路径spring.cloud.config.server.git.searchPaths=springcloudconfig#仓库分支spring.cloud.config.label=master#仓库用户名密码spring.cloud.config.server.git.username=super-potatospring.cloud.config.server.git.password= http请求地址和资源文件映射 spring-cloud-bus(数据总线)事件、消息总线，用于在集群（例如，配置变化事件）中传播状态变化，可与Spring Cloud Config联合实现热部署。消息代理是一种消息验证、传输、路由的架构模式。它在应用程序之间起到通信调度并最小化应用之间的依赖的作用，使得应用程序可以高效地解耦通信过程。 应用场景 将消息路由到一个或多个目的地 消息转换为其他的表现方式 执行消息聚焦、消息的分解，并将结果发送到其他的目的地，然后重新组合相应返回给消息用户。 调用web服务来检索数据 响应错误事件 使用发布-订阅模式来提供内容或基于主题的消息路由 支持的中间件 Kafka RabbitMQ spring-cloud-stream消息驱动微服务。数据流操作开发包，封装了与Redis,Rabbit、Kafka等发送接收消息。 spring-cloud-sleuth日志收集工具包，封装了Dapper和log-based追踪以及Zipkin和HTrace操作，为SpringCloud应用实现了一种分布式追踪解决方案。 spring-cloud-zookeeper操作Zookeeper的工具包，用于使用zookeeper方式的服务发现和配置管理。 spring-cloud-CLI基于 Spring Boot CLI，可以让你以命令行方式快速建立云组件。 spring-docker使用docker构建微服务 常见启动错误 不能在默认包下启动@SpringBootApplication报错信息:Your ApplicationContext is unlikely to start due to a @ComponentScan of the default package.]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>springBoot</tag>
        <tag>springCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka入门]]></title>
    <url>%2F2018%2F08%2F26%2Fkafka%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[这是摘要 入门教程入门搭建使用参考地址:https://blog.csdn.net/csolo/article/details/52389646 一些抽象概念 topic,partition,offset topic是对一组消息的归纳,kafka对每一个topic的日志进行分区(partition)每个分区(partition)都由一系列有序的、不可变的消息组成，这些消息被连续的追加到分区中。分区中的每个消息都有一个连续的序列号叫做offset,用来在分区中唯一的标识这个消息。 在一个可配置的时间段内，Kafka集群保留所有发布的消息，不管这些消息有没有被消费。比如，如果消息的保存策略被设置为2天，那么在一个消息被发布的两天时间内，它都是可以被消费的。之后它将被丢弃以释放空间。Kafka的性能是和数据量无关的常量级的，所以保留太多的数据并不是问题。 实际上每个consumer唯一需要维护的数据是消息在日志中的位置，也就是offset.这个offset有consumer来维护：一般情况下随着consumer不断的读取消息，这offset的值不断增加，但其实consumer可以以任意的顺序读取消息，比如它可以将offset设置成为一个旧的值来重读之前的消息以上特点的结合，使Kafka consumers非常的轻量级：它们可以在不对集群和其他consumer造成影响的情况下读取消息。可以使用命令行来”tail”消息而不会对其他正在消费消息的consumer造成影响。将日志分区可以达到以下目的： 首先这使得每个日志的数量不会太大，可以在单个服务上保存。 另外每个分区可以单独发布和消费，为并发操作topic提供了一种可能。 实现高吞吐量原理 顺序读写kafka的消息是不断追加到文件,这个特性充分利用了磁盘的顺序读写性能,远远快于随机读写,磁盘IO效率高 零拷贝先简单了解下文件系统的操作流程，例如一个程序要把文件内容发送到网络,这个程序是工作在用户空间，文件和网络socket属于硬件资源，两者之间有一个内核空间在操作系统内部，整个过程为： 在Linux kernel2.2 之后出现了一种叫做”零拷贝(zero-copy)”系统调用机制，就是跳过“用户缓冲区”的拷贝，建立一个磁盘空间和内存的直接映射，数据不再复制到“用户态缓冲区”系统上下文切换减少为2次，可以提升一倍的性能 文件分段kafka的队列topic被分为了多个区partition，每个partition又分为多个段segment，所以一个队列中的消息实际上是保存在N多个片段文件中通过分段的方式，每次文件操作都是对一个小文件的操作，非常轻便，同时也增加了并行处理能力 批量发送Kafka允许进行批量发送消息，先将消息缓存在内存中，然后一次请求批量发送出去比如可以指定缓存的消息达到某个量的时候就发出去，或者缓存了固定的时间后就发送出去如100条消息就发送，或者每5秒发送一次这种策略将大大减少服务端的I/O次数 数据压缩Kafka还支持对消息集合进行压缩，Producer可以通过GZIP或Snappy格式对消息集合进行压缩压缩的好处就是减少传输的数据量，减轻对网络传输的压力Producer压缩之后，在Consumer需进行解压，虽然增加了CPU的工作，但在对大数据处理上，瓶颈在网络上而不是CPU，所以这个成本很值得]]></content>
      <categories>
        <category>开源框架</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>消息中间件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[异常处理]]></title>
    <url>%2F2018%2F08%2F22%2F%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[参考地址:http://www.hollischuang.com/archives/430 异常类型 Checked exception必须捕获处理,不然编译无法通过,如IOException,SQLException这类无法保证一定不会抛出的异常 Unchecked exception这类异常都是RuntimeException的子类,在运行期间可能会抛出的但是完全可以避免的异常,不捕获编译也能通过,因此应该从代码上避免此类异常如NullPointerException,IndexOutOfBoundException Error无法处理的异常,如OutOfMemoryError,StackOverflowError等 异常使用的原则 参考地址:https://www.cnblogs.com/skywang12345/p/3544287.html 只针对不正常的情况才使用异常比如不要使用捕获异常来结束数组的遍历 异常机制的初衷是为了针对不正常的情况,JVM很少对异常捕获进行优化,异常的创建,抛出和捕获的开销很大 把代码放在try-catch中返回阻止了JVM实现本来可能要执行的某些特定的优化。(?) 对数组进行遍历的标准模式并不会导致冗余的检查，有些现代的JVM实现会将它们优化掉。(?) 对于可恢复的条件使用checked exception,对于程序错误使用runtime exception 受检异常可以通过处理使得程序恢复正常运行 程序错误应该通过修改程序代码来避免错误的发生,而不是通过开销很大的异常机制 避免不必要的使用checked exception虽然受检异常能够提高程序的可靠性,但是过分使用被检查异常会使API用起来非常不方便。如果一个方法抛出一个或多个被检查的异常，那么调用该方法的代码则必须在一个或多个catch语句块中处理这些异常，或者必须通过throws声明抛出这些异常。 无论是通过catch处理，还是通过throws声明抛出，都给程序员添加了不可忽略的负担。适用于”被检查的异常”必须同时满足两个条件： 即使正确使用API并不能阻止异常条件的发生。 一旦产生了异常，使用API的程序员可以采取有用的动作对程序进行处理。 尽量使用标准异常 这符合代码复用的原则 异常类越少，意味着内存占用越小，并且转载这些类的时间开销也越小。 抛出的异常要适合于相应的抽象1234567public E get(int index) &#123; try &#123; return listIterator(index).next(); &#125; catch (NoSuchElementException exc) &#123; throw new IndexOutOfBoundsException("Index: "+index); &#125;&#125; listIterator(index)会返回ListIterator对象，调用该对象的next()方法可能会抛出NoSuchElementException异常。而在get()方法中，抛出NoSuchElementException异常会让人感到困惑。所以，get()对NoSuchElementException进行了捕获，并抛出了IndexOutOfBoundsException异常。即，相当于将NoSuchElementException转译成了IndexOutOfBoundsException异常。 努力使失败保持原子性一般而言，一个失败的方法调用应该保持使对象保持在”它在被调用之前的状态”。因为调用者通常期望从被检查的异常中恢复过来。 不要忽略异常 在finally中return 千万不要在finally块中使用return，因为finally中的return会覆盖已有的返回值。 如果在finally中有return语句,那么即使在catch中throw异常,上一级也捕获不到抛出的异常 在catch中,return和throw只能使用其中一个]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>异常</tag>
        <tag>exception</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重点复习目录]]></title>
    <url>%2F2018%2F08%2F20%2F%E9%87%8D%E7%82%B9%E5%A4%8D%E4%B9%A0%E7%9B%AE%E5%BD%95%2F</url>
    <content type="text"><![CDATA[这是摘要 springMVCSpringMVC 反射反射 设计模式代理模式 单例模式 常用集合类常用集合类 5层网络模型5层网络模型 Java虚拟机Java虚拟机 Java线程的使用Java线程的使用 线程池的使用线程池的使用 Java中的锁java中的锁 异常处理异常处理 concurrent包常用类concurrent包常用类 http协议http协议 计算机网络计算机网络 tcp和udptcp和udp JDK8新特性JDK8新特性 数据库索引原理数据库索引原理 数据库事务数据库事务 分库分表分库分表 开源框架拓展redis入门 kafka入门 nginx入门 微服务]]></content>
      <categories>
        <category>大纲</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[分库分表]]></title>
    <url>%2F2018%2F08%2F20%2F%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[参考地址:http://www.infoq.com/cn/articles/key-steps-and-likely-problems-of-split-table 垂直分表就是大表拆成小表,一张表字段过多可以考虑拆分成几张表,将不经常使用或者长度较大的字段拆分出去放到“扩展表”中 优点在字段很多的情况下，拆分开确实更便于开发和维护。某种意义上也能避免“跨页”的问题 缺点拆分字段的操作建议在数据库设计阶段就做好。如果是在发展过程中拆分，则需要改写以前的查询语句，会额外带来一定的成本和风险，建议谨慎。 垂直分库基本的思路就是按照业务模块来划分出不同的数据库，而不是像早期一样将所有的数据表都放到同一个数据库中。 优点系统层面的“服务化”拆分操作，能够解决业务系统层面的耦合和性能瓶颈，有利于系统的扩展维护。而数据库层面的拆分，道理也是相通的。与服务的“治理”和“降级”机制类似，我们也能对不同业务类型的数据进行“分级”管理、维护、监控、扩展等。数据库的连接资源比较宝贵且单机处理能力也有限，在高并发场景下，垂直分库一定程度上能够突破IO、连接数及单机硬件资源的瓶颈，是大型分布式系统中优化数据库架构的重要手段。 缺点跨库join，分布式事务等 难点解决思路 垮库join 字段冗余:可以不用去查询另一张表,空间换时间 简单字段,系统层组装在代码中调用不同模块的服务获取数据后进行组装 复杂字段尽量将数据通过条件过滤之后再返回进行组装 跨库事务（分布式事务）的问题 水平分表就是将表中不同的数据行按照一定规律分布到不同的数据库表中（这些表保存在同一个数据库中），这样来降低单表数据量，优化查询性能。最常见的方式就是通过主键或者时间等字段进行Hash和取模后拆分。 优点水平分表，能够降低单表的数据量，一定程度上可以缓解查询性能瓶颈。 缺点但本质上这些表还保存在同一个库中，所以库级别还是会有IO瓶颈。所以，一般不建议采用这种做法。 水平分库分表水平分库分表与上面讲到的水平分表的思想相同，唯一不同的就是将这些拆分出来的表保存在不同的数据库中。 参考地址:http://www.infoq.com/cn/articles/key-steps-and-likely-problems-of-horizontal-split-table]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>分库分表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[concurrent包常用类]]></title>
    <url>%2F2018%2F08%2F19%2Fconcurrent%E5%8C%85%E5%B8%B8%E7%94%A8%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[参考地址:https://my.oschina.net/yu120/blog/689204 ConcurrentHashmap(K/V not null)参考地址(更完整):https://blog.csdn.net/u010412719/article/details/52145145https://www.jianshu.com/p/e694f1e868ec 1.8中的实现 使用Node+CAS+synchronized实现sizeCtl是控制标识符，不同的值表示不同的意义。 负数代表正在进行初始化或扩容操作 ,其中-1代表正在初始化 ,-N 表示有N-1个线程正在进行扩容操作 正数或0代表hash表还没有被初始化，这个数值表示初始化或下一次进行扩容的大小，类似于扩容阈值。它的值始终是当前ConcurrentHashMap容量的0.75倍，这与loadfactor是对应的。实际容量&gt;=sizeCtl，则扩容。 put的实现大致流程 检查key/value是否为空，如果为空，则抛异常，否则进行2 进入for死循环，进行3 检查table是否初始化了，如果没有，则调用initTable()进行初始化然后进行 2，否则进行4 根据key的hash值计算出其应该在table中储存的位置i，取出table[i]的节点用f表示。根据f的不同有如下三种情况： 如果table[i]==null(即该位置的节点为空，没有发生碰撞)，则利用CAS操作直接存储在该位置，如果CAS操作成功则退出死循环。 如果table[i]!=null(即该位置已经有其它节点，发生碰撞)，碰撞处理也有两种情况 检查table[i]的节点的hash是否等于MOVED，如果等于，则检测到正在扩容，则帮助其扩容 说明table[i]的节点的hash值不等于MOVED，如果table[i]为链表节点，则将此节点插入链表中即可如果table[i]为树节点，则将此节点插入树中即可。插入成功后，进行 5 如果table[i]的节点是链表节点，则检查table的第i个位置的链表是否需要转化为数，如果需要则调用treeifyBin函数进行转化 1.7中的实现使用HashEntry和Segment(继承ReentrantLock)实现 ReentrantLock 锁的实现:参考地址:https://www.jianshu.com/p/fe027772e156ReentrantLock基于volatile int state和cas实现,尝试获得锁的时候会判断state的值, 如果是0的话,使用cas算法修改state加1 不是0的话说明锁已经被占用,使用cas加1,进入等待队列ReentrantLock是JDK实现的,synchronized是依赖于JVM实现的 性能的区别:JDK1.6之后(synchronized引入了偏向锁,轻量级锁(自旋锁)之后)synchronized优化的与ReentrantLock性能差不多,在两种方法都可以使用的情况下官方建议使用synchronized,都是试图在用户态就将加锁问题解决,避免进入内核态 功能的区别:synchronized的使用比较简洁,由编译器保证加锁和施放锁,而ReentrantLock需要手工声明加锁和施放锁,但是ReentrantLock在锁的细粒度和灵活性方面更强 ReentrantLock可以指定公平锁(FIFO)和非公平锁(默认是这个),synchronized只能是非公平锁(一个线程想要获得锁,会先尝试插队) ReentrantLock提供了一个Condition类来实现按照满足条件分组唤醒线程,而synchronized只能是随机唤醒一个线程,要么全部唤醒 ReentrantLock提供了一个可以中断等待锁的机制,通过lock.lockInterruptibly()来实现这个机制 Lock实现了可轮询锁,使用if(lock.tryLock())可以尝试获得锁,如果获取不到则else 使用方法12345678910111213141516class SafeSeqWithLock&#123; private long count = 0; private ReentrantLock lock = new ReentrantLock(); public void inc() &#123; lock.lock(); try&#123; count++; &#125; finally&#123; lock.unlock(); &#125; &#125; public long get() &#123; return count; &#125; &#125; Condition参考地址:https://blog.csdn.net/vernonzheng/article/details/8288251生产者消费者模型:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class ProductQueue&lt;T&gt; &#123; private final T[] items; private final Lock lock = new ReentrantLock(); private Condition notFull = lock.newCondition(); private Condition notEmpty = lock.newCondition(); private int head, tail, count; public ProductQueue(int maxSize) &#123; items = (T[]) new Object[maxSize]; &#125; public ProductQueue() &#123; this(10); &#125; public void put(T t) throws InterruptedException &#123; lock.lock(); try &#123; while (count == getCapacity()) &#123; //队列满了就挂起,等待notFull.signal()唤醒 //此时会释放锁,等到唤醒是再次获得锁 notFull.await(); &#125; items[tail] = t; if (++tail == getCapacity()) &#123; tail = 0; &#125; ++count; notEmpty.signalAll(); //队列不空,唤醒之前调用take()但是因为队列空而挂起的线程 &#125; finally &#123; lock.unlock(); &#125; &#125; public T take() throws InterruptedException &#123; lock.lock(); try &#123; while (count == 0) &#123; notEmpty.await(); //如果队列空,挂起线程 &#125; T ret = items[head]; items[head] = null;//GC // if (++head == getCapacity()) &#123; head = 0; &#125; --count; notFull.signalAll(); //唤醒挂起的线程 return ret; &#125; finally &#123; lock.unlock(); &#125; &#125; public int getCapacity() &#123; return items.length; &#125; public int size() &#123; lock.lock(); try &#123; return count; &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; ReentrantReadWriteLock读锁可以有很多个锁同时上锁，只要当前没有写锁写锁是排他的，上了写锁，其他线程既不能上读锁，也不能上写锁；同样，需要上写锁的前提是既没有读锁，也没有写锁； Atomic线程安全的并且无阻塞的类 AtomicLong在多线程操作的时候不需要额外的同步处理1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class AtomicTest &#123; public void testAtomic() &#123; final int loopcount = 10000; int threadcount = 10; final NonSafeSeq seq1 = new NonSafeSeq(); final SafeSeq seq2 = new SafeSeq(); final CountDownLatch l = new CountDownLatch(threadcount); for (int i = 0; i &lt; threadcount; ++i) &#123; final int index = i; new Thread(() -&gt; &#123; for (int j = 0; j &lt; loopcount; ++j) &#123; seq1.inc(); seq2.inc(); &#125; System.out.println("finished : " + index); l.countDown(); &#125;).start(); &#125; try &#123; l.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("both have finished...."); System.out.println("NonSafeSeq:" + seq1.get()); System.out.println("SafeSeq with atomic: " + seq2.get()); &#125;&#125;class NonSafeSeq &#123; private long count = 0; public void inc() &#123;count++;&#125; public long get() &#123;return count;&#125;&#125;class SafeSeq &#123; private AtomicLong count = new AtomicLong(0); public void inc() &#123;count.incrementAndGet();&#125; public long get() &#123;return count.longValue();&#125;&#125; CAS算法参考地址:https://www.jianshu.com/p/21be831e851eCompare and Swap，即比较再交换。就是java中一种乐观锁机制,实际上是一种无锁算法 基本原理 因为t1和t2线程都同时去访问同一变量56，所以他们会把主内存的值完全拷贝一份到自己的工作内存空间，所以t1和t2线程的预期值都为56。假设t1在与t2线程竞争中线程t1能去更新变量的值，而其他线程都失败。（失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次发起尝试）。t1线程去更新变量值改为57，然后写到内存中。此时对于t2来说，内存值变为了57，与预期值56不一致，就操作失败了（想改的值不再是原来的值）。CPU去更新一个值，但如果想改的值不再是原来的值，操作就失败，因为很明显，有其它操作先改变了这个值。 ABA问题比如一个栈中有a,b,c三个元素,现在栈顶是a, 在cas模式下,线程1打算将栈顶变成b, 这时候线程2将a,b弹出,再将a压入, 线程1发现栈顶还是a,就将栈顶变成了b那么此时栈里面只剩下b一个元素,丢失了c Executors工具类 参考地址:https://zhuanlan.zhihu.com/p/32867181 重载实现的newFixedThreadPool()创建一个可重用固定线程数的线程池，超出的线程会在队列中等待。因为阻塞队列无界,要慎用,避免OOM 123456public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, //基于链表的先进先出队列，如果创建时没有指定此队列大小，则默认为Integer.MAX_VALUE new LinkedBlockingQueue&lt;Runnable&gt;()); &#125; 重载实现的newCachedThreadPool()创建一个可缓存线程池，如果执行新任务时没有空闲的线程，新建线程执行任务；否则调用空闲线程执行新任务。对于执行很多短期异步任务的程序而言，比较合适任务的提交速度小于任务的处理速度的场景,不然很容易OOM 123456public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, //这个队列比较特殊，它不会保存提交的任务，而是将直接新建一个线程来执行新来的任务 new SynchronousQueue&lt;Runnable&gt;());&#125; 重载实现的newSingleThreadExecutor()创建一个使用单个 worker 线程的 Executor，以无界队列方式来运行该线程，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 123456public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;())); &#125; 重载实现的newSingleThreadScheduledExecutor() 重载实现的newScheduledThreadPool()创建一个定长线程池，支持定时及周期性任务执行。1234567public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) &#123; return new ScheduledThreadPoolExecutor(corePoolSize); &#125; //延迟执行调用方式 service.schedule(new MyThread(), 2, TimeUnit.SECONDS); //周期性执行调用方式 service.scheduleAtFixedRate(new MyThread(), 2,3, TimeUnit.SECONDS); CountDownLatch 原理CountDownLatch内部维护一个计数器，计数器的值为待完成的任务数Ｎ，需要等待这Ｎ个任务完成的线程调用CountDownLatch的await()方法使自己进入休眠等待状态。当某一个任务线程完成某一个任务后调用CountDownLatch的countDown()方法来表示自己的任务已完成，此时CountDownLatch的计数器值减１，当所有的任务完成式，计数器的值为０。当计数器值为0时，CountDownLatch将唤醒所有因await()方法进入休眠的线程。 使用12345678910111213141516171819202122232425262728public static void main(String[] args)&#123; final CountDownLatch countDownLatch = new CountDownLatch(allGates.size()); ExecutorService threadPool = Executors.newCachedThreadPool(); for () &#123; threadPool.submit(()-&gt; &#123; try &#123; //do sth &#125; catch (Exception e) &#123; LOGGER.error("error", e); &#125; finally &#123; //每次结束一个线程计数就减一 countDownLatch.countDown(); &#125; &#125;); &#125; try &#123; //主线程进入等待,等待时间结束或者计数减到0将会被唤醒 //计数减到0返回true,超时返回false countDownLatch.await(MAXTIMEOUT_SECONDS, TimeUnit.SECONDS); &#125; catch (InterruptedException e) &#123; LOGGER.error("Interrupt exception", e); &#125; finally &#123; threadPool.shutdownNow();//关闭线程池 &#125;&#125;]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>线程安全</tag>
        <tag>Executors</tag>
        <tag>ConcurrentHashmap</tag>
        <tag>cas</tag>
        <tag>atomic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单的设计模式]]></title>
    <url>%2F2018%2F08%2F19%2F%E7%AE%80%E5%8D%95%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[这是摘要 模板方法参考地址:http://www.hollischuang.com/archives/420 概念在一个方法中定义了一个算法的骨架或者步骤，而将一些步骤延迟到子类中去实现。模板方法使得子类可以在不改变算法结构的情况下，重新定义算法中的某一些步骤。 场景去银行的营业厅办理业务需要以下步骤：1.取号、2.办业务、3.评价。三个步骤中取号和评价都是固定的流程，每个人要做的事儿都是一样的。但是办业务这个步骤根据每个人要办的事情不同所以需要有不同的实现。我们可以将整个办业务这件事儿封装成一个抽象类： 实现抽象类步骤骨架 123456789101112131415161718192021222324252627public abstract class AbstractBusinessHandeler &#123; /** * 模板方法 */ public final void execute()&#123; getRowNumber(); handle(); judge(); &#125; /** * 取号 * @return */ private void getRowNumber()&#123; System.out.println("rowNumber-00" + RandomUtils.nextInt()); &#125; /** * 办理业务 */ public abstract void handle(); //抽象的办理业务方法，由子类实现 /** * 评价 */ private void judge()&#123; System.out.println("give a praised"); &#125;&#125; 子类继承实现抽象方法 12345678910public class SaveMoneyHandler extends AbstractBusinessHandeler &#123; @Override public void handle() &#123; System.out.println("save 1000"); &#125; public static void main(String []args)&#123; SaveMoneyHandler saveMoneyHandler = new SaveMoneyHandler(); saveMoneyHandler.execute(); &#125;&#125;//output:编号：rowNumber-001 save 1000 give a praised 钩子方法当在模板方法中某一些步骤是可选的时候，也就是该步骤不一定要执行，可以由子类来决定是否要执行，则此时就需要用上钩子。钩子是一种被声明在抽象类中的方法，但一般来说它只是空的或者具有默认值，子类可以实现覆盖该钩子，来设置算法步骤的某一步骤是否要执行。钩子可以让子类实现算法中可选的部分，让子类能够有机会对模板方法中某些一即将发生的步骤做出反应。 12345678910111213141516171819202122public abstract class AbstractBusinessHandeler &#123; public final void execute()&#123; if(!isVip())&#123;//如果顾客是vip，则不用排队 getRowNumber(); &#125; handle(); judge(); &#125; public abstract boolean isVip();//抽象的钩子方法，由子类实现 private void getRowNumber()&#123; System.out.println("rowNumber-00" + RandomUtils.nextInt()); &#125; public abstract void handle(); private void judge()&#123; System.out.println("give a praised"); &#125;&#125;]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>模板方法</tag>
        <tag>工厂</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中的String和常量池]]></title>
    <url>%2F2018%2F08%2F18%2Fjava%E4%B8%AD%E7%9A%84String%E5%92%8C%E5%B8%B8%E9%87%8F%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[这是摘要 String注意点什么时候会进入常量池可以使用javap -verbose Test.class反编译试试1234567891011121314151617181920public class Test&#123; public static String a = "a";//放入常量池中 public static String str = "ni" + "hap";//将结果nihao放入常量池中 public static void main()&#123; String b = "b";//放入常量池中 String string1 = "ni"; String string2 = "hao"; String string3 = string1+string2; //将ni和hao分别放入常量池中,而不是结果nihao String string4 = string1+"C"; //将C放入常量池中 String str = "hao";//放入常量池,是一个字符串常量 String str2 = new String("ni"); String str3 = new String("hao");//放入堆中, System.out.println(str==str3);// 运行后结果为false String str4 = str3.intern(); System.out.println(str==str4);// 运行后结果为true,通过intern方法返回的引用指向常量池中的hao &#125;&#125;]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>String</tag>
        <tag>常量池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中的锁]]></title>
    <url>%2F2018%2F08%2F18%2Fjava%E4%B8%AD%E7%9A%84%E9%94%81%2F</url>
    <content type="text"><![CDATA[这是摘要 锁 可重入锁在JAVA中，内置锁都是可重入的，也就是说，如果某个线程试图获取一个已经由它自己持有的锁时，那么这个请求会立刻成功，并且会将这个锁的计数值加1，而当线程退出同步代码块时，计数器将会递减，当计数值等于0时，锁释放。 不可重入锁不可重入锁,同一个线程再次请求已经获取过一次的锁时会造成死锁 ReentrantLock和synchronized 锁的实现:ReentrantLock是JDK实现的,synchronized是依赖于JVM实现的 性能的区别:JDK1.6之后(synchronized引入了偏向锁,轻量级锁(自旋锁)之后)synchronized优化的与ReentrantLock性能差不多,在两种方法都可以使用的情况下官方建议使用synchronized,都是试图在用户态就将加锁问题解决,避免进入内核态 功能的区别:synchronized的使用比较简洁,由编译器保证加锁和施放锁,而ReentrantLock需要手工声明加锁和施放锁,但是ReentrantLock在锁的细粒度和灵活性方面更强 ReentrantLock可以指定公平锁和非公平锁,synchronized只能是非公平锁 ReentrantLock提供了一个Condition类来实现按照满足条件分组唤醒线程,而synchronized只能是随机唤醒一个线程,要么全部唤醒 ReentrantLock提供了一个可以中断等待锁的机制,通过lock.lockInterruptibly()来实现这个机制 对象头 参考:http://www.hollischuang.com/archives/1953 锁优化 参考地址:https://blog.csdn.net/sted_zxz/article/details/76854371 偏向锁第一个获得锁的线程,如果在接下来的执行过程中，只要该锁没有被其他的线程获取，则持有偏向锁的线程将就不需要再进行同步。 轻量级锁当发现会有2个线程争抢资源的时候,升级偏向锁,优先使用轻量级锁,同时使用自旋等待锁释放 重量级锁当2个以上的线程竞争锁,升级为重量级锁同步方法和同步代码块中解释的就是重量级锁。 自旋锁和自适应自旋锁 就是当后一个线程请求锁的时候,不要一请求不到就马上挂起,而是等待一下( 前提是物理机器有一个以上的处理器),但不放弃处理器的执行时间,看看持有锁的线程是否会很快释放锁,为了让线程等待,只需要让线程执行一个忙循环(自旋) 自适应意味着自旋的时间不再固定了，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。 锁粗化就是一串连续的零碎操作都对同一个对象加锁,那么就会将加锁同步的范围扩大 锁消除(虚拟机优化)虚拟机即时编译器在运行的时候会对一些代码上要求同步但是实际上不会发生数据共享竞争的锁进行消除,根据逃逸分析的数据支持,如果堆上的数据不会逃逸出去从而被其他线程访问到,就可以把他们当做是栈上的数据对待,认为他们是线程私有的,同步加锁自然无需进行,比如说StringBuilder.append()方法.]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git原理与使用]]></title>
    <url>%2F2018%2F08%2F18%2Fgit%E5%8E%9F%E7%90%86%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[这是摘要 #git git使用提交 合并add和commitgit commit -am &quot;message&quot;以上命令可以直接将tracked状态的文件提交 追加提交git commit --amend commit和merge技巧建议一个commit只修改一个问题,这样如果想在多个分支之间同步一个修改,只需要用git cherry-pick commitId就可以完成 回退 回退单个文件git reset head filename以上命令可以将暂存区的文件回退到工作区,也就是将add过的文件变回add之前的状态git reset head这个命令就是将所有暂存区的文件回退到工作区 回退版本git reset --hard head^以上命令可以将代码回退到当前最新的commitgit reset --hard commit_id将代码回退到指定commit 撤销commitgit revert head还原已经提交的修改,是提交一个新的版本，将需要revert的版本的内容再反向修改回去，版本会递增，不影响之前提交的内容。此次操作之前和之后的commit和history都会保留，并且把这次撤销作为一次最新的提交git revert HEAD撤销前一次 commitgit revert HEAD^撤销前前一次 commitgit revert commit-id撤销指定的版本，撤销也会作为一次提交进行保存 撤销操作 git revert &lt;SHA&gt;场景: 你已经执行了 git push, 把你的修改发送到了 GitHub，现在你意识到这些 commit 的其中一个是有问题的，你需要撤销那一个 commit.原理: git revert 会产生一个新的 commit，它和指定 SHA 对应的 commit 是相反的（或者说是反转的）。任何从原先的 commit 里删除的内容会在新的 commit 里被加回去，任何在原先的 commit 里加入的内容会在新的 commit 里被删除。此操作是安全的,因为产生了新的commit,历史记录还在 git commit --amend场景: 你在最后一条 commit 消息里有个笔误，已经执行了 git commit -m “Fxies bug #42”，但在 git push 之前你意识到消息应该是 “Fixes bug #42″原理: git commit –amend 会用一个新的 commit 更新并替换最近的 commit ，这个新的 commit 会把任何修改内容和上一个 commit 的内容结合起来。如果当前没有提出任何修改，这个操作就只会把上次的 commit 消息重写一遍。 git checkout -- &lt;bad filename&gt;场景: 一只猫从键盘上走过，无意中保存了修改，然后破坏了编辑器。不过，你还没有 commit 这些修改。你想要恢复被修改文件里的所有内容 — 就像上次 commit 的时候一模一样。原理: git checkout 会把工作目录里的文件修改到 Git 之前记录的某个状态。你可以提供一个你想返回的分支名或特定 SHA ，或者在缺省情况下，Git 会认为你希望 checkout 的是 HEAD，当前 checkout 分支的最后一次 commit。记住：你用这种方法“撤销”的任何修改真的会完全消失。因为它们从来没有被提交过，所以之后 Git 也无法帮助我们恢复它们。你要确保自己了解你在这个操作里扔掉的东西是什么！（也许可以先利用 git diff 确认一下） git reset &lt;last good SHA&gt; 或 git reset --hard &lt;last good SHA&gt;场景: 你在本地提交了一些东西（还没有 push），但是所有这些东西都很糟糕，你希望撤销前面的三次提交 — 就像它们从来没有发生过一样。原理: git reset 会把你的代码库历史返回到指定的 SHA 状态。 这样就像是这些提交从来没有发生过。 –soft,工作区和暂存区都不动,只是撤销了commitDoes not touch the index file or the working tree at all. 缺省情况下(–mixed)， git reset 会保留工作目录。这样，提交是没有了，但是修改内容还在磁盘上。这是一种安全的选择，Resets the index but not the working tree. 但通常我们会希望一步就“撤销”提交以及修改内容 ,这就是 –hard 选项的功能。Resets the index and working tree. 利用分支的另一种做法场景: 你进行了一些提交，然后意识到你开始 check out 的是 master 分支。你希望这些提交进到另一个特性（feature）分支里。方法: git branch feature, git reset --hard origin/master, and git checkout feature原理: 你可能习惯了用 git checkout -b &lt;name&gt;创建新的分支 — 这是创建新分支并马上 check out 的流行捷径 — 但是你不希望马上切换分支。这里，git branch feature 创建一个叫做 feature 的新分支并指向你最近的 commit，但还是让你 check out 在 master 分支上。下一步，在提交任何新的 commit 之前，用 git reset --hard 把 master 分支倒回 origin/master 。不过别担心，那些 commit 还在 feature 分支里。最后，用 git checkout 切换到新的 feature 分支，并且让你最近所有的工作成果都完好无损。 git rm --cached application.log场景: 你偶然把 application.log 加到代码库里了，现在每次你运行应用，Git 都会报告在 application.log 里有未提交的修改。你把 *.login放到了 .gitignore 文件里，可文件还是在代码库里 — 你怎么才能告诉 Git “撤销” 对这个文件的追踪呢？原理: 虽然 .gitignore 会阻止 Git 追踪文件的修改，甚至不关注文件是否存在，但这只是针对那些以前从来没有追踪过的文件。一旦有个文件被加入并提交了，Git 就会持续关注该文件的改变。类似地，如果你利用git add -f 来强制或覆盖了 .gitignore， Git 还会持续追踪改变的情况。之后你就不必用-f 来添加这个文件了。如果你希望从 Git 的追踪对象中删除那个本应忽略的文件， git rm --cached会从追踪对象中删除它，但让文件在磁盘上保持原封不动。因为现在它已经被忽略了，你在 git status 里就不会再看见这个文件，也不会再偶然提交该文件的修改了。 分支管理 查看当前分支基于那个分支 将主线代码合并到自己的分支 切换到master分支 git checkout master 将remote master同步到local master git pull origin master 切换到的local开发分支 git checkout dev-mine 合并 local master 到 local的开发分支 git merge master 推送更新到gitlab，使gitlab同步更新显示 git push origin dev-minw 创建分支 基于远程分支创建本地分支git checkout -b dev-local origin/dev-remote这样创建的分支是有关联的,然后基于这个分支再拉一个分支做开发,开发完之后push到远程,发起pull request(github)或者merge request(gitlab) 建立本地分支和远程分支的关联git branch --set-upstream-to origin/dev-remote 基于当前分支创建分支(假设是master)git checkout -b dev-basedon-master或者是git checkout -b dev-basedon-master master 重命名本地分支git branch -m dev-old dev-new 删除远程分支git push origin :dev-remote 查看分支git branch查看本地分支git branch -r查看远程分支git branch -vv查看本地分支和远程分支的关联 删除本地分支git branch -d branch-name-D是强制删除 暂时保存工作进度[]方括号中内容为可选，[&lt;stash&gt;]里面的stash代表进度的编号形如：stash@{0}, &lt;&gt;尖括号内的必填git stash 对当前的暂存区和工作区状态进行保存。git stash save &quot;work about something on branch-xxx&quot;最好写个记录git stash list 列出所有保存的进度列表。git stash pop [--index] [&lt;stash&gt;] 恢复工作进度如：以下命令恢复编号为0的进度的工作区和暂存区git stash pop --index stash@{0}git stash apply [--index] [&lt;stash&gt;] 不删除已恢复的进度，其他同git stash popgit stash drop [&lt;stash&gt;] 删除某一个进度，默认删除最新进度git stash clear 删除所有进度git stash branch &lt;branchname&gt; &lt;stash&gt; 基于进度创建分支 查看记录 查看某次commit的所有文件改动内容git show commit_id 查看历史commitgit log [--pretty=oneline] git原理参考地址:https://www.jianshu.com/p/619122f8747b从根本上来讲，Git是一个内容寻址文件系统,Git 和其他版本控制系统的主要差别在于，Git 只关心文件数据的整体是否发生变化，而大多数其他系统则只关心文件内容的具体差异。这类系统（CVS，Subversion，Perforce，Bazaar 等等）每次记录有哪些文件作了更新，以及都更新了哪些行的什么内容：Git 并不保存这些前后变化的差异数据。实际上，Git 更像是把变化的文件作快照后，记录在一个微型的文件系统中。每次提交更新时，它会纵览一遍所有文件的指纹信息并对文件作一快照，然后保存一个指向这次快照的索引。为提高性能，若文件没有变化，Git 不会再次保存，而只对上次保存的快照作一链接。Git 的工作方式就如下图所示。 工作区和版本库 修改了工作区的文件 对修改的文件进行快照,然后存到版本库的暂存区(index目录) 提交更新后,将暂存区的文件快照永久保存到objects目录 .git目录结构 description文件：仅供GitWeb程序使用config文件：包含项目特有的配置选项info目录：包含一个全局性排除(global exclude)文件，用以放置那些不希望被记录在 .gitignore文件中的忽略模式(ignored patterns)hooks目录：包含客户端或服务端的钩子脚本(hook scripts)HEAD文件：指示目前被检出的分支index文件：保存暂存区信息objects目录：存储所有数据内容refs 目录：存储指向数据（分支）的提交对象的指针 merge的过程 想要merge master分支和other分支,会将master分支当前节点,other当前节点,以及两个分支的公共祖先节点,三者进行比较合并 git底层数据结构 参考地址:https://www.jianshu.com/p/fa31ef8814d2 BlobGit 把版本库中的每一个文件都转换为一个 blob 对象进行存储， Tree而用 tree 对象来表达文件的层次结构。 CommitCommit 对象代表了一次提交操作，它包含了当前的项目快照以及提交人和提交日期等诸多信息。所有的 commit 对象串接起来，组成一个有向无环图。从版本控制的角度看，这些 commit 对象构成了一个完整的版本提交记录；从项目开发的角度看，它们描述了项目是如何从无到有一点一滴地构建起来的。 TagTag 对象指向一个 commit 对象，我们可以通过 tag 对象快速访问到项目的某一次特征提交。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven入门]]></title>
    <url>%2F2018%2F08%2F18%2Fmaven%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[这是摘要 maven插件 统计项目代码行数插件开发 参考地址:https://blog.csdn.net/zjf280441589/article/details/53044308/]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阻塞队列]]></title>
    <url>%2F2018%2F08%2F16%2FFuture%2F</url>
    <content type="text"><![CDATA[参考地址:https://www.cnblogs.com/cz123/p/7693064.html]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>future</tag>
        <tag>druid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[信号量]]></title>
    <url>%2F2018%2F08%2F16%2F%E4%BF%A1%E5%8F%B7%E9%87%8F%2F</url>
    <content type="text"><![CDATA[这是摘要 信号量一个资源有多个副本可供同时使用，比如打印机房有多个打印机、厕所有多个坑可供同时使用，这种情况下，Java提供了另外的并发访问控制–资源的多副本的并发访问控制， 原理Semaphore是用来保护一个或者多个共享资源的访问，Semaphore内部维护了一个计数器，其值为可以访问的共享资源的个数。一个线程要访问共享资源，先获得信号量，如果信号量的计数器值大于1，意味着有共享资源可以访问，则使其计数器值减去1，再访问共享资源。如果计数器值为0,线程进入休眠。当某个线程使用完共享资源后，释放信号量，并将信号量内部的计数器加1，之前进入休眠的线程将被唤醒并再次试图获得信号量。 使用 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class ResourceManage &#123; private final Semaphore semaphore ; private boolean resourceArray[]; private final ReentrantLock lock; public ResourceManage() &#123; this.resourceArray = new boolean[10];//存放厕所状态 this.semaphore = new Semaphore(10,true);//控制10个共享资源的使用，使用先进先出的公平模式进行共享;公平模式的信号量，先来的先获得信号量 this.lock = new ReentrantLock(true);//公平模式的锁，先来的先选 for(int i=0 ;i&lt;10; i++)&#123; resourceArray[i] = true;//初始化为资源可用的情况 &#125; &#125; public void useResource(int userId)&#123; // semaphore.acquire(); try&#123; semaphore.acquire(); int id = getResourceId();//占到一个坑 System.out.print("userId:"+userId+"正在使用资源，资源id:"+id+"\n"); Thread.sleep(100);//do something，相当于于使用资源 resourceArray[id] = true;//退出这个坑 &#125;catch (InterruptedException e)&#123; e.printStackTrace(); &#125;finally &#123; semaphore.release();//释放信号量，计数器加1 &#125; &#125; private int getResourceId()&#123; int id = -1; lock.lock(); try &#123; //lock.lock();//虽然使用了锁控制同步，但由于只是简单的一个数组遍历，效率还是很高的，所以基本不影响性能。 for(int i=0; i&lt;10; i++)&#123; if(resourceArray[i])&#123; resourceArray[i] = false; id = i; break; &#125; &#125; &#125;catch (Exception e)&#123; e.printStackTrace(); &#125;finally &#123; lock.unlock(); &#125; return id; &#125; &#125; 123456789101112131415161718192021222324252627282930public class ResourceUser implements Runnable&#123; private ResourceManage resourceManage; private int userId; public ResourceUser(ResourceManage resourceManage, int userId) &#123; this.resourceManage = resourceManage; this.userId = userId; &#125; public void run()&#123; System.out.print("userId:"+userId+"准备使用资源...\n"); resourceManage.useResource(userId); System.out.print("userId:"+userId+"使用资源完毕...\n"); &#125; public static void main(String[] args)&#123; ResourceManage resourceManage = new ResourceManage(); Thread[] threads = new Thread[100]; for (int i = 0; i &lt; 100; i++) &#123; Thread thread = new Thread(new ResourceUser(resourceManage,i));//创建多个资源使用者 threads[i] = thread; &#125; for(int i = 0; i &lt; 100; i++)&#123; Thread thread = threads[i]; try &#123; thread.start();//启动线程 &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; &#125; &#125;]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>线程安全</tag>
        <tag>信号量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阻塞队列]]></title>
    <url>%2F2018%2F08%2F16%2F%E9%98%BB%E5%A1%9E%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[参考地址:http://ifeve.com/java-blocking-queue/ SynchronousQueue 一个不存储元素的阻塞队列。每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，吞吐量通常要高于LinkedBlockingQueue 信号量实现方式123456789101112131415161718192021public class SemaphoreSynchronousQueue&lt;E&gt; &#123; E item = null; Semaphore sync = new Semaphore(0); Semaphore send = new Semaphore(1); Semaphore recv = new Semaphore(0); public E take() throws InterruptedException &#123; recv.acquire(); E x = item; sync.release(); send.release(); return x; &#125; public void put (E x) throws InterruptedException&#123; send.acquire(); //当放入元素的时候信号量减1,其他线程想要再放入,就会阻塞 item = x; recv.release(); sync.acquire(); &#125;&#125; LinkedBlockingQueue]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>阻塞队列</tag>
        <tag>生产者消费者模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反射]]></title>
    <url>%2F2018%2F08%2F15%2F%E5%8F%8D%E5%B0%84%2F</url>
    <content type="text"><![CDATA[这是摘要 为什么因为有时候编码的时候需要实例化的类并不明确,需要在运行时动态加载,随机应变.比如在运行的时候不知道传进来的classPath具体是哪一个(但是知道它会继承某个类或者实现某个接口). Class类可以理解成类类型对象 三种使用方式1234567891011//方式1 可以把class理解成User类的隐藏的静态类成员变量Class clazz1 = User.class;//方式2 继承自Object对象的 public final native Class&lt;?&gt; getClass();User user = new User();Class clazz2 = user.getClass();//方式3 try &#123; Class clazz3 = Class.forName("cn.hn.Demo");&#125; catch (ClassNotFoundException e) &#123; e.printStackTrace();&#125; 类加载 静态加载类通过new创建对象是静态加载类,需要在编译的时候就将所有可能使用到的类加载好,如果程序中要用到的类找不到,编译就会通不过 动态加载类在代码中使用反射创建类对象(Class.forName()),然后使用,这样就是在运行时刻才决定加载那个类, 使用反射获取类信息 获取成员变量 获取方法 getDeclaredMethods()获取类自己声明的public,private的方法,不包括从父类继承的,反射不能直接获得父类的方法反射父类方法: 1234567891011public static Method getDeclaredMethod(Object object, String methodName, Class&lt;?&gt; ... parameterTypes)&#123; Method method = null ; for(Class&lt;?&gt; clazz = object.getClass() ; clazz != Object.class ; clazz = clazz.getSuperclass()) &#123; try &#123; method = clazz.getDeclaredMethod(methodName, parameterTypes) ; return method ; &#125; catch (Exception e) &#123; &#125; &#125; return null;&#125; getMethods()获取类自己以及所有继承而来的public的方法 getFields()获取类自己声明的,以及继承而来的public的成员变量, getDeclaredFields()获取类自己声明的public,private的成员变量 通过反射理解泛型的本质 去泛型化1234567ArrayList&lt;String&gt; list1 = new ArrayList();ArrayList&lt;Integer&gt; list2 = new ArrayList&lt;&gt;();Class clazz1 = list1.getClass();Class clazz2 = list2.getClass();System.out.println(clazz1 == clazz2);//返回true 以上代码说明java中泛型只在编译的时候有效,用来防止输入错误的类型,在编译之后泛型信息就不存在了这里有个疑问就是,public final native Class&lt;?&gt; getClass();从哪里获取的类信息? 运行时获取泛型信息Map&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;() {};通过匿名内部类的方式保留泛型信息,因为泛型擦除不能擦除类定义中的泛型 泛型符号 12345678E – Element (在集合中使用，因为集合中存放的是元素)T – Type（Java 类）K – Key（键）V – Value（值）N – Number（数值类型）？ – 表示不确定的java类型（无限制通配符类型）S、U、V – 2nd、3rd、4th typesObject – 是所有类的根类，任何类的对象都可以设置给该Object引用变量，使用的时候可能需要类型强制转换，但是用使用了泛型T、E等这些标识符后，在实际用之前类型就已经确定了，不需要再进行类型强制转换。 反射的应用场景 框架中的应用 Spring Web 模块的 RestTemplate其实很多框架就是使用类定义中的泛型不会被擦除这个特性，实现了相应的功能。例如，我们可以使用如下写法： 1ResponseEntity&lt;YourType&gt; responseEntity = restTemplate.exchange(url, HttpMethod.GET, null, new ParameterizedTypeReference&lt;YourType&gt;() &#123;&#125;); 其中的new ParameterizedTypeReference() {} 就是通过定义一个匿名内部类的方式来获得泛型信息，从而进行反序列化的工作。 IoC生成bean的过程 生成bean,并加入Spring bean容器(其实就是个Map)配置文件中如下配置 1&lt;bean id="courseDao" class="com.qcjy.learning.Dao.impl.CourseDaoImpl"&gt;&lt;/bean&gt; Spring通过配置进行实例化对象，并放到容器中的伪代码： 12345678910//解析&lt;bean .../&gt;元素的id属性得到该字符串值为“courseDao”String idStr = "courseDao";//解析&lt;bean .../&gt;元素的class属性得到该字符串值为“com.qcjy.learning.Dao.impl.CourseDaoImpl”String classStr = "com.qcjy.learning.Dao.impl.CourseDaoImpl";//利用反射知识，通过classStr获取Class类对象Class&lt;?&gt; cls = Class.forName(classStr);//实例化对象Object obj = cls.newInstance();//container表示Spring容器container.put(idStr, obj); 其实就是解析配置文件,然后通过配置文件中的class进行反射生成实例,最后将(id,bean)加入Map 一个bean中需要另一个bean的情况配置文件中如下 1234&lt;bean id="courseService" class="com.qcjy.learning.service.impl.CourseServiceImpl"&gt; &lt;!-- 控制调用setCourseDao()方法，将容器中的courseDao bean作为传入参数 --&gt; &lt;property name="courseDao" ref="courseDao"&gt;&lt;/property&gt;&lt;/bean&gt; 利用反射设置bean的伪代码 12345678910111213//解析&lt;property .../&gt;元素的name属性得到该字符串值为“courseDao”String nameStr = "courseDao";//解析&lt;property .../&gt;元素的ref属性得到该字符串值为“courseDao”String refStr = "courseDao";//生成将要调用setter方法名String setterName = "set" + nameStr.substring(0, 1).toUpperCase() + nameStr.substring(1);//获取spring容器中名为refStr的Bean，该Bean将会作为传入参数Object paramBean = container.get(refStr);//获取setter方法的Method类，此处的cls是反射得到的Class对象Method setter = cls.getMethod(setterName, paramBean.getClass());//调用invoke()方法，此处的obj是反射得到的Object对象setter.invoke(obj, paramBean); 这里就是多了一个反射bean的setter方法 JDBC数据库连接]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>反射</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式]]></title>
    <url>%2F2018%2F08%2F14%2F%E5%88%86%E5%B8%83%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[参考地址:http://www.hollischuang.com/archives/1716 目前几乎很多大型网站及应用都是分布式部署的，分布式场景中的数据一致性问题一直是一个比较重要的话题。分布式的CAP理论告诉我们“任何一个分布式系统都无法同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance），最多只能同时满足两项。”所以，很多系统在设计之初就要对这三者做出取舍。在互联网领域的绝大多数的场景中，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证“最终一致性”，只要这个最终时间是在用户可以接受的范围内即可。 分布式锁的实现方法 基于数据库实现分布式锁 基于数据库记录来获取分布式锁 基于数据库的排他锁来获取分布式锁 基于缓存（redis，memcached，tair）实现分布式锁 基于Zookeeper实现分布式锁]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>分布式锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库事务]]></title>
    <url>%2F2018%2F08%2F12%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[参考地址:http://www.hollischuang.com/archives/898 命令行 查看隔离级别SELECT @@tx_isolation; 查看存储引擎show variables like &#39;%storage_engine%&#39;; 锁 行级锁行级锁是Mysql中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 排它锁(写锁)SELECT ... FOR UPDATE;排他锁又称写锁，如果事务T对数据A加上排他锁后，则其他事务不能再对A加任任何类型的封锁。获准排他锁的事务既能读数据，又能修改数据。 共享锁(读锁)SELECT ... LOCK IN SHARE MODE;如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，不能加排他锁。获准共享锁的事务只能读数据，不能修改数据。 表级锁表级锁是MySQL中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分MySQL引擎支持。最常使用的MYISAM与INNODB都支持表级锁定。开销小，加锁快；不会出现死锁；锁定粒度大，发出锁冲突的概率最高，并发度最低。 表共享读锁(共享锁) 表独占写锁(排他锁) 意向共享锁(IS) 意向互斥锁(IX) 有的人可能会对意向锁(intention lock)的目的并不是完全的理解，我们在这里可以举一个例子：如果没有意向锁，当已经有人使用行锁对表中的某一行进行修改时，如果另外一个请求要对全表进行修改，那么就需要对所有的行是否被锁定进行扫描，在这种情况下，效率是非常低的；不过，在引入意向锁之后，当有人使用行锁对表中的某一行进行修改之前，会先为表添加意向互斥锁（IX），再为行记录添加互斥锁（X），在这时如果有人尝试对全表进行修改就不需要判断表中的每一行数据是否被加锁了，只需要通过等待意向互斥锁被释放就可以了。 页级锁页级锁是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。一次锁定相邻的一组记录。BDB支持页级锁 锁算法 记录锁记录锁（Record Lock）是加到索引记录上的锁,如果我们使用 id 或者 last_name 作为 SQL 中 WHERE 语句的过滤条件，那么 InnoDB 就可以通过索引建立的 B+ 树找到行记录并添加索引，但是如果使用 first_name 作为过滤条件时，由于 InnoDB 不知道待修改的记录具体存放的位置，也无法对将要修改哪条记录提前做出判断就会锁定整个表。 间隙锁间隙锁是对索引记录中的一段连续区域的锁；当使用类似 SELECT * FROM users WHERE id BETWEEN 10 AND 20 FOR UPDATE; 的 SQL 语句时，就会阻止其他事务向表中插入 id = 15的记录，因为整个范围都被间隙锁锁定了。 next-key锁(解决幻读)当我们更新一条记录，比如 SELECT * FROM users WHERE age = 30 FOR UPDATE;，InnoDB 不仅会在范围 (21, 30] 上加 Next-Key 锁，还会在这条记录后面的范围 (30, 40] 加间隙锁，所以插入 (21, 40] 范围内的记录都会被锁定。 事务概念一个数据库事务通常包含对数据库的读或写的一个操作序列,它存在的目的如下: 为数据库操作提供一个从失败状态恢复到正常状态的方法,同时提供了数据库即使在异常状态也能保持一致性的方法 当多个应用程序并发访问数据库时,提供隔离这些操作的方法,避免操作之间相互干扰 事务特性(ACID)数据库事务:银行转账的例子 原子性(atomicity)事务中对数据库的一系列操作要么全部被执行,要么都不执行 一致性(consistent)从一个一致性状态变到另一个一致性状态,满足数据库的完整性约束 隔离性(isolation)多个事务并发执行的时候,一个事务不会影响到另一个事务 持久性(Durability)事务一旦提交成功,对数据库的修改将永久保存在数据库中 事务的隔离级别参考地址:http://www.hollischuang.com/archives/943 未提交读(read Uncommited) 概念一个事务可以读取另一个事务未提交的数据 实现事务读取数据的时候不加锁事务修改数据的时候加行级共享锁 可能现象出现脏读 提交读(read committed) 概念在一个事务修改数据过程中，如果事务还没提交，其他事务不能读该数据。解决脏读 实现事务对当前被读取的数据加 行级共享锁（当读到时才加锁），一旦读完该行，立即释放该行级共享锁；事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加 行级排他锁，直到事务结束才释放。 可能现象不可重复读:对于同一数据项,前面读取的数据和后来读取的数据可能不一样可能出现不可重复读和幻读 可重复读(repeated read) 概念可重复读,只有在事务一提交之后，事务二才能更改该行数据。所以，只要在事务一从开始到结束的这段时间内，无论他读取该行数据多少次，结果都是一样的。解决不可重复读 实现事务在读取某数据的瞬间（就是开始读取的瞬间），必须先对其加 行级共享锁，直到事务结束才释放；事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加 行级排他锁，直到事务结束才释放。innodb中使用快照技术,即晚于当前事务的其他的事务的更新对本事务不可见 可能现象数据库进行读事务的时候就会禁止其他的写事务,进行写事务的时候就会禁止其他任何事务写操作可能出现幻读,因为其他事务虽然不能修改该行数据,但是可以新增数据行, 可序列化(serializable) 概念可序列化:最高级别,事务串行,解决幻读 实现事务在读取数据时，必须先对其加 表级共享锁 ，直到事务结束才释放；事务在更新数据时，必须先对其加 表级排他锁 ，直到事务结束才释放。 可能现象虽然可序列化解决了脏读、不可重复读、幻读等读现象。但是序列化事务会产生以下效果： 无法读取其它事务已修改但未提交的记录。 在当前事务完成之前，其它事务不能修改目前事务已读取的记录。 在当前事务完成之前，其它事务所插入的新记录，其索引键值不能在当前事务的任何语句所读取的索引键范围中。 由于低级别隔离性产生的问题: 脏读:session1查询test为空,此时session2插入数据不提交,然后session再次查询test就有数据了 提交读解决脏读的方法在于,当session2插入数据的时候加了排他行锁,直到session2提交才能被访问到 不可重复读:session1查询id=3的地方v=2,此时session2把v改成3并且提交,session1再次查询id=2的地方v变成了3,同一事务前后访问不一致,重点在同一条记录 可重复读解决不可重复读的方法在于,当session1读取数据的时候加了共享行锁,知道session1结束才释放,所以session2无法修改数据 幻读:重点在插入了新数据,session1查询返回结果为空表,此时session2插入新数据1并提交,(图中有误,第二次查询应去掉)然后事务1也插入数据1,此时报错,刚刚明明是空表,现在却提示数据冲突. 出现幻读的原因是可重复读虽然加了行锁,但是无法阻止数据的插入,解决的办法就是加表锁,还有乐观锁机制? mysql常用存储引擎 MyISAM和MEMORY采用表级锁(table-level locking) 在myisam中是不会出现死锁的,因为一次性获取所有需要的锁要么全部满足,要么全部等待 BDB采用页面锁(page-level locking)或表级锁，默认为页面锁 InnoDB支持行级锁(row-level locking)和表级锁,默认为行级锁innodb中的行锁实现原理是在索引中的索引项上加锁,所以只有通过索引条件检索数据InnoDB才会使用行锁,否则使用的是表锁 实际应用: 因为MySQL行锁是针对索引,而不是记录,所以使用相同的索引键,即使是访问不同的行也会产生锁冲突 如果一张表使用多个索引,那么可以根据不同的索引键访问不同的行在MySQL中，行级锁并不是直接锁记录，而是锁索引。索引分为主键索引和非主键索引两种，如果一条sql语句操作了主键索引，MySQL就会锁定这条主键索引；如果一条语句操作了非主键索引，MySQL会先锁定该非主键索引，再锁定相关的主键索引。 在UPDATE、DELETE操作时，MySQL不仅锁定WHERE条件扫描过的所有索引记录，而且会锁定相邻的键值，即所谓的next-key locking。当两个事务同时执行，一个锁住了主键索引，在等待其他相关索引。另一个锁定了非主键索引，在等待主键索引。这样就会发生死锁。发生死锁后，InnoDB一般都可以检测到，并使一个事务释放锁回退，另一个获取锁完成事务。参考地址:https://draveness.me/mysql-innodb 产生死锁的可能: 事务1获取数据a的锁,想要访问数据b,但是此时数据b被事务2锁定,而事务2希望访问数据a,陷入相互等待的局面.解决的方式就是从根源上避免这种情况的出现,就是修改程序逻辑,避免出现环,可以一开始就将所有要用到的资源锁定还有就是采用抢占和回滚机制,根据时间戳来判断事务应该回滚还是等待 避免死锁的方式 如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。 在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率； 对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率； 并发控制机制参考地址:http://www.hollischuang.com/archives/934 悲观锁(是真正的锁)就是之前的共享锁和互斥锁(排它锁),就是不管如何,想要修改数据之前先加锁,可能会产生死锁 乐观锁(其实是一种并发控制思想,基于时间戳或者版本号)这种思想假设数据操作的时候一般不会产生冲突,只有在最后要写回数据的时候检测数据是否发生了变化,发生冲突的话进行重试或者返回错误信息让用户决定怎么处理,否则就可以正常写回在整个执行过程中实际上并没有加任何锁,在要求响应速度和高并发的场景下适合,不会产生死锁,但是在高冲突频率重试成本的情况下还是建议使用悲观锁 版本控制:就是为数据的每一个写操作创建版本,在进行读操作的时候就会在有限多的版本中选择最合适的返回通常是在数据表中增加一列version,然后写回数据的时候比对version 基于时间戳]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mpv快捷键]]></title>
    <url>%2F2018%2F08%2F11%2Fmpv%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[这是摘要 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960611对比度 -2对比度+3亮度 -4亮度+5伽马 -6伽马+7饱和度 -8饱和度+9音量减小0音量增加上方向键播放进度快进1分钟下方向键播放进度快退1分钟左方向键播放进度后退5秒右方向键播放进度前进5秒ctrl与 - 号组合 音轨和视频的调节（音轨快进一些）CTRL与+号组合 音轨和视频的调节（音轨减慢一些）p或者空格 暂停i 显示视频的详情参数m 静音z 字幕后退x 字母前进f全屏q 退出＃切换音频轨，多音频的可以用j 切换字幕，多字幕的时候可以使用J 大写，反方向的切换字幕【或&#123; 播放速度减慢，变化速度不一样】或&#125; 播放速度加快，倍速变化速度不一样backspace 播放速度返回到一倍速&lt; 逐帧播放&gt; 按下播放，回弹暂停]]></content>
      <categories>
        <category>生活方式</category>
      </categories>
      <tags>
        <tag>mpv</tag>
        <tag>快捷键</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC]]></title>
    <url>%2F2018%2F08%2F07%2FSpringMVC%2F</url>
    <content type="text"><![CDATA[开发自己的SpringMVC参考地址:https://mp.weixin.qq.com/s/Fo3ZN64qm7e2bWEKEIfBFA参考地址:https://github.com/CFMystery/bfmvc/blob/master/bfmvc/src/main/java/cc/cleverfan/web/DispatherServlet.java spring框架的基本原理 基本原理其实就是通过反射(反射的原理)解析类及其类的各种信息，包括构造器、方法及其参数，属性。然后将其封装成bean定义信息类、constructor信息类、method信息类、property信息类，最终放在一个map里，也就是所谓的container，池等等，其实就是个map。。汗。。。。当你写好配置文件，启动项目后，框架会先按照你的配置文件找到那个要scan的包(一般是哪些包?)，然后解析包里面的所有类，找到所有含有@bean，@service等注解的类，利用反射解析它们，包括解析构造器，方法，属性等等，然后封装成各种信息类放到一个map里。每当你需要一个bean的时候，框架就会从container找是不是有这个类的定义啊？如果找到则通过构造器new出来（这就是控制反转，不用你new,框架帮你new），再在这个类找是不是有要注入的属性或者方法，比如标有@autowired的属性，如果有则还是到container找对应的解析类，new出对象，并通过之前解析出来的信息类找到setter方法，然后用该方法注入对象（这就是依赖注入）。如果其中有一个类container里没找到，则抛出异常，比如常见的spring无法找到该类定义，无法wire的异常。还有就是嵌套bean则用了一下递归，container会放到servletcontext里面，每次reQuest从servletcontext找这个container即可，不用多次解析类定义。如果bean的scope是singleton，则会重用这个bean不再重新创建，将这个bean放到一个map里，每次用都先从这个map里面找。如果scope是session，则该bean会放到session里面。仅此而已，没必要花更多精力。 bean工厂123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class BeanFactory &#123; private Map&lt;String, Object&gt; beanMap = new HashMap&lt;String, Object&gt;(); /** * bean工厂的初始化. * @param xml xml配置文件 */ public void init(String xml) &#123; try &#123; //读取指定的配置文件 SAXReader reader = new SAXReader(); ClassLoader classLoader = Thread.currentThread().getContextClassLoader(); //从class目录下获取指定的xml文件 InputStream ins = classLoader.getResourceAsStream(xml); Document doc = reader.read(ins); Element root = doc.getRootElement(); Element foo; //遍历bean for (Iterator i = root.elementIterator("bean"); i.hasNext();) &#123; foo = (Element) i.next(); //获取bean的属性id和class Attribute id = foo.attribute("id"); Attribute cls = foo.attribute("class"); //利用Java反射机制，通过class的名称获取Class对象 Class bean = Class.forName(cls.getText()); //获取对应class的信息 java.beans.BeanInfo info = java.beans.Introspector.getBeanInfo(bean); //获取其属性描述 java.beans.PropertyDescriptor pd[] = info.getPropertyDescriptors(); //设置值的方法 Method mSet = null; //创建一个对象 Object obj = bean.newInstance(); //遍历该bean的property属性 for (Iterator ite = foo.elementIterator("property"); ite.hasNext();) &#123; Element foo2 = (Element) ite.next(); //获取该property的name属性 Attribute name = foo2.attribute("name"); String value = null; //获取该property的子元素value的值 for(Iterator ite1 = foo2.elementIterator("value"); ite1.hasNext();) &#123; Element node = (Element) ite1.next(); value = node.getText(); break; &#125; for (int k = 0; k &lt; pd.length; k++) &#123; if (pd[k].getName().equalsIgnoreCase(name.getText())) &#123; mSet = pd[k].getWriteMethod(); //利用Java的反射机制调用对象的某个set方法，并将值设置进去 mSet.invoke(obj, value); &#125; &#125; &#125; //将对象放入beanMap中，其中key为id值，value为对象 beanMap.put(id.getText(), obj); &#125; &#125; catch (Exception e) &#123; System.out.println(e.toString()); &#125; &#125; //other codes&#125; 依赖注入()就是通过反射bean的setter方法,set成员bean的过程 控制反转(IOC)就是通过配置文件+反射生成bean 切面编程(AOP)1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;&lt;/dependency&gt; 参考网址: https://www.liaoxuefeng.com/article/0013738774263173c42eae58864423698dd40556af23bb5000 通知(Advice)定义切点何时工作,以工作内容 1. 前置通知(Before):一般是方法调用前打出传入参数 2. 后置通知(After):是返回和异常通知的并集 3. 返回通知(After-running):方法正常返回 4. 异常通知(After-throwing):方法抛出异常 5. 环绕通知(Around):可以同时定义前置和后置 切点(PointCut)定义在何处工作,也就是对哪个方法应用通知 例子12345678910111213141516171819202122232425262728@Aspect@Componentpublic class AopLog &#123; //这个类就是切面,切面由若干切点和通知组成 Logger logger = LoggerFactory.getLogger(AopLog.class); @Autowired JobService jobService; //这个方法定义切点,一般就是要代理的方法 @Pointcut("execution(* com.hn.controller.JobController.delete*(..))") public void forDelete() &#123;&#125; @Pointcut("execution(* com.hn.controller.JobController.query*(..))") public void forQuery() &#123;&#125; //这个就是通知,定义切点工作的时机和工作内容 @Before("forDelete()") public void deleteLog(JoinPoint joinPoint) &#123; String className = joinPoint.getTarget().getClass().getName(); String methodName = joinPoint.getSignature().getName(); Object[] params = joinPoint.getArgs(); logger.info("target class is &#123;&#125;, and target method is &#123;&#125;, and params are &#123;&#125;", className, methodName, Arrays.asList(params)); &#125; @AfterReturning(value = "forQuery()", returning = "ret") public void queryLog(JoinPoint joinPoint, Object ret) &#123; String result = ret.toString(); logger.info("result is &#123;&#125;", result); &#125;&#125; 使用场景: 比较常见的就是日志记录,如果不使用aop的话,例子中实现日志功能的代码就是切面,而需要添加日志的类或者方法就是切点. 就会在业务组件的核心功能之外附带了其他功能代码,会让业务代码变得混乱,业务组件应该只关心核心功能的实现 比如日志功能如果在多个组件中被使用,那么日志功能的业务逻辑发生改变时,维护修改就很麻烦 日志应用:可以在日志中输出监控对象的方法调用的参数输入,返回结果等比如要监控的对象是要监控LoginServiceImpl.login(String name,String password),只需要准备一个LogServiceImpl,LogServiceImpl中实现无参日志方法,有参日志方法,还有有参有返回值的日志方法,然后通过applicationContext.xml中的&lt;aop:config&gt;将其中一个日志方法织入LoginServiceImpl中配置,然后在客户端调用login方法的时候日志中就会有相应的输出内容参考地址:http://www.cnblogs.com/yulinfeng/p/7719128.html 注解 参考地址:https://blog.csdn.net/lylwo317/article/details/52163304 注解本质是一个继承了Annotation的特殊接口，其具体实现类是Java运行时生成的动态代理类。通过代理对象调用自定义注解（接口）的方法，会最终调用AnnotationInvocationHandler的invoke方法。该方法会从memberValues这个Map中索引出对应的值。而memberValues的来源是Java常量池。 注解的使用 声明作用在类上的注解,保留到运行时,默认值”default”12345@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)public @interface HelloAnnotation &#123; String say() default "Hi";&#125; 其实注解被编译后的本质就是一个继承Annotation接口的接口,所以@HelloAnnotation其实就是public interface HelloAnnotation extends Annotation,当我们通过AnnotationTest.class.getAnnotation(HelloAnnotation.class)调用时，JDK会通过动态代理生成一个实现了HelloAnnotation接口的对象，并把将RuntimeVisibleAnnotations属性值设置进此对象中，此对象即为HelloAnnotation注解对象，通过它的say()方法就可以获取到注解值。 声明作用在方法上的类,保留到运行时, 12345@Target(&#123;ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)public @interface TestMethod &#123; String value();&#125; 通过class获取注解 1234567891011121314151617@HelloAnnotation("Do it")public class AnnotationTest &#123; @TestMethod("tomcat-method") public void test()&#123; &#125; public static void main(String[] args)&#123; HelloAnnotation t = AnnotationTest.class.getAnnotation(HelloAnnotation.class); System.out.println(t.say());//Do it TestMethod tm = null; try &#123; tm = AnnotationTest.class.getDeclaredMethod("test",null).getAnnotation(TestMethod.class); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(tm.value());//tomcat-method &#125;&#125; SpringMVC中的常用注解 参考地址:https://www.javazhiyin.com/12491.html @ResponseBody@ResponseBody注解使用的返回值处理器是RequestResponseBodyMethodProcessor,调用 HttpMessagConverter 消息转换机制,使用后Controller中的方法可以直接返回json数据 @RestController相当于使用了@ResponseBody和@Controller @RequestParam绑定请求和Controller中方法的参数 @PathVariable请求URI中的模板变量部分到处理器功能处理方法的方法参数上的绑定,比如 12345@RequestMapping(value="/get/&#123;bookId&#125;")public String getBookById(@PathVariable String bookId,Model model)&#123; model.addAttribute("bookId", bookId); return "book";&#125; @RequestMapping请求路径映射RequestMapping注解有六个属性，下面我们把她分成三类进行说明（下面有相应示例）。 value， method；value： 指定请求的实际地址，指定的地址可以是URI Template 模式（后面将会说明）；method： 指定请求的method类型， GET、POST、PUT、DELETE等； consumes，producesconsumes： 指定处理请求的提交内容类型（Content-Type），例如application/json, text/html;produces: 指定返回的内容类型，仅当request请求头中的(Accept)类型中包含该指定类型才返回； params，headersparams： 指定request中必须包含某些参数值是，才让该方法处理。headers： 指定request中必须包含某些指定的header值，才能让该方法处理请求。 @Scope(“prototype”) 默认是singleton单例的作用域,声明成原型作用域每次使用都是创建新的实例 适用于Web应用的还有request,session,application @Bean使用在返回实例的方法上 SpringBoot常用配置 默认扫描位置 springboot默认扫描@SpringBootApplication入口类所在路径,同级目录包及其子包 也可以使用注解@ComponentScan(basePackages={&quot;com.hn&quot;})指定扫描路径 配置日志 配置优先级Spring Boot 所提供的配置优先级顺序比较复杂。按照优先级从高到低的顺序: 命令行参数。 通过 System.getProperties() 获取的 Java 系统参数。 操作系统环境变量。 从 java:comp/env 得到的 JNDI 属性。 通过 RandomValuePropertySource 生成的1random.*1属性。 应用 Jar 文件之外的属性文件。 应用 Jar 文件内部的属性文件。 在应用配置 Java 类（包含@Configuration注解的 Java 类）中通过@PropertySource注解声明的属性文件。 通过SpringApplication.setDefaultProperties声明的默认属性。SpringBoot的这个配置优先级看似复杂，其实是很合理的。比如命令行参数的优先级被设置为最高。这样的好处是可以在测试或生产环境中快速地修改配置参数值，而不需要重新打包和部署应用。 参考地址https://www.javazhiyin.com/16381.html 配置mybatis 添加mybatis和mysql驱动依赖 配置文件中添加数据库参数 Bean的实例化过程spring事务 可以使用注解@Transaction(propagation=Propagation. REQUIRED,readOnly=true)配置 传播行为传播行为定义了客户端与被调用方法之间的事务边界，即传播规则回答了这样的一个问题，新的事务应该被启动还是挂起，或者方法是否要在事务环境中运行。 隔离级别ISOLATION_DEFAULT:使用后端数据库默认的规则ISOLATION_READ_UNCOMMITTED:允许读取尚未提交的数据变更，可能会导致脏读，幻读或不可重复读ISOLATION_READ_COMMITTED:允许读取并发事务已经提交的数据，可以防止脏读，但是幻读或不可重复读仍有可能发生ISOLATION_REPEATABLE_READ:对同意字段的多次读取结果是一致的，除非数据是被本事务自己所修改，看阻止脏读和不可重复读，但幻读仍有可能发生ISOLATIOM_SERIALIZABLE:完全服从ACID的隔离级别，确保阻止脏读，不可重复读以及幻读，这是最慢的数据隔离级别 是否只读如果事务只对后端的数据库进行读操作，数据库可以利用事务ID只读特性来进行一些特定的优化。通过将事务设置为只读，你就可以给数据库一个机会，让他应用它认为合适的优化措施。因为是否只读是在事务启动的时候由数据库实施的，所以只有对那些具备可能启动一个新事务的传播行为（PROPAGATION_REQUIRED,PROPAGATION_REQUIRED_NEW,PROPAGATION_NESTED）的方法来说，才有意义。 事务超时为了使应用程序很好地运行，事务不能运行太长时间。因为超时时钟会在事务开始时启动，所以只有对那些具备可能启动一个新事务的传播行为（PROPAGATION_REQUIRED,PROPAGATION_REQUIRED_NEW,PROPAGATION_NESTED）的方法来说，才有意义。 事务回滚事务回滚规则定义了哪些异常会导致事务回滚而哪些不会。默认情况下，事务只有在遇到运行时期异常才回滚，而在遇到检查型异常时不会回滚。 spring和springMVC spring是父容器,springMVC是子容器,spring容器中有Service层的bean,springMVC容器中有Controller层的bean,子容器中找不到的bean可以去父容器中找,但是父容器不能在子容器中查找bean,这就是为什么由SpringMVC容器创建的Controller可以自动注入Spring容器创建的Service的bean了 传统springMVC的不足 传统MVC模式的不足 每次请求都必须经过”控制器-模型-视图”才能最终在浏览器上呈现,过程略显复杂 视图依赖于模型,没有模型,浏览器就无法展示页面,客户体验不好 视图的渲染是在服务器端进行的,最后返回给浏览器的是带有模型的视图,渲染性能不能得到很好的优化 前后端分离的方式就是浏览器发送AJAX请求,然后收到JSON数据返回,然后浏览器来渲染页面 SpringMVC HTTP请求响应的过程 参考地址:http://lgbolgger.iteye.com/blog/2105101 1234567891011121314151.用户发起请求到前端控制器。2.前端控制器通过处理器映射器查找handler。3.处理器映射器返回执行链。 a)handler对象 b)拦截器（集合）4.前端控制器通处理器适配器包装，执行handler对象。思考：为什么要通过适配器来执行？因为普通的servlet方法中参数都是固定的request和response,而灵活多变的handler中的方法参数不确定,因此需要handlerAdapter5.通过模型handler处理业务逻辑。6.处理业务完成后，返回ModeAndView对象，其中有视图名称，模型数据。7.将视图名称和模型数据返回到前端控制器。8.前端控制器通过视图解释器查找视图对象。9.视图解释器返回真正的视图。10.前端控制器通过返回的视图和数据进行渲染。11.返回渲染完成的视图。12.将最终的视图返回给用户，产生响应。 代码调用流程 经过ApplicationFilterChain.doFilter() HttpServlet.service() FrameworkServlet.doGet() FrameworkServlet.proecssRequest() DispatcherServlet.doService() DispatcherServlet.doDispatch()在这个方法会完成获取handler以及找到合适的handlerAdapter,然后还会调用handlerAdapter.handle(request,response,handler)方法进行真正的处理返回modelAndView HandlerMapping通过handlerMapping来寻找HandlerExecutionChain(其中包含了拦截器和handler),其中的handler也就是平时用的XXXController类 BeanNameUrlHandlerMapping ：通过对比url和bean的name找到对应的对象 SimpleUrlHandlerMapping ：也是直接配置url和对应bean,比BeanNameUrlHandlerMapping功能更多 RequestMappingHandlerMapping :主要是针对注解配置@RequestMapping的 HandlerAdapter通过handlerAdapter来寻找具体的方法,也就是XXXController类中的某个方法 RequestMappingHandlerAdapter :和上面的RequestMappingHandlerMapping配对使用，针对@RequestMapping HttpRequestHandlerAdapter ：要求handler实现HttpRequestHandler接口，该接口的方法为void handleRequest(HttpServletRequest request, HttpServletResponse response)也就是 handler必须有一个handleRequest方法 SimpleControllerHandlerAdapter：要求handler实现Controller接口，该接口的方法为ModelAndView handleRequest(HttpServletRequest request, HttpServletResponse response) 拦截器和过滤器 拦截器(Interceptor) 使用实现HandlerInterceptor接口,创建类继承WebMvcConfigurerAdapter并重写addInterceptors()方法,添加拦截器实例就行了 应用123456789101112public class AuthorityInterceptor implements HandlerInterceptor &#123; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; String id = request.getParameter("id"); if (StringUtils.equals(id, "2")) &#123; logger.info("request pass"); return true; &#125; logger.info("request intercepted"); return false; &#125;&#125; 123456789@Configurationpublic class WebMvcConfigurer extends WebMvcConfigurationSupport &#123; @Override public void addInterceptors(InterceptorRegistry registry) &#123; // 拦截器按照顺序执行,不止One，还有Two，Three registry.addInterceptor(new AuthorityInterceptor()).addPathPatterns("/job/delete"); super.addInterceptors(registry); &#125;&#125; 注意点: 基于反射实现 其中WebMvcConfigurerAdapter已经过时,使用WebMvcConfigurationSupport代替,但是据说会影响到WebMvcConfiguration自动配置,访问不到静态资源 拦截可以在handle之前,之后,以及在整个请求结束之后(即dispatcherServlet渲染对应视图之后),细粒度可以根据@RequestMapping具体到方法, 拦截器只能拦截经过dispatcherServlet转发的请求 过滤器(Filter) 使用 实现Filter接口,添加注解@WebFilter(urlPatterns = &quot;/*&quot;) 在启动类添加注解@ServletComponentScan(basePackages = &quot;com.hn.filter&quot;) 应用 比如过拦截掉我们不需要的接口请求 修改请求（request）和响应（response）内容 完成CORS跨域请求等等 注意点 基于回调实现 过滤器属于Servlet范畴的API,和spring没什么关系,并且需要在Servlet容器中实现 只能在request前后有用 几乎所有请求,只要符合过滤规则 九大组件12345678910111213141516171819202122232425262728protected void initStrategies(ApplicationContext context) &#123; //用于处理上传请求。处理方法是将普通的request包装成 //MultipartHttpServletRequest，后者可以直接调用getFile方法获取File. initMultipartResolver(context); //SpringMVC主要有两个地方用到了Locale： //一是ViewResolver视图解析的时候； //二是用到国际化资源或者主题的时候。 initLocaleResolver(context); //用于解析主题。SpringMVC中一个主题对应 一个properties文件，里面存放着跟当前主题相关的所有资源、//如图片、css样式等。SpringMVC的主题也支持国际化， initThemeResolver(context); //用来查找Handler的。 initHandlerMappings(context); //从名字上看，它就是一个适配器。Servlet需要的处理方法的结构却是固定的，都是以request和response为参数的方法。//如何让固定的Servlet处理方法调用灵活的Handler来进行处理呢？这就是HandlerAdapter要做的事情 initHandlerAdapters(context); //其它组件都是用来干活的。在干活的过程中难免会出现问题，出问题后怎么办呢？//这就需要有一个专门的角色对异常情况进行处理，在SpringMVC中就是HandlerExceptionResolver。 initHandlerExceptionResolvers(context); //有的Handler处理完后并没有设置View也没有设置ViewName，这时就需要从request获取ViewName了，//如何从request中获取ViewName就是RequestToViewNameTranslator要做的事情了。 initRequestToViewNameTranslator(context);//ViewResolver用来将String类型的视图名和Locale解析为View类型的视图。//View是用来渲染页面的，也就是将程序返回的参数填入模板里，生成html（也可能是其它类型）文件。 initViewResolvers(context); //用来管理FlashMap的，FlashMap主要用在redirect重定向中传递参数。 initFlashMapManager(context); &#125; ##Json问题 返回json数据中文乱码的问题 方法1:在springMVC.xml中配置 123456789101112&lt;!-- 处理请求返回json字符串的中文乱码问题 --&gt; &lt;mvc:annotation-driven&gt; &lt;mvc:message-converters&gt; &lt;bean class="org.springframework.http.converter.StringHttpMessageConverter"&gt; &lt;property name="supportedMediaTypes"&gt; &lt;list&gt; &lt;value&gt;application/json;charset=UTF-8&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/mvc:message-converters&gt; &lt;/mvc:annotation-driven&gt; 方法二:在url映射注解中添加一段 @RequestMapping(value=&quot;/getUsersByPage&quot;,produces = &quot;application/json; charset=utf-8&quot;)]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>SpringMVC</tag>
        <tag>开源框架</tag>
        <tag>spring</tag>
        <tag>注解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java枚举类]]></title>
    <url>%2F2018%2F08%2F04%2Fjava%E6%9E%9A%E4%B8%BE%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[参考地址:http://www.hollischuang.com/archives/195 使用方式123456789101112131415161718192021222324252627282930313233public enum Color &#123; RED("红色", 1), GREEN("绿色", 2), BLANK("白色", 3), YELLO("黄色", 4); // 成员变量 private String name; private int index; // 构造方法 private Color(String name, int index) &#123; this.name = name; this.index = index; &#125; // 普通方法 public static String getName(int index) &#123; for (Color c : Color.values()) &#123; if (c.getIndex() == index) &#123; return c.name; &#125; &#125; return null; &#125; // get set 方法 public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getIndex() &#123; return index; &#125; public void setIndex(int index) &#123; this.index = index; &#125; &#125; enum反编译123public enum Season &#123; SPRING,SUMMER,AUTUMN,WINTER;&#125; 12345678910111213141516171819public final class Season extends Enum&#123; //省略部分内容 public static final Season SPRING; public static final Season SUMMER; public static final Season AUTUMN; public static final Season WINTER; private static final Season ENUM$VALUES[]; static &#123; SPRING = new Season("SPRING", 0); SUMMER = new Season("SUMMER", 1); AUTUMN = new Season("AUTUMN", 2); WINTER = new Season("WINTER", 3); ENUM$VALUES = (new Season[] &#123; SPRING, SUMMER, AUTUMN, WINTER &#125;); &#125;&#125;]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>枚举</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单例模式]]></title>
    <url>%2F2018%2F08%2F04%2F%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[参考地址:http://www.hollischuang.com/archives/2498 反序列化破坏单例性参考地址:http://www.hollischuang.com/archives/1144 为什么会被破坏因为反序列化过程中,是通过反射调用无参构造函数来创建新的对象 实现可序列化的单例 1234567891011121314public class Singleton implements Serializable&#123; private volatile static Singleton singleton; private Singleton ()&#123;&#125; public static Singleton getSingleton() &#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125;&#125; 序列化和反序列化 1234567891011121314public class SerializableDemo1 &#123; public static void main(String[] args) throws IOException, ClassNotFoundException &#123; //Write Obj to file ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream("tempFile")); oos.writeObject(Singleton.getSingleton()); //Read Obj from file File file = new File("tempFile"); ObjectInputStream ois = new ObjectInputStream(new FileInputStream(file)); Singleton newInstance = (Singleton) ois.readObject(); //判断是否是同一个对象 System.out.println(newInstance == Singleton.getSingleton()); &#125;&#125;//false 解决方法在Singleton中,实现serializable 或者 externalizable接口的类中包含readResolve()方法，并在该方法中指定要返回的对象的生成策略，这样一来,在反序列化的过程中,使用反射生成新对象之前会判断一下有没有实现这个方法,如果实现了,就会使用过这个方法中的生成策略返回单例对象,这样就可以防止单例被破坏。 双重校验锁实现单例,并且实现安全的序列化123456789101112131415161718192021public class Singleton implements Serializable&#123; private volatile static Singleton instance; private Singleton() &#123;&#125; public static Singleton getInstance() &#123; if (instance == null) &#123; synchronized (Singleton.class) &#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125; //反序列化得到的instance是通过反射生成的新对象 //实现接口中的该方法后,可以防止反序列化破坏单例性,因为instance对象反序列化之后会调用该方法来返回,而不是反射生成新对象 private Object readResolve() &#123; return singleton; &#125;&#125; 枚举实现单例12345public enum Singleton &#123; INSTANCE; public void whateverMethod() &#123; &#125; &#125; 枚举实现单例的线程安全反编译枚举类得到代码发现,类中的几个属性和方法都是static类型的，因为static类型的属性会在类被加载之后被初始化,当一个Java类第一次被真正使用到的时候静态资源被初始化、Java类的加载和初始化过程都是线程安全的（因为虚拟机在加载枚举的类的时候，会使用ClassLoader的loadClass方法，而这个方法使用同步代码块保证了线程安全）。所以，创建一个enum类型是线程安全的。也就是说，我们定义的一个枚举，在第一次被真正用到的时候，会被虚拟机加载并初始化，而这个初始化过程是线程安全的。而我们知道，解决单例的并发问题，主要解决的就是初始化过程中的线程安全问题。所以，由于枚举的以上特性，枚举实现的单例是天生线程安全的。 枚举可解决反序列化会破坏单例的问题枚举的反序列化并不是通过反射实现的。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>单例模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代理模式]]></title>
    <url>%2F2018%2F08%2F04%2F%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[这是摘要 正向代理 正向代理的用途 突破访问限制通过代理服务器，可以突破自身IP访问限制，访问国外网站，教育网等。 提高访问速度 通常代理服务器都设置一个较大的硬盘缓冲区，会将部分请求的响应保存到缓冲区中，当其他用户再访问相同的信息时,则直接由缓冲区中取出信息，传给用户，以提高访问速度。 隐藏客户端真实IP上网者也可以通过这种方法隐藏自己的IP，免受攻击。 反向代理 隐藏服务器真实IP使用反向代理，可以对客户端隐藏服务器的IP地址。 负载均衡反向代理服务器可以做负载均衡，根据所有真实服务器的负载情况，将客户端请求分发到不同的真实服务器上。 提高访问速度反向代理服务器可以对于静态内容及短时间内有大量访问请求的动态内容提供缓存服务，提高访问速度。 提供安全保障反向代理服务器可以作为应用层防火墙，为网站提供对基于Web的攻击行为（例如DoS/DDoS）的防护，更容易排查恶意软件等。还可以为后端服务器统一提供加密和SSL加速（如SSL终端代理），提供HTTP访问认证等。 代理模式代理模式可以对代理对象进行增强,不过主要还是对代理对象的访问进行拦截 静态代理 创建代理的步骤: 首先定义一个代理类并实现要代理的接口 将要代理的接口声明为成员属性,一种是直接new一个要代理的对象(这种方式下使用代理的人甚至都不知道被代理对象的存在),另一种是将要代理的对象作为参数传入(这样就和装饰模式有点像) 定义代理类的构造函数,在构造函数张 实现接口的方法,并在方法中调用构造函数中传入的代理对象的方法,这个时候就可以对代理对象进行加强或者拦截 优点 在不改变目标对象的前提下,对目标对象的功能进行拓展 缺点 代理类和目标类几乎一样,目标类多的话,代理类也很多,代码几乎重复 因为代理类和和目标类要实现相同的接口,一旦接口中增加新功能,两个类都要维护 动态代理JDK动态代理 优点:代理类不用实现目标类实现的接口,但是要将目标类的接口作为参数传入 缺点目标类需要有实现的接口才行,不然无法使用JDK动态代理 创建代理对象的核心方法 1234//Proxy类的静态方法public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) //关键接口 用代理工厂类举例: 12345678910111213141516171819202122232425262728public class ProxyFactory&#123; //维护一个目标对象 private Object target; public ProxyFactory(Object target)&#123; this.target=target; &#125; //给目标对象生成代理对象 public Object getProxyInstance()&#123; return Proxy.newProxyInstance( target.getClass().getClassLoader(),//获取类加载器 target.getClass().getInterfaces(),//获取接口 new InvocationHandler() &#123;//实现匿名内部类,使用反射调用目标类方法,进行代理 @Override public Object invoke(Object proxy, //代理对象实例 Method method, //代理对象调用的目标对象的方法 Object[] args //方法参数 ) throws Throwable &#123; System.out.println("开始事务2"); //执行目标对象方法 Object returnValue = method.invoke(target, args); System.out.println("提交事务2"); return returnValue; &#125; &#125; ); &#125;&#125; 测试类 12345678910111213141516public class App &#123; public static void main(String[] args) &#123; // 目标对象 IUserDao target = new UserDao(); // 【原始的类型 class cn.itcast.b_dynamic.UserDao】 System.out.println(target.getClass()); // 给目标对象，创建代理对象 IUserDao proxy = (IUserDao) new ProxyFactory(target).getProxyInstance(); // class $Proxy0 内存中动态生成的代理对象 System.out.println(proxy.getClass()); // 执行方法 【代理对象】 proxy.save(); &#125;&#125; 参考地址:https://www.javazhiyin.com/173.html CGlib代理(子类代理) 优点:适用于要代理的业务类没有实现接口的情况,在内存中构建一个子类从而扩展目标类的功能,因此代理的目标类不能使final的 缺点 工厂 1234567891011121314151617181920212223242526272829public class ProxyFactory implements MethodInterceptor&#123; //维护目标对象 private Object target; public ProxyFactory(Object target) &#123; this.target = target; &#125; //给目标对象创建一个代理对象 public Object getProxyInstance()&#123; //1.工具类 Enhancer en = new Enhancer(); //2.设置父类 en.setSuperclass(target.getClass()); //3.设置回调函数,就是将实现了MethodInterceptor接口的对象传入 en.setCallback(this); //4.创建子类(代理对象) return en.create(); &#125; @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable &#123; System.out.println("开始事务..."); //执行目标对象的方法 Object returnValue = method.invoke(target, args); System.out.println("提交事务..."); return returnValue; &#125;&#125; 测试类 12345678910public class App &#123; public void test()&#123; //目标对象 UserDao target = new UserDao(); //代理对象 UserDao proxy = (UserDao)new ProxyFactory(target).getProxyInstance(); //执行代理对象的方法 proxy.save(); &#125;&#125; 代理模式的应用比如AOP，比如过滤器、拦截器等。在我们平时使用的框架中，像servlet的filter、包括spring提供的aop以及struts2的拦截器都使用了动态代理功能。我们日常看到的mybatis分页插件，以及日志拦截、事务拦截、权限拦截等. 在Spring的AOP编程中 如果目标类实现了接口,使用JDK动态代理 如果没有实现接口,使用CGlib动态代理]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>反向代理</tag>
        <tag>代理模式</tag>
        <tag>正向代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法真题1]]></title>
    <url>%2F2018%2F07%2F31%2F%E7%AE%97%E6%B3%95%E7%9C%9F%E9%A2%981%2F</url>
    <content type="text"><![CDATA[这是摘要 网易真题数对 矩形重叠 牛牛找工作 拼多多竖着的棋盘 回合制游戏 拼词游戏 循环小数 阿里真题运送范围]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[左程云算法课进阶班2]]></title>
    <url>%2F2018%2F07%2F29%2F%E5%B7%A6%E7%A8%8B%E4%BA%91%E7%AE%97%E6%B3%95%E8%AF%BE%E8%BF%9B%E9%98%B6%E7%8F%AD2%2F</url>
    <content type="text"><![CDATA[这是摘要 LRU缓存算法(Least Recently Used)(进阶5)使用哈希表和双向链表实现 定制Node 123456public static class Node&lt;K,V&gt; &#123; public K key; public V value; public Node&lt;K,V&gt; last; public Node&lt;K,V&gt; next;&#125; 定制双向链表 12345678910111213141516public static class NodeDoubleLinkedList&lt;K,V&gt; &#123; private Node&lt;K,V&gt; head;//头指针 private Node&lt;K,V&gt; tail;//尾指针 public NodeDoubleLinkedList() &#123;&#125; //构造方法 public void addNode(Node&lt;K,V&gt; newNode) &#123;&#125; //增加节点到尾部 public void moveNodeToTail(Node&lt;K,V&gt; node) &#123;//使用value,将对应的Node移到尾部 // 1. node本来就是尾部 // 2. node是头部 // 3. node在中间 &#125; public Node&lt;K,V&gt; removeHead() &#123; //cache满了,将头部的Node移除 //1. 头指针为null //2. list只有一个节点 //3. 普通情况 &#125;&#125; lruCache的结构 1234567891011121314public static class MyCache&lt;K, V&gt; &#123; private HashMap&lt;K, Node&lt;K,V&gt;&gt; keyNodeMap; //保存key对应的Node,方便从找到地址,不用遍历链表,空间换时间 private NodeDoubleLinkedList&lt;K,V&gt; nodeList; //双向链表,维护更新状态 private int capacity; public MyCache(int capacity) &#123;&#125; //构造方法 public V get(K key) &#123;&#125; //从keyNodeMap中取得对应的Node,并将对应的Node移到链表尾部 public void set(K key, V value) &#123; //1. 如果缓存中已经包含了key,直接从缓存中取node,并将node的value更新,然后将node移到链表尾部 //2. 否则,生成新的node,判断容量大小,超过则先删除最不常用的记录,然后添加到两个map,以及list &#125; private void removeMostUnusedCache() &#123;&#125; //移除list的头部,并且移除两个map中的记录&#125; LFU缓存算法(Least Frequently Used)(进阶6) 横着的双向链表每个节点代表使用次数,次数多的在尾部 竖着的节点代表元素,新加入的在尾部 加入元素的时候判断结构中有没有这个元素,没有的话加入1那一列的尾部;有的话加入当前次数加一的一列 获取元素的时候如果没有返回null 删除元素选择次数最少的那一列的尾元素 列节点定义 1234567public static class Node &#123; public Integer key; public Integer value; public Integer times; public Node up; public Node down;&#125; 列链表定义 123456789101112131415public static class NodeList &#123; public Node head; //列链表的头 public Node tail; //列链表的尾 public NodeList last; //前一个列链表 public NodeList next; //后一个列链表 public void addNodeFromHead(Node newHead) &#123;&#125; //在头部增加node,所以头部的节点优先级高 public boolean isEmpty() &#123;&#125; //列链表是否为空 public void deleteNode(Node node) &#123;//删除列链表中一个指定节点 //1. 判断是不是只有一个节点了 //2. 判断是不是头结点 //3. 判断是不是尾节点 //4. 普通节点 &#125; &#125; lfuCache结构 12345678910111213141516171819202122232425public static class LFUCache &#123; private int capacity;//指定容量 private int size;//当前元素数量 private HashMap&lt;Integer, Node&gt; records;//key-&gt;node private HashMap&lt;Node, NodeList&gt; heads;//node-&gt;列链表 private NodeList headList;//次数最少的列链表,方便删除 public LFUCache(int capacity)&#123;&#125;//规定大小 public void set(int key, int value) &#123; if (records.containsKey(key)) //如果结构中存在key,取出节点,放到下一个列链表 else //如果不包含 if (size == capacity) //如果满了,从头列链表中移除尾巴,更新头列链表,从map中移除该node相关 //新建node if (headList == null) //如果头列链表空,新建头列链表 else //否则 if (headList.head.times.equals(node.times)) //判断当前头列链表次数是不是1次 else //不是的话,更新头列链表 //将node放进两个map,size++ &#125; private void move(Node node, NodeList oldNodeList) &#123;&#125;//将node从oldNodeList中取出,放到下一个列链表中 private boolean modifyHeadList(NodeList nodeList) &#123;&#125;//判断列链表是不是变空了,是的话更新列链表的前后 public int get(int key) &#123;&#125;//获取value&#125; 正则匹配问题(进阶8) cn.hn.advanced.RegularExpressionMatch 递归函数 f(i,j)表示str从i位置开始,exp从j位置开始能不能配上j+1位置有三种情况 越界 是* 如果str[i]!=exp[j],那么要看f(i,j+2) 如果str[i]==exp[j]或者exp[j]是.,那么看exp[j+1]能匹配多少个 不是*那么i位置和j位置必须能匹配,也就是说str[i]==exp[j]或者exp[j]==.,然后进入子过程看i+1和j+1位置 环形单链表的约瑟夫问题(google级别)(进阶8) cn/hn/advanced/JosephusProblem.java 累加和为aim的最长子数组(数组中的数字包括正负0)(进阶4-1)(进阶8) HashMap就行,时间复杂度O(N),额外空间O(N) 遍历数组,每遍历一个位置累加一个数字,如果sum没出现过,将sum和下标存进map.假设当前位置遍历到4,sum为14,想要知道0到3位置中哪个位置到位置4的累加和为7且距离最长,只要知道哪个位置的sum为(14-7)且最小即可 当数组中都是正数或者负数的时候,时间复杂度O(N),空间O(1),(进阶8) cn/hn/advanced/LongestSumSubArrayLengthInPositiveArray.java 用指针L和R维护一个窗口,用sum表示窗口中的数字的和 当sum&lt;=aim值的时候,R往右边扩大, 当sum&gt;=aim值的时候,L往右边扩大 每次扩大都计算sum值,==aim且(R-L)&gt;上一次等于aim的窗口长度则更新长度 变形 一个数组中有奇数和偶数,求奇数和偶数数量相等的最长子数组 数组异或和 累加和小于aim的最长子数组(包含正负0)(进阶8) cn/hn/advanced/LongestSubarrayLessSumAwesomeSolution.java 准备两个数组 一个数组存放:先求数组中每个数字开头往右累加能得到的最小累加和 另一个数组存放:每个数字最小累加和加到哪个右边界 利用前面两个数组的信息,求原数组中0位置的开头的满足条件(假设aim=6)的最长子数组,就是0位置开始往右边扩,直接可以得到0位置开头的最小累加和为-3,扩到位置3,因为位置4开头的最小累加和为4,加进来之后累加和为1,还是小于aim,然后继续右扩到位置6,位置7开头的最小累加和为9,加进来后累加和为10,超过aim,所以位置0开头的最长子数组位置只能扩到位置6 然后右移L,将位置0的数字排除,看右边能不能继续扩,如果不能的话,说明比之前找到的位置0开头的数组短,直接不用考虑 机器人最终位置(阿里)(进阶7) 给你1到N位置,一开始机器人在M位置(1&lt;=M&lt;=N),问机器人走P步之后能到达K位置的方法数量. 暴力递归 1234567891011121314151617//N:右边界//M:来的位置//P:剩余步数//K:目标位置public int ways(int N,int M,int P,int K)&#123; if(N &lt; 1 || M &gt; N || M &lt; 1 || P &lt; 0 || K &lt;1 || K &gt; N) return 0; if (P == 0) &#123; return M == K ? 1 : 0; &#125; if (M == 1) &#123;//在左边界,只能向右走 return ways(N,M+1,P-1,K); &#125;else if (M == N) &#123; //在右边界,只能向左走 return ways(N,M-1,P-1,K); &#125;else&#123; return ways(N,M+1,P-1,K) + ways(N,M-1,P-1,K); &#125;&#125; 改dp就是一个杨辉三角形 左边界的值就是右上方的值 右边界的值就是左上方的值 普通位置的值就是左上方加右上方123456789101112131415161718192021222324252627282930public int betterWays(int right, int now, int rest, int target) throws Exception &#123; if (right &lt; 1 || now &gt; right || now &lt; 1 || rest &lt; 0 || target &lt; 1 || target &gt; right) return 0; int[][] dp = new int[rest+1][right+1]; //剩余步数为0的时候只有target位置值为1,其他位置都是0 for(int i = 0; i &lt;= right; i++) &#123; dp[0][i] = 0; &#125; dp[0][target] = 1; calculate(dp, rest, now); return dp[rest][now];&#125;//递归计算二维dp表中某个位置的值public int calculate(int[][] dp, int row, int col) throws Exception&#123; if (row &gt;= dp.length || col &gt;= dp[0].length) &#123; throw new Exception("out of bound"); &#125; if (row == 0) &#123; return dp[row][col]; &#125; if (col == 1) &#123; dp[row][col] = calculate(dp, row - 1, col + 1); return dp[row][col]; &#125; else if (col == dp[0].length - 1) &#123; dp[row][col] = calculate(dp, row - 1, col - 1); return dp[row][col]; &#125; else &#123; dp[row][col] = calculate(dp, row - 1, col + 1) + calculate(dp, row - 1, col - 1); return dp[row][col]; &#125;&#125; 参数8 4 6 6的结果 排成一线的纸牌博弈问题(进阶7) cn.hn.advanced.CardsInLine 纸牌博弈暴力递归分析 定义f(int[] arr,int i,int j)表示从数组的i到j位置上,先拿获得什么分数 123456789public int f(int[] arr,int i,int j)&#123; if (i == j) &#123; return arr[i];//只有一张牌了,先拿的直接拿走,直接返回 &#125; //先拿的人有两种选择,要么先拿i位置的,要么先拿j位置的, //拿走一个以后在剩下的数组中他就成了后拿的人, //获得的分数就是在剩下的数组中后拿获得的分数加上左边或者右边中最大的 return Math.max(arr[i] + s(arr,i+1,j), arr[j] + s(arr,i,j-1));&#125; 定义s(int[] arr,int i,int j)表示从数组的i到j位置上,后拿获得什么分数 12345678public int s(int[] arr,int i, int j)&#123; if (i == j) &#123; return 0;//只有一张牌,后拿的人什么也拿不到 &#125; //后拿的人只能从先拿的人选剩下的结果中拿,必然剩下分数小的 //对手拿走之后,自己就成了先拿的人 return Math.min(f(arr,i+1,j),f(arr,i,j-1));&#125; 改成dp 因为i==就是终止位置,f表和s表中的对角线上的值是确定的 f(i,j) = max(s(i+1,j) + arr[i], s(i,j-1) + arr[j]) s(i,j) = min(f(i+1,j) , f(i,j-1)) 换钱的方法数(进阶7) cn/hn/advanced/CoinsWay.java 尝试版本使用count张arr[start],求拼凑剩下的金额的方法数 优化版本1因为一旦确定了aim和start,返回值就是确定的,在递归过程中用一个map将(aim_start,result)保存起来,然后遇到的时候直接取出来计算 优化版本2通过递归过程推出二维表 再优化版实际上,在二维表中(i,aim)位置的结果就是下面和左边某个结果的和 计算公式(进阶6) 如果没有括号,只有加减乘除,那么可以用栈来做,将数字和符号放入栈,遇到乘除,计算好在放进去 遇到左括号,递归进入子过程,计算到匹配的右括号返回 求子数组的最大异或和(进阶7) 用前缀树解,(0异或到start-1)^(start异或到i) = (0异或到i)所以(start异或到i) = (0异或到start-1)^(0异或到i)所以(0异或到start-1)^(0异或到i)的最大值就是答案所以用前缀树装着(0异或到start-1)的结果,eor在遍历过程中更新(0异或到i)的结果,知道eor,只要知道前缀树中和eor异或最大的是哪一个就行了1111是-11011是-5所以负数也是高位一越多越大]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tcp和udp]]></title>
    <url>%2F2018%2F07%2F28%2Ftcp%E5%92%8Cudp%2F</url>
    <content type="text"><![CDATA[这是摘要 tcp和udp比较 tcp协议 TCP支持的应用协议主要有：Telnet、FTP、SMTP,http,pop3等； 流量控制参考地址:https://blog.csdn.net/yechaodechuntian/article/details/25429143使用滑动窗口来控制流量,限制发送端可以发送的数据量 拥塞控制发送方控制拥塞窗口的原则是：只要网络没有出现拥塞，拥塞窗口就再增大一些，以便把更多的分组发送出去。但只要网络出现拥塞，拥塞窗口就减小一些，以减少注入到网络中的分组数。 慢开始和拥塞避免 当 cwnd &lt; ssthresh 时，使用慢开始算法,每次经过一轮传输,cwnd就扩大一倍。 当 cwnd &gt; ssthresh 时，停止使用慢开始算法而改用拥塞避免算法。 当 cwnd = ssthresh 时，既可使用慢开始算法，也可使用拥塞控制避免算法。 当网络出现拥塞的时候,将ssthresh设置成当前cwnd的一半(但是不能小于2),重新执行慢开始算法 慢开始算法一开始将cwnd设置为1,每经过一个传输轮次，拥塞窗口 cwnd 就加倍。 拥塞避免算法让拥塞窗口cwnd缓慢地增大，即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1，而不是加倍。这样拥塞窗口cwnd按线性规律缓慢增长，比慢开始算法的拥塞窗口增长速率缓慢得多。 快重传和快恢复 快重传算法首先要求接收方每收到一个失序的报文段后就立即发出重复确认（为的是使发送方及早知道有报文段没有到达对方）而不要等到自己发送数据时才进行捎带确认。 当发送方连续收到3次重复确认消息,就会直接发送对应的数据包,并且将ssthresh减半,开始使用快恢复算法(即从新的ththresh开始拥塞避免算法) 应用场景优点:可靠稳定,通过三次握手建立连接,有流量控制,差错控制等机制(握手 确认 窗口 拥塞控制 重传等)缺点:传输速度慢,效率低,占用系统资源高,容易被攻击(DOS,DDOS,CC,通过大量建立无效连接来耗光服务器资源) 文件传输:FTP 邮件:SMTP putty:ssh,TELNET 浏览器:httpDOS攻击:denial of serviceDDOS攻击:distributed denial of service参考地址:https://www.jianshu.com/p/dff5a0d537d8 建立连接过程(3次握手) C发送一个请求连接的位码SYN和一个随机产生的序列号给Seq，然后S收到了这些数据。 S收到了这个请求连接的位码，啊呀，有人向我发出请求了么，那我要不要接受他的请求，得实现确认一下，于是，发送了一个确认码 ACN（seq+1），和SYN，Seq给C，然后C收到了，这个是第二次连接。 C收到了确认的码和之前发送的SYN一比较，偶哟，对上了么，于是他又发送了一个ACN（SEQ+1）给S，S收到以后就确定建立连接，至此，TCP连接建立完成。为什么要3次:一端(client)A发出去的第一个连接请求报文并没有丢失，而是因为某些未知的原因在某个网络节点上发生滞留，导致延迟到连接释放以后的某个时间才到达另一端(server)B。本来这是一个早已失效的报文段，但是B收到此失效的报文之后，会误认为是A再次发出的一个新的连接请求，于是B端就向A又发出确认报文，表示同意建立连接。如果不采用“三次握手”，那么只要B端发出确认报文就会认为新的连接已经建立了，但是A端并没有发出建立连接的请求，因此不会去向B端发送数据，B端没有收到数据就会一直等待，这样B端就会白白浪费掉很多资源。如果采用“三次握手”的话就不会出现这种情况，B端收到一个过时失效的报文段之后，向A端发出确认，此时A并没有要求建立连接，所以就不会向B端发送确认，这个时候B端也能够知道连接没有建立。 断开连接过程(4次挥手) 客户端进程发出连接释放报文，并且停止发送数据。释放数据报文首部，FIN=1，其序列号为seq=u（等于前面已经传送过来的数据的最后一个字节的序号加1），此时，客户端进入FIN-WAIT-1（终止等待1）状态。 TCP规定，FIN报文段即使不携带数据，也要消耗一个序号。 服务器收到连接释放报文，发出确认报文，ACK=1，ack=u+1，并且带上自己的序列号seq=v，此时，服务端就进入了CLOSE-WAIT（关闭等待）状态。TCP服务器通知高层的应用进程，客户端向服务器的方向就释放了，这时候处于半关闭状态，即客户端已经没有数据要发送了，但是服务器若发送数据，客户端依然要接受。这个状态还要持续一段时间，也就是整个CLOSE-WAIT状态持续的时间。 客户端收到服务器的确认请求后，此时，客户端就进入FIN-WAIT-2（终止等待2）状态，等待服务器发送连接释放报文（在这之前还需要接受服务器发送的最后的数据）。 服务器将最后的数据发送完毕后，就向客户端发送连接释放报文，FIN=1，ack=u+1，由于在半关闭状态，服务器很可能又发送了一些数据，假定此时的序列号为seq=w，此时，服务器就进入了LAST-ACK（最后确认）状态，等待客户端的确认。 客户端收到服务器的连接释放报文后，必须发出确认，ACK=1，ack=w+1，而自己的序列号是seq=u+1，此时，客户端就进入了TIME-WAIT（时间等待）状态。注意此时TCP连接还没有释放，必须经过2∗∗MSL（最长报文段寿命）的时间后，当客户端撤销相应的TCB后，才进入CLOSED状态。 服务器只要收到了客户端发出的确认，立即进入CLOSED状态。同样，撤销TCB后，就结束了这次的TCP连接。可以看到，服务器结束TCP连接的时间要比客户端早一些。为了确保数据能够完成传输。关闭连接时，当收到对方的FIN报文通知时，它仅仅表示对方没有数据发送给你了；但未必你所有的数据都全部发送给对方了，所以你可以未必会马上会关闭SOCKET,也即你可能还需要发送一些数据给对方之后，再发送FIN报文给对方来表示你同意现在可以关闭连接了，所以它这里的ACK报文和FIN报文多数情况下都是分开发送的。 udp协议 UDP支持的应用层协议主要有：NFS（网络文件系统）、SNMP（简单网络管理协议）、DNS（主域名称系统）、TFTP（通用文件传输协议）等。 应用场景优点:因为是无状态传输协议,所以传输速度很快;因为没有各种控制机制,容易被利用的漏洞较少(但是也有UDP FLOOD攻击)缺点:不稳定,不可靠,网络条件差的情况下容易丢包 在线视频: 语音通话 TFTP]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>tcp</tag>
        <tag>udp</tag>
        <tag>流量控制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[http协议]]></title>
    <url>%2F2018%2F07%2F28%2Fhttp%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[这是摘要 http协议http请求方法 GET(幂等,无副作用,不能带Request Body)获取资源 POST(非幂等,有副作用,能带Request Body)新建或者更新资源RFC文档例举的使用场景: 123456- Annotation of existing resources;- Posting a message to a bulletin board, newsgroup, mailing list, or similar group of articles;- Providing a block of data, such as the result of submitting a form, to a data-handling process;- Extending a database through an append operation. PUT(幂等,有副作用,能带Request Body)更新资源,如修改帖子,同一个修改不管提交几次都是一样的结果 DELETE(幂等,有副作用,不能带Request Body)删除资源 PATCH()更新资源的一部分 HEADHEAD方法跟GET方法相同，只不过服务器响应时不会返回消息体。一个HEAD请求的响应中，HTTP头中包含的元信息应该和一个GET请求的响应消息相同。这种方法可以用来获取请求中隐含的元信息，而不用传输实体本身。也经常用来测试超链接的有效性、可用性和最近的修改。 关于GET,POST,PUT的区别 很多浏览器对GET请求的URI长度有限制 GET请求是安全的(对服务器而言),因为不会改变什么 请求和响应http1.0规定请求头或者响应头的数据格式必须是ASCII码,后面的数据可以是任意类型 请求 1234567GET / HTTP/1.1 //第一行指定请求方法和协议版本号,后面的都是描述客户端情况Transfer-Encoding: chunkedAccept-Encoding: gzip, deflate //指定可以接受的压缩类型Accept-Language: zh-CN,zh;q=0.8Accept-Charset: GBK,utf-8;q=0.7,*;q=0.3Connection: close //明确要求服务器关闭tcp连接,在http1.0还只有短连接的时候请求头中需要声明keep-alive来保持连接Accept: */* //指定客户端可以接收的数据类型 响应 12345678910HTTP/1.1 200 OK 第一行是协议版本和状态码Content-Type: text/plain //说明response-body中的数据格式Content-Length: 137582 //本次回应的数据长度,1.1中一个tcp连接可以回应多个请求Transfer-Encoding: chunked //分块传输,返回数据长度不定Server: Apache 0.84 //服务器类型Content-Encoding: gzip //说明数据的压缩类型&lt;html&gt; &lt;body&gt;Hello World&lt;/body&gt;&lt;/html&gt; 常见http协议状态码123456789101112200 OK - [GET]：服务器成功返回用户请求的数据，该操作是幂等的（Idempotent）。201 CREATED - [POST/PUT/PATCH]：用户新建或修改数据成功。202 Accepted - [*]：表示一个请求已经进入后台排队（异步任务）204 NO CONTENT - [DELETE]：用户删除数据成功。400 INVALID REQUEST - [POST/PUT/PATCH]：用户发出的请求有错误，服务器没有进行新建或修改数据的操作，该操作是幂等的。401 Unauthorized - [*]：表示用户没有权限（令牌、用户名、密码错误）。403 Forbidden - [*] 表示用户得到授权（与401错误相对），但是访问是被禁止的。404 NOT FOUND - [*]：用户发出的请求针对的是不存在的记录，服务器没有进行操作，该操作是幂等的。406 Not Acceptable - [GET]：用户请求的格式不可得（比如用户请求JSON格式，但是只有XML格式）。410 Gone -[GET]：用户请求的资源被永久删除，且不会再得到的。422 Unprocesable entity - [POST/PUT/PATCH] 当创建一个对象时，发生一个验证错误。500 INTERNAL SERVER ERROR - [*]：服务器发生错误，用户将无法判断发出的请求是否成功。 Httpsaes与sra加密 aes是对称加密,秘钥只有一个,一般是256bit,优点是加密速度快,缺点是在不安全的网络中传输容易别窃取 sra是非对称加密,分为公钥和私钥,公钥在网络中传输,私钥安全保存,优点是网络传输更安全,缺点是加密速度相对较慢 结合两者:将公钥给对方,让对方用公钥加密aes的秘钥,然后发回给自己,自己通过私钥解密获得aes秘钥后加密数据发给对方,对方就能用公用的aes秘钥解密,这样既能保证加密速度,又能保证秘钥传输安全 SSL层实现安全连接 Socket通信 java封装类 阻塞和非阻塞 Java服务器网络开发时， 请说明通讯中阻塞（blocking）/非阻塞（non-blocking 与 同步/异步IO的区别。 同步／异步主要针对客户端： 同步：就是当客户端发出一个功能调用时，在没有得到结果之前，该调用就不返回。也就是说必须一件一件的事情去做，等一件做完了才能去做下一件。 异步：就是当客户端发出一个功能调用时，调用者不用等接收方发出响应。实际处理这个调用的部件在完成后，会通过状态，通知和回调来通知调用者。客户端可以接着去做 后面的事情。 虽然主要是针对客户端，但是服务器端不是完全没有关系的，同步／异步必须配合服务器端才能实现。同步／异步是由客户端自己控制，但是服务器端是否阻塞/非阻塞，客户端完全不需要关心。 阻塞／非阻塞主要是针对服务器端： 阻塞：阻塞调用是指服务器端被调用者调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。 非阻塞：指在不能立即得到结果之前，该调用不会阻塞当前线程。 NIO的使用 HTTPS 握手机制： 客户端 向服务器发起请求。 服务端 取出公有密钥及证书并发送给客户端。 客户端判断公有密钥是否有效，无效则显示警告。有效则生成一个随机数串，并以此生成客户端的对称密钥。 用步骤 2 到的公有密钥对该随机数串加密，发送到服务器。 服务器得到加密报文，用私有密钥解密报文，得到随机数串，并以此生成服务器端的对称密钥。此时客户端和服务端拥有相同的对称密钥，可以用该对称密钥进行安全通信。 服务器对响应进行加密，客户端对报文进行解密。 跨站请求伪造CSRF:Cross—Site Request Forgery参考地址:http://blog.csdn.net/stpeace/article/details/53512283过程: me登录网易邮箱 hack制作了钓鱼网站 me在没有登出邮箱的时候访问了钓鱼网站 于是钓鱼网站通过me的浏览器访问了网易邮箱,带着me的cookie(此时me的浏览器与网易邮箱网站服务器之间的session尚未过期)进行了一些操作 防御CSRF攻击 验证HTTP referer字段HTTP referer指向前一个网页的URL,而hack要发起跨站请求伪造的话只能通过他的钓鱼网站发起,合法的HTTP referer一般都是网易邮箱的页面的URL.但是这种方法依赖于浏览器的安全性和设置,浏览器可以设置成不带上referer值 在请求地址中添加token并验证服务器生成token发给客户端,然后客户端发起请求的时候带上tokenGET将token放在URL后面POST则将请求放在form标签最后加上&lt;input type=&quot;hidden&quot; name=&quot;token&quot; value=&quot;tokenValue&quot;&gt;但是一些论坛网站中token很难保证安全,以为用户可以自己发帖子,然后帖子中的链接就会带上token 在HTTP头中增加自定义属性并验证 SSL证书 域名型SSL证书(DVSSL Domain Validation SSL) 审核内容:域名 管理权限 证书详情:使用者身份只显示某某域名 一般用途:个人站点 IOS应用分发下载 登录等纯HTTPS加密需求的链接 企业型SSL(OVSSL Organization Validation SSL) 审核内容:域名管理权限 企业名称,地址,电话等信息的真实性 证书详情:使用证身份显示公司名称 一般用途:企业站点 增强型SSL(EVSSL extend Validation SSL) 审核内容:域名管理权限 企业名称地址电话等信息的真实性 第三方数据库的审查,例如邓白氏,114查号台,律师证明等 证书详情:显示使用者显示公司名称 一般用途:企业官网,电商,P2P等互联网金融网站]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>http</tag>
        <tag>https</tag>
        <tag>CSRF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5层网络模型]]></title>
    <url>%2F2018%2F07%2F28%2F5%E5%B1%82%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[这是摘要 OSI七层协议模型 五层网络模型 在向下的过程中，需要添加下层协议所需要的首部或者尾部 中间经过路由器的时候为了读取MAC地址和IP地址会拆包,拆到网络层 在向上的过程中不断拆开首部和尾部 对比 设计者角度参考地址:http://www.ruanyifeng.com/blog/2012/05/internet_protocol_suite_part_i.html 物理层电缆连接计算机,传输电信号, 数据链路层(MAC地址) 以太网协议(Ethernet):定义01信号的意义 MAC地址:前6个十六进制数是厂商编号，后6个是该厂商的网卡流水号。有了MAC地址，就可以定位网卡和数据包的路径了。 广播:子网中进行广播 网络层(IP地址)简历主机到主机之间的通信 由于一台电脑不可能接受世界上所有电脑的数据包,所以全世界广播是不可能的,需要新的寻址方法 “网络层”出现以后，每台计算机有了两种地址，一种是MAC地址，另一种是网络地址。两种地址之间没有任何联系，MAC地址是绑定在网卡上的，网络地址则是管理员分配的，它们只是随机组合在一起。 网络层使用IP协议,根据IP协议找到子网络,在子网络中通过MAC地址找到具体的计算机 互联网上的每一台计算机，都会分配到一个IP地址。这个地址分成两个部分，前一部分代表网络，后一部分代表主机。比如，IP地址172.16.254.1，这是一个32位的地址，假定它的网络部分是前24位（172.16.254），那么主机部分就是后8位（最后的那个1）。 通过IP地址和子网掩码来确定网段比如，已知IP地址172.16.254.1和172.16.254.233的子网掩码都是255.255.255.0，请问它们是否在同一个子网络？两者与子网掩码分别进行AND运算，结果都是172.16.254.0，因此它们在同一个子网络。 如何获取目标IP地址主机的MAC地址 如果不在同一个子网,那么只能通过网关(gateway)处理 如果在同一个子网则通过ARP(Address Resolution Protocol)协议,也是通过广播询问对方(某个IP地址)的MAC地址 传输层(端口号)“传输层”的功能，就是建立”端口到端口”的通信。只要确定主机和端口，我们就能实现程序之间的交流。 UDP协议和TCP协议 应用层(http协议)应用程序收到”传输层”的数据，接下来就要进行解读。由于互联网是开放架构，数据来源五花八门，必须事先规定好格式，否则根本无法解读。“应用层”的作用，就是规定应用程序的数据格式。 用户角度参考地址:http://www.ruanyifeng.com/blog/2012/06/internet_protocol_suite_part_ii.html 电脑是如何上网的 设置静态IP也就是手动设置固定的: IP地址 子网掩码 网关的IP地址 还有DNS的IP地址 设置动态IP(DHCP协议)这个协议规定，每一个子网络中，有一台计算机负责管理本网络的所有IP地址，它叫做”DHCP服务器”。新的计算机加入网络，必须向”DHCP服务器”发送一个”DHCP请求”数据包，申请IP地址和相关的网络参数。 数据包格式 最前面的”以太网标头”，设置发出方（本机）的MAC地址和接收方（DHCP服务器）的MAC地址。前者就是本机网卡的MAC地址，后者这时不知道，就填入一个广播地址：FF-FF-FF-FF-FF-FF。 后面的”IP标头”，设置发出方的IP地址和接收方的IP地址。这时，对于这两者，本机都不知道。于是，发出方的IP地址就设为0.0.0.0，接收方的IP地址设为255.255.255.255。然后广播的时候DHCP服务器就知道这个数据包是发给自己的 最后的”UDP标头”，设置发出方的端口和接收方的端口。这一部分是DHCP协议规定好的，发出方是68端口，接收方是67端口。 原理这个数据包构造完成后，就可以发出了。以太网是广播发送，同一个子网络的每台计算机都收到了这个包。因为接收方的MAC地址是FF-FF-FF-FF-FF-FF，看不出是发给谁的，所以每台收到这个包的计算机，还必须分析这个包的IP地址，才能确定是不是发给自己的。当看到发出方IP地址是0.0.0.0，接收方是255.255.255.255，于是DHCP服务器知道”这个包是发给我的”，而其他计算机就可以丢弃这个包。接下来，DHCP服务器读出这个包的数据内容，分配好IP地址，发送回去一个”DHCP响应”数据包。这个响应包的结构也是类似的，以太网标头的MAC地址是双方的网卡地址，IP标头的IP地址是DHCP服务器的IP地址（发出方）和255.255.255.255（接收方），UDP标头的端口是67（发出方）和68（接收方），分配给请求端的IP地址和本网络的具体参数则包含在Data部分。 DNS(domain name service)协议IP 8.8.8.8 端口 53获取域名对应的IP地址,现在开始,假设要浏览器要访问Google: 首先会通过DNS得到Google服务器的IP地址 判断得到不是同一个网段,所以请求需要通过网关转发处理 于是确定是数据包的目的IP地址为Google服务器的IP地址,目的MAC地址为同网关的MAC地址,目的端口号80 走5层网络模型 应用层浏览器根据http协议,打包数据 传输层按照TCP协议,打包数据,加入端口号 网络层按照IP协议打包数据,加入IP地址 数据链路层按照以太网协议打包数据,加入MAC地址 物理层传输电信号因为以太网协议的数据包最大长度为1500字节,所有请求可能会被分为几个数据包来发送 Google服务器收到请求将收到的数据包按照IP头标号顺序拼接,读取最终的数据后,进行处理并返回response]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>IP地址</tag>
        <tag>MAC地址</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK8新特性]]></title>
    <url>%2F2018%2F07%2F25%2FJDK8%E6%96%B0%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[这是摘要 lambda表达式将函数作为一个方法的参数进行传递,其实就是原来的用匿名内部类实现接口作为参数传递,换了一种写法. 函数式接口函数式接口(Functional Interface)就是一个有且仅有一个抽象方法，但是可以有多个非抽象方法的接口。1234567interface Hello&#123; void sayHi(String name); default void sayBye(String name,String word)&#123; System.out.println(name + word); &#125;&#125; 函数式接口可以被隐式转换为lambda表达式。123String name = "Jack";Hello hello = name1 -&gt; System.out.println("hi " + name1);hello.sayHi(name); 函数式编程stream常用方法 map 123List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);// 获取对应的平方数List&lt;Integer&gt; squaresList = numbers.stream().map( i -&gt; i*i).distinct().collect(Collectors.toList()); filter 12List&lt;String&gt; strings = Arrays.asList("abc", "", "bc", "efg", "abcd","", "jkl");List&lt;String&gt; filtered = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.toList()); forEach 123Random random = new Random();//limit 方法用于获取指定数量的流random.ints().limit(10).forEach(System.out::println); sorted 12Random random = new Random();random.ints().limit(10).sorted().forEach(System.out::println); 转并行 123List&lt;String&gt; strings = Arrays.asList("abc", "", "bc", "efg", "abcd","", "jkl");// 获取空字符串的数量int count = strings.parallelStream().filter(string -&gt; string.isEmpty()).count(); Collectors 123456List&lt;String&gt;strings = Arrays.asList("abc", "", "bc", "efg", "abcd","", "jkl");List&lt;String&gt; filtered = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.toList()); System.out.println("筛选列表: " + filtered);String mergedString = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.joining(", "));System.out.println("合并字符串: " + mergedString); 统计 123456List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);IntSummaryStatistics stats = integers.stream().mapToInt((x) -&gt; x).summaryStatistics();System.out.println("列表中最大的数 : " + stats.getMax());System.out.println("列表中最小的数 : " + stats.getMin());System.out.println("所有数之和 : " + stats.getSum());System.out.println("平均数 : " + stats.getAverage()); FP接口默认方法首先，之前的接口是个双刃剑，好处是面向抽象而不是面向具体编程，缺陷是，当需要修改接口时候，需要修改全部实现该接口的类，目前的java 8之前的集合框架没有foreach方法，通常能想到的解决办法是在JDK里给相关的接口添加新的方法及实现。然而，对于已经发布的版本，是没法在给接口添加新方法的同时不影响已有的实现。所以引进的默认方法。他们的目的是为了解决接口的修改与现有的实现不兼容的问题。 新的日期API 本地化日期API 123456789101112131415161718192021222324252627282930313233343536373839404142import java.time.LocalDate;import java.time.LocalTime;import java.time.LocalDateTime;import java.time.Month; public class Java8Tester &#123; public static void main(String args[])&#123; Java8Tester java8tester = new Java8Tester(); java8tester.testLocalDateTime(); &#125; public void testLocalDateTime()&#123; // 获取当前的日期时间 LocalDateTime currentTime = LocalDateTime.now(); System.out.println("当前时间: " + currentTime); //当前时间: 2016-04-15T16:55:48.668 LocalDate date1 = currentTime.toLocalDate(); System.out.println("date1: " + date1); //date1: 2016-04-15 Month month = currentTime.getMonth(); int day = currentTime.getDayOfMonth(); int seconds = currentTime.getSecond(); System.out.println("月: " + month +", 日: " + day +", 秒: " + seconds);//月: APRIL, 日: 15, 秒: 48 LocalDateTime date2 = currentTime.withDayOfMonth(10).withYear(2012); System.out.println("date2: " + date2); //date2: 2012-04-10T16:55:48.668 // 12 december 2014 LocalDate date3 = LocalDate.of(2014, Month.DECEMBER, 12); System.out.println("date3: " + date3); //date3: 2014-12-12 // 22 小时 15 分钟 LocalTime date4 = LocalTime.of(22, 15); System.out.println("date4: " + date4); //date4: 22:15 // 解析字符串 LocalTime date5 = LocalTime.parse("20:15:30"); System.out.println("date5: " + date5); //date5: 20:15:30 &#125;&#125; 使用时区的日期API 12345678910111213141516171819202122import java.time.ZonedDateTime;import java.time.ZoneId; public class Java8Tester &#123; public static void main(String args[])&#123; Java8Tester java8tester = new Java8Tester(); java8tester.testZonedDateTime(); &#125; public void testZonedDateTime()&#123; // 获取当前时间日期 ZonedDateTime date1 = ZonedDateTime.parse("2015-12-03T10:15:30+05:30[Asia/Shanghai]"); System.out.println("date1: " + date1); //date1: 2015-12-03T10:15:30+08:00[Asia/Shanghai] ZoneId id = ZoneId.of("Europe/Paris"); System.out.println("ZoneId: " + id); //ZoneId: Europe/Paris ZoneId currentZone = ZoneId.systemDefault(); System.out.println("当期时区: " + currentZone); //当期时区: Asia/Shanghai &#125;&#125;]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>JDK8</tag>
        <tag>lambda表达式</tag>
        <tag>stream</tag>
        <tag>函数式编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM类加载机制]]></title>
    <url>%2F2018%2F07%2F24%2FJVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[虚拟机把描述类的数据从Class文件加载到内存,并对数据进行校验,转换解析和初始化,最终形成可以被虚拟机直接使用的Java类型,这就是虚拟机的类加载机制. 类加载时机]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>类加载机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用集合类]]></title>
    <url>%2F2018%2F07%2F23%2F%E5%B8%B8%E7%94%A8%E9%9B%86%E5%90%88%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[这是摘要 线程安全的定义当多个线程访问某个类时，不管运行时环境采用何种调度方式或者这些线程将如何交替进行，并且在主调代码中不需要任何额外的同步或协同，这个类都能表现出正确的行为，那么称这个类是线程安全的 线程安全的集合Vector(+) 基于数组实现,默认初始化10个 同步方法,Vector的方法和ArrayList基本一样,就是一些方法都加上了synchronize关键字 123456789/** * Appends the specified element to the end of this Vector. */public synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125; 扩容原理 1234567891011private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity);&#125; HashTable就是线程安全的HashMap 线程不安全的集合HashSetset中元素唯一,其实底层就是用HashMap实现的 LinkedList(+) 基于双向链表实现 1234567891011private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; Node&lt;E&gt; prev; Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125;&#125; 增加一个元素 1234567891011void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125; ArrayList(+) 基于数组实现,默认初始容量1012345678/** * Appends the specified element to the end of this list. */public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125; 从上面的源码中可以看出,ArrayList中增加一个元素的时候会分层两步进行: 先在elementData[size++]的地方存放元素, 然后再增大size. 因此如果是多线程进行访问的话,有可能两个线程在同一个位置存放了元素. 扩容原理1234567891011private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; HashMap(+)参考地址:http://www.importnew.com/22011.html参考地址:http://www.hollischuang.com/archives/2091HashMap的底层实现原理 HashMap的结构:是一个Node&lt;K,V&gt;[]的数组,Node实现了Map.Entry,初始容量16(代码中是1&lt;&lt;4),最大是2^30,当检查到插入数据时容量超出capacity*loadFactor,就要增大Hash表的尺寸, 放入规则:计算A的键的hash值,然后%数组长度就是index,放在数组的index位置,如果后面put进来B的键的hash值和A相等的,那就是B.next=A,Node[index]=B 为什么capacity的值是2的次幂:因为index = hash &amp; (length - 1),相当于取余,这样length-1的二进制就是1,和hash进行&amp;操作之后不容易产生相同的结果,也就是尽量减少在同一个index放置多个元素,这样不但空间利用率最高,同时为了利用数组查找元素速度,我们当然是希望每一个位置只有一个元素,这样就不用遍历链表去比对key了. resize()死循环的问题:当HashMap进行扩容的时候,原来的链表就要进行重排,这个时候如果并发操作,由于transfer()函数,就可能形成环,然后下次get元素的时候就可能进入死循环,jdk1.8好像解决了这个问题,进行优化之后省去了重新计算hash值的时间,而是通过if ((e.hash &amp; oldCap) == 0)判断元素hash的高位(即oldCap的最高位对应的)是1还是0(0则是原位置,1则是原位置+oldCap),均匀地将之前冲突的节点分散到新的桶上, 支持NULL键和NULL值,而且null值的key总是放在数组第一个位置 遍历的方法,取出keySet().iterator(),遍历iterator,即Map.entry,从entry中取key和value比较高效. TreeMap参考地址:https://blog.csdn.net/myblog_dhy/article/details/44618775 常用方法: Map.Entry firstEntry();返回该map中最小的key对应的key-value,若map为空，则返回为null； Object firstKey();返回该Map中最小的key值，若map为空，则返回null； Map.Entry lastEntry();返回map中最大的key对应的key-value,若map为空，则返回为null； Object lastKey();返回该Map中最大的key值，若map为空，则返回null； Map.Entry higerEntry(Object key);返回该map中大于指定key的最小的key-value键值对，若map为空，则返回为null； Object higherKey(Object key);返回该map中大于指定key的最小的key，若map为空，则返回为null； Map.Entry lowerEntry(Object key);返回该map中小于指定key的最大的key-value键值对，若map为空，则返回为null； Object lowerKey(Object key);返回该map中小于指定key的最大的key，若map为空，则返回为null； NavigableMap subMap(Object fromKey,boolean fromInclusive,object toKey,boolean toInclusive);返回该Map的子Map,他的key的范围是从fomKey（是否包含取决于第二个参数）到toKey(是否包含取决于第四个参数） SortMap subMap(Object fromKey,Object toKey);返回Map 的子Map，其Key的范围是从fromKey(包括)到toKey(不包括). SortedMap tailMap(Object fromKey):返回该Map的子Map.其key的范围是大于fromKey(包括)的所有的key NavigableMap tailMap(Object fromKey,boolean inclusive);返回该Map的子Map.其key的范围是大于fromKey(是否包括取决于第二个参数)的所有的key SortedMap tailMap(Object toKey):返回该Map的子Map.其key的范围是小于toKey(包括)的所有的key NavigableMap tailMap(Object toKey,boolean inclusive);返回该Map的子Map.其key的范围是小于toKey(是否包括取决于第二个参数)的所有的key ConcurrentHashMap面试者可以先说历史，1.8之前采用分段锁，核心就是一句话：尽量降低同步锁的粒度。1.8之后使用CAS思想代替冗杂的分段锁实现。不出意料，面试者答出CAS之后必定会被追问其思想以及应用，换做我自己的话会有如下思路作答：CAS采用乐观锁思想达到lock free，提一下sun.misc.Unsafe中的native方法，至于CAS的其他应用可以聊一聊Atomic原子类和一些无锁并发框架（如Amino），提到ABA问题加分。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>集合</tag>
        <tag>Vector</tag>
        <tag>HashTable</tag>
        <tag>HashSet</tag>
        <tag>LinkedList</tag>
        <tag>ArrayList</tag>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程池的使用]]></title>
    <url>%2F2018%2F07%2F23%2F%E7%BA%BF%E7%A8%8B%E6%B1%A0%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[参考地址:https://blog.csdn.net/a837199685/article/details/50619311 jdk原生线程池参考地址:http://www.importnew.com/19011.html 目的 为了线程的复用,提高效率 方便对并发多线程资源的限制和管理 方便并发多线程执行中的一些数据维护 处理新任务的大致流程 如果当前线程池中线程的数目低于corePoolSize，则创建新线程执行提交的任务，而无需检查当前是否有空闲线程 如果提交任务时线程池中线程数已达到corePoolSize，则将提交的任务加入等待执行队列 如果提交任务时等待执行的任务队列是有限队列，而且已满，则在线程池中开辟新线程执行此任务 如果线程池中线程数目已达到maximumPoolSize，则提交的任务交由RejectedExecutionHandler处理 ##构造函数中各个参数的作用(四个构造函数中前三个构造函数都调用了最后一个构造器进行初始化) corePoolSize:核心池的大小,创建线程池之后默认情况下,线程池中是没有线程的,除非调用了prestartCoreThread()方法预创建线程,然后当有任务进来的时候,就会开始创建线程去执行任务,当线程池中的线程数目达到corePoolSize之后,就会把到达的任务放到缓存队列中 maximumPoolSize:线程池的最大线程数, keepAliveTime:表示空闲线程能够存活的时间,默认情况下,只有当线程数大于corePoolSize的时候此参数才会生效,即当一个线程空闲的时间超过此参数时,线程就会终止.如果调用allowCoreThreadTimeOut(true)方法,那么线程数只要大于0,次参数就会生效.次参数的单位可以从天到纳秒 workQueue:一个阻塞队列,用来存储等待执行的任务,重要参数,对线程池的运行过程产生重大影响,这里的阻塞队列一般有以下几种选择: ArrayBlockingQueue;ArrayBlockingQueue有界队列 LinkedBlockingQueue;FixedThreadPool和SingleThreadExecutor使用的是这个BlockingQueue，队列长度是无界的，适合用于提交任务相互独立无依赖的场景。 SynchronousQueue;CachedThreadPool使用的是这个BlockingQueue，通常要求线程池不设定最大的线程数，以保证提交的任务有机会执行而不被丢掉。通常这个适合任务间有依赖的场景。线程池的排队策略和阻塞队列的选择有关 threadFactory:线程工厂,用来创建线程 handler:当线程池满了的时候选取的拒绝处理任务的策略: ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。 ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。 ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程） ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务 ##ThreadPoolExecutor类中几个重要的方法: 1. execute():实现了Executor接口中的方法,通过此方法想线程池提交任务,交由线程池去执行 2. submit():继承而来的方法,也是向线程池提交任务,不过可以返回任务执行的结果,其实内部实现还是调用了execute()方法,只是利用Future来获取任务执行结果 3. shutdown():用来关闭线程池 4. shutdownNow():同上 ##深入理解线程池的实现 线程池的状态: 12345volatile int runState; //用来保证线程之间可见static final int RUNNING = 0; //创建线程池后的初始状态static final int SHUTDOWN = 1; //调用shutdown()方法,此时线程池不接受新的任务,会等待现有的任务执行完毕static final int STOP = 2; //调用shutdownNow()方法,不接受新任务并且尝试终止正在执行的任务static final int TERMINATED = 3; //当线程池处于STOP或者SHUTDOWN状态,然后所有的工作线程都已经销毁,任务缓存队列中被清空或者执行结束,线程池被置为TERMINATED 任务的执行ThreadPoolExecutor类中比较重要的几个成员变量 123456789101112private final BlockingQueue&lt;Runnable&gt; workQueue; //任务缓存队列，用来存放等待执行的任务private final ReentrantLock mainLock = new ReentrantLock(); //线程池的主要状态锁，对线程池状态（比如线程池大小、runState等）的改变都要使用这个锁private final HashSet&lt;Worker&gt; workers = new HashSet&lt;Worker&gt;(); //用来存放工作集private volatile long keepAliveTime; //线程存货时间 private volatile boolean allowCoreThreadTimeOut; //是否允许为核心线程设置存活时间private volatile int corePoolSize; //核心池的大小（即线程池中的线程数目大于这个参数时，提交的任务会被放进任务缓存队列）private volatile int maximumPoolSize; //线程池最大能容忍的线程数private volatile int poolSize; //线程池中当前的线程数private volatile RejectedExecutionHandler handler; //任务拒绝策略private volatile ThreadFactory threadFactory; //线程工厂，用来创建线程private int largestPoolSize; //用来记录线程池中曾经出现过的最大线程数private long completedTaskCount; //用来记录已经执行完毕的任务个数 ThreadPoolExecutor类中的execute()的实现原理:12345678910111213141516171819202122232425262728public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); /* * Proceed in 3 steps: */ int c = ctl.get(); //如果当前线程数小于核心线程数,则尝试启动新线程,调用的addWorker()方法会原子性检查runState和workCount,防止错误添加线程 //如果无法添加则重新获取当前线程数 if (workerCountOf(c) &lt; corePoolSize) &#123; //addWorker()方法中使用了ReentrantLock锁来保证线程池的状态 if (addWorker(command, true)) return; c = ctl.get(); &#125; //如果当前线程处于RUNNING状态,则将新任务加入缓存队列 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); //如果线程池变成非RUNNING状态,移除新任务,并reject() if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; //如果当前线程不处于RUNNING状态,尝试新建线程,创建线程失败则reject() else if (!addWorker(command, false)) reject(command);&#125; 总结一下任务提交给线程池之后到执行的过程: 1. 弄清楚corePoolSize和maximumPoolSize的含义 2. Worker的作用 3. 任务交给线程池的处理策略 * 当前线程数小于核心线程数,创建新的线程处理任务 * 如果当前线程数大于核心线程数,那么会尝试将新任务添加到缓存队列,如果添加成功,那么新任务等待空闲线程来处理;如果添加失败,一般是队列已经满了,那么会尝试创建新的线程去处理 * 当前线程数达到maximumPoolSize时,会采取任务拒绝策略处理 * 如果当前线程数量大于corePoolSize,那么当有线程的空闲时间超过keepAliveTime,线程将被终止,知道当前线程数不大于corePoolSize;如果允许核心线程设置空闲存活时间的话,那核心线程的空闲时间超出也会被终止. 线程池的初始化 prestartCoreThread():初始化一个核心线程, prestartAllCoreThread():初始化所有核心线程.如何合理配置线程池的大小(N为cpu个数) 121. 如果是CPU密集型应用，则线程池大小设置为N+12. 如果是IO密集型应用，则线程池大小设置为2N+1 以上公式只是最简单的情况(假设一台服务器上),一般线程池大小还要结合部署的系统中的CPU个数,内存大小,主要的任务执行的是计算还是I/O,还是混合操作比如I/O优化中,如下公式更合适: 1最佳线程数目 = （（线程等待时间+线程CPU时间）/线程CPU时间 ）* CPU数目 因为很显然，线程等待时间所占比例越高，需要越多线程。线程CPU运行时间所占比例越高，需要越少线程。 任务缓存队列及排队策略在前面我们多次提到了任务缓存队列，即workQueue，它用来存放等待执行的任务。workQueue的类型为BlockingQueue，通常可以取下面三种类型： ArrayBlockingQueue：基于数组的先进先出队列，此队列创建时必须指定大小； LinkedBlockingQueue：基于链表的先进先出队列，如果创建时没有指定此队列大小，则默认为Integer.MAX_VALUE； synchronousQueue：这个队列比较特殊，它不会保存提交的任务，而是将直接新建一个线程来执行新来的任务。 任务拒绝策略:上面提到的4中 线程池的关闭:上面提到的2种方法 线程池容量的动态调整ThreadPoolExecutor提供了动态调整线程池容量大小的方法：setCorePoolSize()和setMaximumPoolSize()，当上述参数从小变大时，ThreadPoolExecutor进行线程赋值，还可能立即创建新的线程来执行任务。 Executor接口类ThreadPoolExecutor,继承自,抽象类AbstractExecutorService,实现了,接口ExecutorService,实现了,接口Executor ThreadFactory接口根据需要创建新线程的对象。使用线程工厂就无需再手工编写对 new Thread 的调用了，从而允许应用程序使用特殊的线程子类、属性等等。简单实现:12345public class SimpleThreadFactory implements ThreadFactory &#123; public Thread newThread(Runnable r) &#123; return new Thread(r); &#125;&#125; 多线程同步synchronizedLock ReentrantLock ReadWriteLock Semaphore(信号量)锁是信号量的一种特殊情况,即只有一个线程独占这时的解决方法，就是在门口挂n把钥匙。进去的人就取一把钥匙，出来时再把钥匙挂回原处。后到的人发现钥匙架空了，就知道必须在门口排队等着了。这种做法叫做”信号量”（Semaphore），用来保证多个线程不会互相冲突。 CountDownLatchconcurrent包常用类 原理CountDownLatch内部维护一个计数器，计数器的值为待完成的任务数Ｎ，需要等待这Ｎ个任务完成的线程调用CountDownLatch的await()方法使自己进入休眠等待状态。当某一个任务线程完成某一个任务后调用CountDownLatch的countDown()方法来表示自己的任务已完成，此时CountDownLatch的计数器值减１，当所有的任务完成式，计数器的值为０。当计数器值为0时，CountDownLatch将唤醒所有因await()方法进入休眠的线程。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>线程池</tag>
        <tag>jdk原生线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java线程的使用]]></title>
    <url>%2F2018%2F07%2F23%2FJava%E7%BA%BF%E7%A8%8B%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[这是摘要 线程 什么是线程前面介绍CPU时间片的时候提到了CPU会根据不同的调度算法把时间片分配给“用户”，这里的“用户”在以前指的是进程，随着操作系统的不断发展，现在一般指线程。在过去没有线程的操作系统中，资源的分配和执行都是由进程完成的。随着技术的发展，为了减少由于进程切换带来的开销，提升并发能力，操作系统中引入线程。把原本属于进程的工作一分为二，进程还是负责资源的分配，而线程负责执行。也就是说，进程是资源分配的基本单位，而线程是调度的基本单位。 线程和进程的区别参考地址:http://www.ruanyifeng.com/blog/2013/04/processes_and_threads.html 单个CPU同一时刻只能运行一个进程,其他进程处于非运行状态 一个进程可以包括多个线程,共享进程的内存,还有自己的内存空间(线程栈),JVM以及程序和可执行文件,class文件等是一个静态的概念,JVM实例以及进程和线程是一个动态的概念.比如说,JVM运行起来的java程序实例其实就是一个进程,会分配到系统的一部分资源,所谓的进程是资源分配的基本单位;然后这个进程下面有一个主线程,然后可以有很多子线程,这些线程是进程里的多个执行路径,所谓的线程是基本的调度单位.操作系统的设计总结: 单个CPU可以,以多进程的形式运行多个任务 单个任务可以,以多线程的形式分成多个部分执行 提供协调机制，一方面防止进程之间和线程之间产生冲突，另一方面允许进程之间或线程之间共享资源。 java中的多线程是一种抢占机制而不是分时机制,抢占机制指的是多个线程处于可运行状态,但是只允许一个线程处于运行状态. 通信方式 进程之间:socket,信号,管道,消息,共享内存 线程之间:共享内存,管道 系统可以启动的线程数量限制不考虑系统本身的限制,主要和以下JVM启动参数有关 -Xms()初始堆栈大小, -Xmx()最大堆栈大小, -Xss()单个线程堆栈 线程的状态 新生状态（New）：当一个线程的实例被创建即使用new关键字和Thread类或其子类创建一个线程对象后，此时该线程处于新生(new)状态，处于新生状态的线程有自己的内存空间，但该线程并没有运行，此时线程还不是活着的（not alive）； 就绪状态（Runnable）：通过调用线程实例的start()方法来启动线程使线程进入就绪状态(runnable)；处于就绪状态的线程已经具备了运行条件，但还没有被分配到CPU即不一定会被立即执行，此时处于线程就绪队列，等待系统为其分配CPU，等待状态并不是执行状态； 此时线程是活着的（alive）； 运行状态（Running）：一旦获取CPU(被JVM选中)，线程就进入运行(running)状态，线程的run()方法才开始被执行；在运行状态的线程执行自己的run()方法中的操作，直到 调用其他的方法而终止 等待某种资源而阻塞 完成任务而死亡 如果在给定的时间片内没有执行结束，就会被系统给换下来回到线程的等待状态；此时线程是活着的（alive）； 阻塞状态（Blocked）：通过调用join()、sleep()、wait()或者资源被暂用使线程处于阻塞(blocked)状态；处于Blocking状态的线程仍然是活着的（alive） 死亡状态（Dead）：当一个线程的run()方法运行完毕或被中断(Thread.interrupt())或被异常退出，该线程到达死亡(dead)状态。此时可能仍然存在一个该Thread的实例对象，当该Thread已经不可能在被作为一个可被独立执行的线程对待了，线程的独立的call stack已经被dissolved。一旦某一线程进入Dead状态，他就再也不能进入一个独立线程的生命周期了。对于一个处于Dead状态的线程调用start()方法，会出现一个运行期(runtime exception)的异常；处于Dead状态的线程不是活着的（not alive）。 定义线程的方法 继承Thread类,重写run()方法,但是这种方法扩张性不高,因为不能继承其他类 实现Runnable接口,实现接口中的run()方法,然后创建Thread实例时作为参数传入 线程的方法和属性 优先级（priority）每个类都有自己的优先级，一般property用1-10的整数表示，默认优先级是5，优先级最高是10；优先级高的线程并不一定比优先级低的线程执行的机会高，只是执行的机率高；默认一个线程的优先级和创建他的线程优先级相同； Thread.yield() 让出CPU的使用权，给其他线程执行机会、让同等优先权的线程运行（但并不保证当前线程会被JVM再次调度、使该线程重新进入Running状态) 如果没有同等优先权的线程，那么yield()方法将不会起作用。 Thread.sleep(long millis) throws InterruptedException 当前线程睡眠millis的时间（millis指定睡眠时间是其最小的不执行时间，因为sleep(millis)休眠到达后，无法保证会被JVM立即调度）； sleep()是一个静态方法(static method) ，所以他不会停止其他的线程； 线程sleep()时不会失去拥有的对象锁。 作用：保持对象锁，让出CPU，调用目的是不让当前线程独自霸占该进程所获取的CPU资源，以留一定的时间给其他线程执行的机会； sleep(0)的作用是立刻进行一次CPU竞争,让其他线程有机会执行 thread.join() throws InterruptedException,可以有时间参数 1234567891011121314151617public static void main(String[] args) &#123; Thread thread0 = new Thread(()-&gt;&#123; for (int i = 0; i &lt; 3; i++) &#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); thread0.start(); try &#123; thread0.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125; 123456789101112131415161718192021222324public final synchronized void join(long millis) throws InterruptedException &#123; long base = System.currentTimeMillis(); long now = 0; if (millis &lt; 0) &#123; throw new IllegalArgumentException("timeout value is negative"); &#125; if (millis == 0) &#123; while (isAlive()) &#123; wait(0); //wait()方法其实调用的就是wait(0) &#125; &#125; else &#123; while (isAlive()) &#123; long delay = millis - now; if (delay &lt;= 0) &#123; break; &#125; wait(delay); now = System.currentTimeMillis() - base; &#125; &#125; &#125; 比如在main()方法中调用了thread0.join()方法,那么main线程会阻塞,等到thread0执行完之后再执行后面的代码 join()方法实际上基于wait()方法实现,join()方法中会调用join(0)方法,join(0)方法(是一个同步方法)中调用了wait(0)方法,main线程先获得thread0的对象锁,然后发现thread0还活着,于是释放锁,并进入等待直到thread0结束(至于结束后怎么通知?) 可以设置等待时间 object.wait() throws InterruptedException,可以有时间参数 当一个线程执行到obj.wait()方法时，他就进入到一个和该obj对象相关的等待池(Waiting Pool)中，同时失去了obj的锁—暂时的，wait后还要返还对象锁。 当前线程必须拥有obj的锁才能调用obj.wait()方法，如果当前线程不是此锁的拥有者，会抛出IllegalMonitorStateException异常,所以wait()必须在synchronized block中调用。 比如:线程A 1234567synchronized(obj) &#123; while(!condition) &#123; obj.wait(); &#125; obj.doSomething();...&#125; 当线程A获得obj的锁,但是发现条件不满足的时候,就会调用obj.wait()方法进入等待线程B 12345synchronized(obj) &#123; condition = true; obj.notify(); ...&#125; 然后当线程B获得obj的锁,将条件满足,然后调用obj.notify()唤醒等待obj锁的其他线程要注意的是: 当obj.wait()方法返回后，线程A需要再次获得obj锁，才能继续执行。 如果A1，A2，A3都obj.wait()，则B调用obj.notify()只能唤醒A1，A2，A3中的一个（具体哪一个由JVM决定）。 obj.notifyAll()则能全部唤醒A1，A2，A3，但是要继续执行obj.wait()的下一条语句，必须获得obj锁，因此，A1，A2，A3只有一个有机会获得锁继续执行，例如A1，其余的需要等待A1释放obj锁之后才能继续执行。 当线程B调用obj.notify/notifyAll的时候，B正持有obj锁，因此，A1，A2，A3虽被唤醒，但是仍无法获得obj锁。直到B退出synchronized块，释放obj锁后，A1，A2，A3中的一个才有机会获得锁继续执行。 object.notify()/notifyAll() 唤醒在object对象等待池中等待的一个线程/所有线程。 notify()/notifyAll()也必须拥有相同对象锁，否则也会抛出IllegalMonitorStateException异常。 Synchronizing BlockSynchronized Block/方法控制对类成员变量的访问； Java中的每一个对象都有唯一的一个内置的锁，每个Synchronized Block/方法只有持有 对应的对象锁才可以执行同步方法，否则所属线程阻塞； 机锁具有独占性、一旦被一个Thread持有，其他的Thread就不能再拥有（不能访问其他同步方法,普通方法还是可以的），方法一旦执行，就独占该锁，直到从该方法返回时才将锁释放，此后被阻塞的线程方能获得该锁，重新进入可执行状态。 中断参考地址:https://www.cnblogs.com/onlywujun/p/3565082.html Thread.interrupted()方法会返回中断标志位,并将中断标志位清除 Thread.isInterrupted()方法只是返回中断标志位 thread0.interrupt()方法设置中断标志 线程被synchronized阻塞,即在获取锁的阻塞过程中是不会被中断的,所以中断不能处理死锁 调用wait(),sleep(),join()方法的等待过程中可以被中断,因此他们都会抛InterruptedException异常 使用中断来结束阻塞状态:while在try里面的时候 1234567891011121314151617public void run() &#123; try &#123; ... /* * 不管循环里是否调用过线程阻塞的方法如sleep、join、wait，这里还是需要加上 * !Thread.currentThread().isInterrupted()条件，虽然抛出异常后退出了循环，显 * 得用阻塞的情况下是多余的，但如果调用了阻塞方法但没有阻塞时，这样会更安全、更及时。 */ while (!Thread.currentThread().isInterrupted()&amp;&amp; more work to do) &#123; do more work &#125; &#125; catch (InterruptedException e) &#123; //线程在wait或sleep期间被中断了 &#125; finally &#123; //线程结束前做一些清理工作 &#125;&#125; try在while里面的时候,应该在catch块里重新设置一下中断标示，因为抛出InterruptedException异常后，中断标示位会自动清除，此时应该这样： 12345678910public void run() &#123; while (!Thread.currentThread().isInterrupted()&amp;&amp; more work to do) &#123; try &#123; ... sleep(delay); &#125; catch (InterruptedException e) &#123; Thread.currentThread().interrupt();//是否重新设置中断标示,来决定是否还要循环 &#125; &#125;&#125; setDaemon()设置成守护线程或者用户线程,守护线程和用户线程的区别在于：守护线程依赖于创建它的线程，而用户线程则不依赖。 线程安全volatile的使用(有两种语义) 能够保证修饰的变量对所有线程的可见性,当一条线程修改了了这个变量,其他线程可以马上得知.线程A修改了普通变量的值之后需要先回写到主内存之后,新的变量值才能被线程B读取到.但是并不能保证基于volatile变量的运算在并发下是安全的. 使用volatile进行安全的并发运算的规则 运算结果不依赖变量的当前值,或者确保只有单一线程修改变量的值 变量不需要与其他状态变量共同参与不变约束比如多线程的累加是不安全的,因为累加不是原子操作;比如布尔值的判断可以是安全的,多个线程判断一个flag是true还是false是可以线程安全的. 禁止指令重排序优化,普通变量不能保证普通的赋值操作和代码中的顺序是一致的. 比如说线程a进行初始化工作,初始化完之后将初始化完成的标志置为true,线程b中根据初始化完成标志开始其他工作, 如果线程a,b并发,这个时候就有可能出错,因为指令重排序优化的关系,线程a中的初始化标志的指令可能会提前执行, 线程安全的实现方法 互斥同步(阻塞同步)互斥同步对性能最大的影响就是对阻塞的实现,挂起和恢复线程的操作需要切换到内核态中完成 synchronized关键是要分清楚是给类加锁还是给对象实例加锁 对象锁 同步方法当多个线程访问同一个对象的同步方法时,需要同步执行,但是可以访问该对象的其他非同步方法 123synchronized T methodName()&#123;//do smoething&#125; 锁定临界对象锁定的是object对象 12345678public class SynchronizedDemo&lt;T&gt; &#123; Object object = new Object(); T methodName()&#123; synchronized(object)&#123; //do something &#125; &#125;&#125; 锁定当前对象相当于同步方法 12345T methodName()&#123; synchronized(this)&#123; //do something &#125;&#125; 同步代码块只是锁住方法中的部分代码 类锁(同步静态方法)也就是锁住Class对象 java.util.concurrent.ReentrantLock(注意锁的声明要放在需要同步的方法的外面)和synchronized基本用法类似,使用lock()和unlock()方法配合try/finally语句块来完成,但是有以下三个特色: 等待可中断 可实现公平锁 锁可以绑定多个条件 非阻塞同步不断重试 无同步方案 可重入代码可以在代码执行的任何时刻中断去执行其他代码(包括递归调用自己),并且在回来的时候原来的程序不会出现任何错误 线程本地存储将共享数据的代码保证在同一个线程中完成 线程安全是相对的(深入理解Java虚拟机P390)比如说Vector是一个线程安全的集合容器,因为它的add(),get(),size(),remove()方法都是被synchronized修饰过的,但是并不意味着调用这些方法就永远不需要额外的同步手段了,一下代码就有可能会出现ArrayIndexOutOfBoundsException.12345678910111213141516171819202122232425262728public class VectorDemo &#123; private static Vector&lt;Integer&gt; vector = new Vector(); public static void main(String[] args) &#123; while (true) &#123; for (int i = 0; i&lt;10;i++) &#123; vector.add(i); &#125; Thread removeTask = new Thread(new Runnable() &#123; public void run() &#123; for(int i = 0;i &lt; 10; i ++) &#123; vector.remove(i); &#125; &#125; &#125;); Thread printTask = new Thread(new Runnable() &#123; public void run() &#123; for(int i = 0;i &lt; 10; i ++) &#123; System.out.println(vector.get(i)); &#125; &#125; &#125;); removeTask.start(); printTask.start(); //防止线程过多 while (Thread.activeCount() &gt; 20) ; &#125; &#125;&#125; 线程进程之间的访问 临界区(Critical Section)适合一个进程内的多线程访问公共区域或代码段时使用。 互斥量(Mutex)适合不同进程内多线程访问公共区域或代码段时使用，与临界区相似。 事件(Event)通过线程间触发事件实现同步互斥。 信号量(Semaphore)与临界区和互斥量不同，可以实现多个线程同时访问公共区域数据，原理与操作系统中PV操作类似，先设置一个访问公共区域的线程最大连接数，每有一个线程访问共享区资源数就减一，直到资源数小于等于零]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>线程</tag>
        <tag>进程</tag>
        <tag>线程安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库索引原理]]></title>
    <url>%2F2018%2F07%2F23%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[这是摘要 索引优缺点 优点 大大加快数据的检索速度，这也是创建索引的最主要的原因； 可以创建唯一性索引，保证数据库表中每一行数据的唯一性； 可以将表的外键制作为索引，加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义； 将随机I/O变为顺序I/O，帮助服务器避免排序和临时表，在使用分组和排序子句进行数据检索时，可以显著减少查询中分组和排序的时间； 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。 缺点 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加； 索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间； 当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。 关于mysql索引: BTree 索引是大多数 MySQL 存储引擎的默认索引类型 在 MySQL 中只有 Memory 引擎显式支持哈希索引 哈希索引只包含 哈希值和行指针，而不存储字段值 索引将随机 I/O变为顺序 I/O innodb行锁原理与myisam的区别数据库事务 innodb中的索引知识小结参考地址:http://www.hollischuang.com/archives/1712 不同查询条件下的命中情况聚集索引加速原理磁盘读写原理和效率参考地址:https://blog.csdn.net/v_july_v/article/details/6530142 读写数据时间的组成 查找时间:就是磁头移动到需要的柱面上(一个圈就叫柱面),定位到指定磁道,时间代价最大,最大可达0.1s 等待时间:盘片开始旋转,将指定的磁道段移动到磁头下,因为硬盘转速有7200转/分,所以转一圈的时间就0.0083s,很快 传输时间:数据通过系统总线到内存的时间,传输一个byte0.02微秒(2*10^-8s) 提速手段 硬盘读取数据的速度是以块(block)为基本单位的,而磁盘IO的时间代价主要花费在查找时间,因此一方面要将相关数据放在相近的区域(同一盘块,同一磁道,或者至少放在同一柱面或者相邻柱面),减少读/写数据时来回移动磁头的次数 B树(B-树)也是一种多叉平衡查找树,一个节点内有x个关键字,那么这个节点就有x+1个子节点如上图树查找字母R的路径根节点中的17代表一个磁盘文件的文件名,红色方块代表17文件在硬盘中的存储位置(这就是和B+树的主要区别),P1指向17的左子树. B+树的介绍参考地址:http://blog.csdn.net/guoziqing506/article/details/64122287与b树相比较 b+树的磁盘读写代价更低,因为b+树的内部节点并没有指向关键字的指针的具体信息,所以盘快内可以容纳更多的关键字,盘块读进内存后查找的范围更大,从而降低树的高度 查询效率更加稳定,因为内部节点并不是指向最终文件内容的节点,而是指向关键字的节点,所以每一次查询都必须从根节点到叶子结点,长度相同 b树在解决io效率的时候并没有解决元素遍历效率低下的问题,而b+树只需要遍历叶子节点就能解决对所有关键字的查询,对于数据库中经常使用的范围查询性能更高]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>索引</tag>
        <tag>平衡树</tag>
        <tag>磁盘存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NIO的使用]]></title>
    <url>%2F2018%2F07%2F14%2FNIO%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[这是摘要 同步和异步说的是消息的通知机制,阻塞和非阻塞说的是线程的状态, 同步阻塞io指的就是传统的Java IO,线程发起io后会阻塞,等到io结束之后才会恢复 同步非阻塞io(消息机制会阻塞,io不会阻塞)Java中的NIO(Non-blocking IO)是这种模型,线程发起io后可以去干其他事情,但是另外起一个线程去监听或者轮询io状态(是否有可读数据,或者是否可以写入) 123456789101112131415161718192021222324252627282930313233//创建Selector监听IO事件,Selector selector = Selector.open(); //for循环中,为每一个监听的port打开一个channel,并设置成非阻塞ServerSocketChannel ssc = ServerSocketChannel.open();ssc.configureBlocking( false ); ServerSocket ss = ssc.socket();InetSocketAddress address = new InetSocketAddress( ports[i] );ss.bind( address );SelectionKey key = ssc.register( selector, SelectionKey.OP_ACCEPT );//将channel注册到selector//while循环while(ture)&#123; //该方法会阻塞,直到至少有一个已经注册的事件发生时,会返回发生事件的数量 int num = selector.select(); //事件发生后就可以获得SelectionKey集合,然后进行迭代处理 Set selectedKeys = selector.selectedKeys(); Iterator it = selectedKeys.iterator(); while (it.hasNext()) &#123; SelectionKey key = (SelectionKey)it.next(); // ... deal with I/O event ... if ((key.readyOps() &amp; SelectionKey.OP_ACCEPT) == SelectionKey.OP_ACCEPT) &#123; // Accept the new connection &#125; else if ((key.readyOps() &amp; SelectionKey.OP_READ) == SelectionKey.OP_READ) &#123; // Read the data &#125; &#125;&#125; 异步非阻塞ioJava中的AIO((Asynchronous IO))是这种模型,线程发起io后可以直接干其他事前,系统会监听io状态,IO完成后会通知线程 使用参考地址:https://www.ibm.com/developerworks/cn/education/java/j-nio/j-nio.html NIO123456789101112131415161718192021222324252627282930313233343536public class FastCopyFile &#123; static public void main(String args[]) throws Exception &#123; if (args.length &lt; 2) &#123; System.err.println("Usage: java FastCopyFile infile outfile"); System.exit(1); &#125; String infile = args[0]; String outfile = args[1]; FileInputStream fin = new FileInputStream(infile); FileOutputStream fout = new FileOutputStream(outfile); FileChannel fcin = fin.getChannel(); FileChannel fcout = fout.getChannel(); ByteBuffer buffer = ByteBuffer.allocateDirect(1024); while (true) &#123; //设置缓存区,使之可以读入新数据 //position = 0;limit = capacity;mark = -1; buffer.clear(); //将输入通道中的数据读到缓存区 int r = fcin.read(buffer); //判断输入通道的数据是不是读完了 if (r == -1) &#123; break; &#125; //设置缓存区,然缓存区中的数据可以写入新的通道 //limit = position;position = 0;mark = -1; buffer.flip(); //将数据从缓存区写入输出通道 fcout.write(buffer); &#125; &#125;&#125;]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>NIO</tag>
        <tag>Java基础</tag>
        <tag>socket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置文件集]]></title>
    <url>%2F2018%2F07%2F08%2F%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E9%9B%86%2F</url>
    <content type="text"><![CDATA[常用配置文件 数据库dbcpconfig.properties123456789101112131415161718192021222324252627282930#连接设置driverClassName=com.mysql.jdbc.Driverurl=jdbc:mysql://localhost:3306/jdbcusername=rootpassword=hn123456#初始化连接initialSize=10#最大连接maxActive=50#最大空闲连接maxIdle=20#最小空闲连接minIdle=5#超时等待时间，毫秒为单位maxWait=60000connectionProperties=useUnicode=true&amp;characterEncoding=UTF8#指定连接池所创建的连接的自动提交（auto-commit）状态defaultAutoCommit=true#指定连接池所创建的连接的只读（read-only）状态defaultReadOnly#指定连接池创建的连接的事务级别 NONE ,READ_UNCOMMITED,READ_COMMMITED,REAPEATABLE_READ,SERIALIIZABLEdefaultTransactionIsolation=READ_UNCOMMITED git.gitignore123456789101112131415161718192021222324target/target/.settings/.classpath/.project/out//classes/.idea//education.iml/src/main/resources/db.properties.DS_Store.AppleDouble.LSOverrideIcon# Thumbnails._*# Files that might appear on external disk.Spotlight-V100.Trashes]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Javaweb25天笔记]]></title>
    <url>%2F2018%2F07%2F08%2FJavaweb25%E5%A4%A9%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[传智播客视频javaweb25天笔记 总结：tomcat服务器的安装和配置，http协议 虚拟目录的映射， 虚拟目 录的配置（serve.xml,catalina\localhost\） http协议的请求和响应的各个组成部分 各个常见请求头的含义 各个响应头的含义servlet开发 servlet的开发7个细节性的问题 ServletConfig ServletContext对象 域对象 读取资源文件（三种方式） ServletContext（资源不在类目录下的时候 ClassLoader（文件比较小的时候，因为资源h会被读到内存中 FileInputStream（ 转发 this.getServletContext.getRequestDispatcher() request和response的使用一个http响应应该包含响应状态行 响应头 响应数据 要掌握通过response发送者三部分内容2 response发送中文3 response发送动态图片4 response实现下载5 response实现请求重定向6 meta标签模拟http响应头 1 http请求包含请求头 请求头 请求数据 要掌握通过request获取者三部分内容2 使用request获取中文数据的问题（get/post3 使用request获取各种不同类型的表单数据（text\checkbox\radio4 域对象5 实现请求转发 session和Cookie（回话过程中多个web组件之间数据共享问题cookie：吧数据打给客户机 1 显示用户上次登录的时间 2 用户的历史浏览商品记录 session：把数据存在服务器的session对象中 1 购物案例 1 实现关掉浏览器后，session还能为用户服务，一个session为多个窗口服务 2 实现用户禁用Cookie后，还能实现购物 2 用户登录 3 防止表单重复提交 1 md5/base64 jsp1 9大内置对象2 jsp常见的标签3 el表达式和jstl标签库 javabean开发1 jsp+javabean（简单计算器2 jsp+servlet+javabean（mvc3 用户登录和注册案例4 购物车案例 自定义标签1 传统标签（了解 tag：是否执行标签体，以及标签余下的jsp IterationTag：迭代标签体 bodyTag：修改标签体2 简单标签（掌握3 SimpleTag（可以替换上面几个传统标签4 标签属性5 打包标签库6 .EXE文件 el + jstl web国际化1 固定文本的国际化 properties文件，一组资源文件就是一个资源包，ResourceBundle.getBundle（）2 动态数据国际化 DateFormat：日期国际化 Format（Date date） 符合国家地区习惯的字符串 parse（String date） 把字符串格式化成日期 3 MessageFormat 批量国际化 jdbc开发1 入门 2 jdbc中涉及到的每一个对象的方法解析 DriverManager Connection Statement（PreparedStatement） ResultSet 3 用jdbc改造注册登录案例 DaoFactory DaoException sqlz注入问题 4 PreparedStatement（PreparedStatement和Statement对象的区别 5 jdbc案例（客户关系 jdbc开发21 分页（Page） 2 批处理 3 大数据 4 存储过程的调用 5 获取自动增长的主键 jdbc开发31 事务 1 什么是事务 start transactioncommit rollback； 2 事务的四大特性：ACID 3 四大特性的隔离级别， 脏读 一个事务读取到另一个事务已经提交的数据 不可重复读 虚读 4 子安jdbc程序中怎么控制多条sql作为整体执行，在jdbc中如何设置事务隔离级别 5 连接池的实现原理 6 常用的连接池产品 1 tomcat内置（DBCP，JNDI 2 DBCP C3P0 7 元数据 8 自己设计类似于DBUtils的框架 jdbc开发41 DbUtils的使用（常见的集中处理器 2 实际开发中如何控制事务（ThreadKLocal*** 3 jdbc多表操作（hibernate基础 一对多（多对一） 一般建议在多的一方维护关系 多对多 一对一 Oracle的大数据 过滤器和监听器 day07 会话管理 cookie技术（客户端的技术） session技术（服务器端的技术） 用cookie写一个历史浏览记录 session技术 用session实现购物记录 session的原理 设置session的有效期 浏览器禁用Cookie后servlet共享数据导致的问题 url重写 session的常见应用 1 登录 2 防止表单重复提交 前台javaScript 后台表单带随机数 3 实现一次性校验码 实现带验证码的登录 day17 dbutils 01 dbutils文档 大数据不建议使用这个处理，自己写传统的方法处理 02 dbutils的已经定义的处理器，如何进行实物处理 03 ThreadLocal：看成一个容器，想这个容器中存放的对象，在当前线程范围内都可以取得出来 day08 jsp JSP技术介绍原理 1 web服务器是如何调用并执行一个jsp页面的 JSP实际上就是servlet 2 jsp页面中的html排版标签是如何被发送到客户端的 out.write() 3 jsp页面中的java代码服务器是如何执行的 原封不动 4 web服务器在调用JSP时，会给JSP提供一些什么java对象 jsp语法 自定义标签 el表达式 error-pagejsp乱码问题 jsp以什么码表保存在硬盘上，就通知浏览器以什么码表打开 include指令（jsp指令不能冲突）JSP九大隐式对象 1 Request 2 Response 3 Session 4 Application servletContext 5 Config servletConfig 6 Page this 7 Exception 8 Out JSPWriter 9 pageContext 主要讲Out隐式对象和pageContext隐式对象（最重要）可以获得其他八大隐式对象 jsp标签重点 web开发涉及到的4个域对象1 pageContext 页面范围内都可取 2 Request 如果客户端向服务器发请求，产生数据，客户看完就没用了（新闻） 3 Session 如果客户端向服务器发请求，产生数据，客户看完还有用（购物，购物信息，结账，浏览记录） 4 servletContext 如果客户端向服务器发请求，产生数据，客户看完了还要给其他用户用（聊天室） day09 mvc设计模式开发注册登录 javabean 和jsp 什么是javabean（遵循特定写法的java类）1 这个java类必须有一个无参构造函数 2 属性必须私有化 2 有getter和setter函数 JSP提供的三个javabean组件动作元素 1 JSP：useBean 2 jsp：setBean 3 JSP：setProperty mvc设计模式 el表达式( ${customerBean.address} ) jsp自定义标签el表达式如何取数据 jsp标签 day10 mvc购物车 day11 自定义标签库 移除jsp中的java代码1 实现Tag接口的java类 2 编写标签库描述符（tld）文件，在tld文件中岁标签处理器类进行描述 自定义标签如何控制jsp页面1 实现tag接口，控制dostarttag方法的返回值，如果这个方法的返回值为 `EVAL_BODY_INCLUDE`：执行标签体 `SKIP_BODY`：不返回标签体 2 控制doendtag的返回值， `EVAL_PAGE`：执行余下的JSP页面 `SKIP_PAGE`：不执行剩下的 3 实现IterationTag接口，控制doafterbody方法的返回值， `EVAL_BODY_AGAIN`：又执行一次标签体，直到返回SKIP_BODY， 4 实现BodyTag接口，控制doStartTag接口，控制doStartTag方法 `EVAL_BODY_BUFFERED`，web服务器会创建BodyContent对象捕获标签体，然后在doendtag方法中得到代表bodyContent，就行修改 （以上内容听说已经淘汰，但是以前的框架在用，要能看得懂） 简单标签库开发 SimpleTag接口1 `//得到代表标签体的JSPFragment` `JspFragment jf = this.getJspBody();` `//其实默认就是输出给浏览` `jf.invoke(null);` 2 控制余下的jsp不执行， `throw new SkipPageException();` 开发带属性的标签 防盗链1 如果标签接受的是复杂类型，如何给其赋值，用el表达式 开发 foreach标签 if else标签html转义标签 打包标签库 配置文件放在META-INF如何将java程序打包成可执行的jar包 1 先导出jar包，然后修改META-INFO文件夹中的MANIFEST.MF文件 Main-Class: cn.hn.demo.JarDemo day12 el表达式1 支持的运算符 empty运算符` &lt;c:if test=&quot;${!empty(list)}&quot;&gt;` 二元表达式 `user == null ? &quot;对不起，没登录&quot; : &quot;welcome you &quot;` []和. 先用.再用[] 不行再用自定义 2 11个隐式对象 3 调用java的方法 1 静态方法 2 el函数用来移除jsp中与web开发无关的java代码 el自定义标签可以移除一切java代码 el表达式不能相互嵌套 各种常用标签国际化开发1 固定文本国际化 写在properties文件中 2 动态数据国际化 1 日期 DateFormat 2 货币 NumberFormat 3 动态文本 MessageFormat 批量处理 占位符 国际化标签库文件的上传和下载 day13 01 sql 定义外键create table husband{ id int primary key, name varchar(20) }; create table wife{ id int primary key, name varchar(20), id int husband, constraint husband_FK foreign key(husband_id) references husband(id) }; 表与表之间的关系：1 一对多，在多的表加外键约束，如部门和员工，在员工表加外键约束 2 多对多，采用中间表，中间表都是外键，如老师和学生，一个老师有多个学生，一个学生有多个老师 3 一对一，分为主从，一般在从表的主键约束列加外键约束， day14 JDBC JDBC简介Connection对象常用方法1 createStatement（） 2 prepareStatement（sql） 3 prepareCall（sql） 4 setAutoCommit（Boolean autoCommit） 5 commit（） 6 rollback（） Statement对象常用方法1 executeQuery（String sql）：发送查询语句 2 executeUpdate（String sql）：发送insert，delete，update语句 3 execute（String sql）：发送所有，返回boolean 4 addBatch（String sql）：把多条sql语句放在一个批处理中 5 executeBatch（）：向数据库发送一批sql语句执行 用jdbc改造用户登录项目PreparedStatement类1 prepareStatement（）：程序对sql语句进行预编译，减轻数据库的压力 2 防范SQL语句注入的问题 customer案例 day15 分页功能1 改写dao 从数据库获得数据计算分页信息 2 设计page对象 封装分页的页面数据 3 改写service，对web层提供分页数据服务 一个页面里面显示的页码数重构分页 将分页抽取出来作为一个jsp文件，然后要用的地方 &lt;%@include file=”/public/page.jsp” %&gt;用自定义标签显示省略号 对数据修改和删除使用jdbc处理大数据（大文本和二进制）jdbc用批处理 两种处理方式Statement和prepareStatement 用java调用存储过程 day16 事务 事务的概念 将多条sql语句作为一个整体执行Start transaction 开始事务 rollback 回滚事务 commit 提交事务 事务的特性 ACID atomicity 原子性 consistency 一致性 isolation 隔离性 durability 持久性 事务的隔离性1 脏读 一个事务读取到另一个事务未提交的数据 2 不可重复读 第一次和第二次读的数据不一致 3 虚读 一个事务读到另一个事务插入的数据 解决方法 set transaction isolation level Serializable select @@_isolation serializable（级别最高） reapeatable read read committed read uncommitted 连接池如何改写Connection.close（）方法 当发现一个对象的方法不够用的时候 1、写一个子类，覆盖方法 2、写一个包装类，增强方法 1 写一个类实现与被增强对象相同的接口 2 定义一个变量，指向被增强对象 3 定义一个构造方法，接受被增强对象 4 覆盖想增强的方法 5 对于不想增强的方法，直接调用被增强对象的方法 3、用动态代理，返回一个代理对象，拦截原方法的调用(拦截机制) 1 Proxy.newProxyInstance(JdbcPool.class.getClassLoader(), conn.getClass().getInterfaces(), new InvocationHandler() { @override ... }); 使用开源库，dbcp 开源库 c3p0的使用使用tomcat服务器创建的连接池 JNDI容器(java naming and directory interface) 编写自己的jdbc框架1 利用元数据，改写dao的增删改方法，使用ResultSetMetaData对象获取数据信息 2 使用接口以及反射技术改写查方法，即处理结果集的方法做成接口对象参数，让用户传入， day17 dbutil day18 web.xml中的配置不正确可能会导致tomcat服务器启动失败01 servlet过滤器Filter filter可以对request和response进行一些预处理，比如进行字符集设置 可以对filter设置初始化参数 用 ${pageContext.request.contextPath }获得项目路径02 filter的常见应用 1 阻止浏览器缓存jsp页面的filter 2 自动登录功能 完善自动登录功能，因为filter会拦截所有页面的请求，自动登录以后每次访问网页又执行登陆验证的代码 03 filter的部署和注册 可以拦截4种向服务器请求资源的方式 request forward error include 过滤器高级开发，对拦截的request，response增强之后进行传递 使用包装设计模式增强BufferedReader Day19 权限过滤器 搭建框架三个对象1. 用户 2. 角色 3. 权限 数据库五张表User表create table user( id varchar(40) primary key, username varchar(100) not null unique, password varchar(100) not null, description varchar(255) ); role表create table role( id varchar(40) primary key, name varchar(100) not null unique, description varchar(255) ); privilege表create table privilege( id varchar(40) primary key, name varchar(100) not null unique, description varchar(255) ); user_role表create table user_role( user_id varchar(40), role_id varchar(40), primary key(user_id,role_id), constraint user_id_FK foreign key(user_id) references user(id), constraint role_id_FK foreign key(role_id) references role(id) ); role_privilege表create table role_privilege( role_id varchar(40), privilege_id varchar(40), primary key(role_id,privilege_id), constraint role_id_FK1 foreign key(role_id) references role(id), constraint privilege_id_FK foreign key(privilege_id) references privilege(id) ); sp页面分割 head left body权限管理 jsp页面设计 servlet设计 day20 文件的上传和下载 01 表单文件上传 1 解决上传文件名和上传文件数据的乱码问题 2 表单为文件上传，request设置编码无效,只能手工转化 value = new String(value.getBytes(&quot;iso8859-1&quot;),&quot;UTF-8&quot;); 3 为了保证服务器安全，上传文件应该放在外界无法访问的目录 4 为了保证上传文件的唯一性，为用户上传的文件名生成UUID 5 保存上传文件的目录要打散，使用hash算法分配路径makePath（），防止一个目录下存放太多的文件 02 限制文件上传最大值 1 调用item.delete()确保删除临时文件 2 限制上传文件类型 3 设置上传文件进度监听器 要在upload.parseRequest(request);之前 4 在web页面中动态添加上传输入项 03 Servlet监听器 1 frame窗口demo， 2 如何为自己写的对象设计监听器,观察者设计模式（Observer） 3 Servlet监听器 1 ServletContext监听器，实现ServletContextListener接口，然后在web.xml中配置注册，`&lt;listener&gt;`标签 04 各种监听器 1 HttpSessionListener，session的创建和销毁（网站在线人数） 2 HttpRequest监听器（网站请求数量） 05 各种监听器 1 自定义session扫面器 （增删改查用linkedList 2 用监听器定时发送邮件 3 监听三个域对象的属性变化 1 ServletCOntextAttributeListener(据说没什么大用) 2 HttpSessionAttributeListener 3 ServletRequestAttributeListener 4 感知Session绑定的事件监听器 06 显示登录用户列表，并实现踢人功能 day21 书店管理系统 day22 java加强 泛型 泛型方法和泛型类，要先定义后使用,类上声明的泛型只能用于类中的非静态方法，静态方法需要单独声明public class Dema{ public &lt;T&gt; void aa(T t){ //泛型方法 } public &lt;T&gt; void bb(T t){ } } 等同于 public class Demo&lt;T&gt;{ public void aa(T t){ } public void bb(T t){ } public static &lt;T&gt; void cc(T t){ } } 泛型的使用场景，dao层的接口，结合HIbernate写非常简单，反射泛型以下示例代码可以直接继承，继承之后就拥有泛型类型的增删改查方法 import org.hibernate.Session; public abstract class BaseDao&lt;T&gt;{ private Session session; private Class clazz; //哪个子类调用这个方法就得到的class就是子类的处理类型（非常重要） public BaseDao(){ Class clazz = this.getClass(); //得到子类 ParameterizedType pt = (ParameterizedType)clazz.getGenericSuperclass(); //得到父类 clazz = （Class)pt.getActualTypeArguments()[0]; } public void add(T t){ session.save(t); } public T find(String id){ return (T)session.get(clazz,id) } public void update(T t){ session.update(t); } public void delete(String id){ T t = (T)session.get(clazz,id); session.delete(t); } } 注解配置 1 annotation 1 自定义注解 注解可以接受的数据类型有： 1 基本数据类型，String，Class，annotation，枚举类型，以及上述类型的一维数组 2 元annotation，修饰注解的注解 @Retention 用于指定该Annotation可以保留的域，可以通过RetentionPolicy.xxx来指定 @Target 用于指定注解可以用于修饰的那些成员 @Documented 可以被提取文档 @Inherited 被他修饰的注解具有可继承性，就是说被修饰的注解修饰的方法被子类继承后，子类的给方法上也自动拥有该注解 3 通过注解注入对象 1 得到需要注入的属性 2 得到要注入的属性需要的类型 3 创建属性需要的对象 4 得到属性的写方法 5 反射出方法上声明的注解 6 得到注解上声明的信息，填充要注入的对象 7 将对象注入目标 2 解析注解 1 使用反射技术拿到方法，然后拿到方法上的注解 04 动态代理（非常重要） 1 newProxyInstance(ClassLoader，Class，InvacationHandler) 2 使用动态代理做filte可以不用重写代理对象中的所有方法，只要拦截自己感兴趣的方法就行了 3 用动态代理修改bookstores的权限系统 1 建立实体建立表来保存权限数据 2 在serviceFactory中运用动态代理技术进行权限拦截，细粒度拦截到方法上 4 类加载器 1 负责将.class加载到内存中，并为之生成相应的java.lang.Class对象 2 父类委托机制 tomcat没有采用该机制，所以有可能产生classCastException异常 3 cache机制 day23 邮件原理与javaMail开发 终端直接连接互联网上的主机命令：telnet smtp.souhu.com portnumber QQ邮箱 POP3 和 SMTP 服务器地址设置如下：邮箱POP3服务器（端口995）SMTP服务器（端口465或587）qq.compop.qq.comsmtp.qq.comSMTP服务器需要身份验证。 gmail pop.gmail.com 995 SMTP服务器，处理发送邮件 telnet smtp.sina.com 25 smtp协议 ehlo xxx auth login bmluZ2h1YW5nMDA= （经过base64编码的用户名，不需要@后面的内容） aG4xMjM0NTY= （经过base64编码的密码） mail from:&lt;ninghuang00@sina.com&gt; rcpt to:&lt;ninghuang00@gmail.com&gt; data from:&lt;ninghuang00@sina.com&gt; to:&lt;ninghuang00@gmail.com&gt; subject:test xxxxxxxxxx . quit POP3服务器，处理接收邮件telnet pop3.sina.com 110 user ninghaung00 pass hn123456 stat list Struts开发快乐是建立在痛苦的基础上的，你要知道这个框架为什么好，就要知道没有这个框架之前有什么不好。 1 之前没有struts的时候写servlet遇到的问题 1 需要封装bean 2 需要转发地址 3 代码中的异常要自己手动捕捉 2 struts入门 day 24 struts Struts配置文件的常用属性&lt;action path=&quot;/register&quot; //指定action处理的地址 type=&quot;cn.hn.web.RegisterAction&quot; //指定处理请求的Action的完整类名 name=&quot;User&quot; //指定封装数据的bean的名字 scope=&quot;request&quot; //指定封装之后的bean存在什么域中，不指定默认是session attribute=&quot;myformbean&quot; //封装好的bean的key，不指定默认是name属性的值 input=&quot;/request.jsp&quot; //指定接受数据的页面，校验失败后跳转回去 forword=&quot;message,jsp&quot; //指定 收到请求时跳转的jsp页面，配置后action不再被调用 include=&quot;welcome&quot; //指定搜到请求时进行页面包含 unknow=&quot;true&quot; //默认值false，处理用户机的无效请求 parameter=&quot;method&quot; //配置action参数，在action中通过调用actionMapping.getParameter方法可以获取在这里配置的参数参数 classname=&quot;&quot; //&lt;action&gt;标签和所有的配资信息通过哪个对象进行封装，默认是actionMapping对象 validate=&quot;true&quot;&gt; //默认是true，是否在封装信息时进行校验 &lt;forword name=&quot;hello&quot; path=&quot;/success.jsp&quot;/&gt; &lt;/action&gt; 以后转发请求在struts中主要在struts-config.xml中配置就行，这样就可以直接将jsp保护起来 &lt;acition path=&quot;/RegisterUI&quot; forword=&quot;/WEB-INF/jsp/register.jsp&quot;&gt;&lt;/action&gt; 使用struts的 &lt;html:link&gt;标签构建url额好处1 使用 `&lt;html:link&gt; `标签不用关心web应用程序的名称 2 自动附加Jsessionid进行url重写 3 可以对参数信息进行url编码 使用struts完成表单校验1 在继承了ActionForm类的bean中重写函数validate方法完成校验，如果有错误跳转也由struts完成 2 在properties中配置文件信息实现国际化，在file encoding中设置勾选可以自动将中文转换成ASCII编码 struts中解决乱码问题1 可以自己写一个processor，不过中级解决方案还是用过滤器filter struts中防止表单重复提交1 点击注册按钮不直接Forword到注册页面，而是先通过一个action产生随机数，然后再转发到注册jsp页面 使用beanUtils转换表单数据UserFromBean formbean = （UserFormBean）form; User ueer = new User(); BeanUtils.copyProperties(user,formbean); 在表单校验的时候进行封装，就是在UserFormBean中维护一个User，后面直接取出来就好了，这样可以避免转换两次。不想进行表单校验的时候可以直接在UserFormBean中进行封装 day25 1 表单数据的业务逻辑校验 1 用户名 2 struts中的文件上传 1 jsp页面中表单使用post方式提交 2 enctype=”multipart/form-data”属性 3 ActionForm中使用org.apache.struts.upload.FormFile类型定义文件字段 4 fileupload.jar和io.jar 5 Action中的execute方法中使用InputStream is = file.getInputStream();方法得到文件的输入流 6 使用io的方法对文件进行上传 多文件上传在formbean中搞一个ArrayListAction中的execute方法: UpFileFormBean formbean = (UpFileFormBean) form; /*String username = formbean.getUsername(); FormFile file = formbean.getUpfile(); String filename = file.getFileName(); InputStream is = file.getInputStream(); FileOutputStream out = new FileOutputStream(&quot;/Users/huangning/Desktop/&quot; + filename); int len = 0; byte[] buffer = new byte[1024]; while((len = is.read(buffer))&gt;0){ out.write(buffer,0,len); } out.close(); is.close();*/ List&lt;FormFile&gt; list = ((UpFileFormBean) form).getAll(); for (FormFile file : list) { String filename = file.getFileName(); InputStream is = file.getInputStream(); FileOutputStream out = new FileOutputStream(&quot;/Users/huangning/Desktop/&quot; + filename); int len = 0; byte[] buffer = new byte[1024]; while ((len = is.read(buffer)) &gt; 0) { out.write(buffer, 0, len); } out.close(); is.close(); } formbean的写法: public class UpFileFormBean extends ActionForm { private String username; //private FormFile upfile; List&lt;FormFile&gt; list = new ArrayList&lt;&gt;(); public String getUsername() { return username; } public void setUsername(String username) { this.username = username; } public FormFile getList(int index) { return list.get(index); } public void setList(FormFile formFile,int index) { this.list.add(formFile); } public List&lt;FormFile&gt; getAll(){ return this.list; } } jsp中的: &lt;form action=&quot;${pageContext.request.contextPath}/UpFile.do&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt; 用户名:&lt;input type=&quot;text&quot; name=&quot;username&quot;&gt;&lt;br&gt; 上传文件1:&lt;input type=&quot;file&quot; name=&quot;list[0]&quot;&gt;&lt;br&gt; 上传文件2:&lt;input type=&quot;file&quot; name=&quot;list[1]&quot;&gt;&lt;br&gt; &lt;input type=&quot;submit&quot; value=&quot;upload&quot;&gt; &lt;/form&gt; 使用struts下载文件1 继承DownloadAction类,实现getStreamInfo方法 2 得到下载文件路径,然后返回FileStreamInfo对象就行了 DispatchAction,使用一个Action处理多个请求动态的Formbean在配置文件中配合就行 truts-Validate框架,一个struts插件,使用声明式的方式对表单进行校验,因此不需要写java代码1 维护方便 2 校验实现标准化 3 自动客户端校验 使用方法:1 把数据封装到org.Apache.struts.validator.DynaValidatorForm 2 DynaValidatorForm的validate方法被调用时,他会去调用validate框架,需要在struts中配置插件,配置步骤查看struts-core-xxx.jar下的validation-rules.xml中的&lt;plugin... 3 为了只指挥表单校验,需要在WEB-INF目录下创建validation.xml文件 4 为了在校验失败后提示错误消息,在src目录下创建资源文件,参照struts-core-xxx.jar下的validation-rules.xml struts中的权限管理 在要求权限访问的方法上使用@Permission注解, 然后继承Struts的RequestProcessor类, 重写processActionPerform方法 写一个方法得到struts调用的方法 Method getInvokeMethod(HTTPServletRequest request,HTTPServletResponse response, Action action,ActionForm form,ActionMapping mapping){ //判断Struts调用的是是不是继承了DispatchAction还 String methodName = null; if(!DispatchAction.class.isAssignableFrom(action.getClass())){ methodName = &quot;execute&quot;; }else{ methodName = request.getParameter(mapping.getParameter()); } Class[] args = {ActionMapping.class,ActionForm.class,HTTPServletRequest.class,HTTPServletResponse.class}; try{ Method method = action.getClass().getMethod(methodName, args); return method; }catch(Exception e){ throw new RuntimeException(e); } } 判断调用的方法上有没有注解 Permission p = method.getAnnotation(Permission.class); if(p == null){ return super.....; }else{ //检查用户是否登录,是否有权限... } 在配置文件总进行配置controller log4j的使用 导入jar包 然后将log4j.properties放在src下.可以配置log的输出级别,相互出样式,路径,输出的追加方法 得到Logger对象 Logger log = Logger.getLogger(Demo.class); struts数据库管理模块 设计一个数据库类Database public class Database{ private String id; private String name; private String path; private Date backuptime; ... } 然后编写DatabaseDaoImpl 创建数据库 编写Service层中的方法backupDatabase()方法 调用java中使用Windows cmd的方法备份数据库 String filename = path + &quot;\\&quot; + &quot;bookstore&quot; + number + &quot;.sql&quot;;//得到数据库保存文件路径 String cmd = &quot;cmd /c mysqldump -uroot -phn123456 bookstore&gt;&quot; + filename; //导入命令是 &quot;cmd /c mysql -uroot -phn123456 bookstore&lt;&quot; + path + &quot;\\&quot; + filename; Runtime.getRuntime().exec(cmd); 事务管理 在JdbcUtils中添加事务管理的方法 public void deleteDatabase(String id){ //删除数据库还要删除文件 try{ JdbcUtils.startTransaction(); Database db = dao.find(id); ddao.delete(id); String filename = ... File file = new File(filename); if(file.exists()){ file.delete(); } JdbcUtils.commit(); }catch(Exception e){ JdbcUtils.rollback(); }finally{ //很重要,解除线程上绑定的连接 JdbcUtils.close(); } }]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>javaweb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring32天笔记]]></title>
    <url>%2F2018%2F07%2F08%2Fspring32%E5%A4%A9%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[这是摘要 spring 021 环境搭建，如何获得beans.xml的标签提示2 在xml中配置bean spring 03 自己写的实例化bean容器1 spring是如何创建bean 自己写一个 spring 04 bean实例化的另外两种方式1 静态工厂方法2 实例工厂方法 spring 05 bean的作用域1 singleton 单例模式，在每个spring ioc 容器中一个bean定义只有一个对象实例。（默认模式）2 prototype 每次从容器获取bean都是新对象指定bean的初始化方法和销毁方法 单例模式是在容器初始化的时候初始化（可以用lazy-init=”true”）延迟 prototype是在获取bean实例的时候实例化 spring 07 依赖注入 1 通过构造器参数注入 2 通过属性的set方法 3 通过内部bean（bean无法被其他的调用） 如何注入基本类型的属性 spring 09 注入集合类型的属性 set list map properties spring 10 通过构造器注入属性 spring 11 通过Field（字段）注入（用于注解方式）1 手工装配依赖对象 1 xml方式 2 使用注解进行装配 @Autowired 默认按类型进行装配 @Resource 默认按名称进行装配2 自动装配依赖对象 spring 11 注解方式注入 自己实现注解spring 12 @Resource 实现在字段和setter方法上的注解spring 13 @Autowired 注解的使用 默认按类型进行装配 可以用@Qualified(“personDaoxxx”)用名称进行装配 自动装配spring 14 通过在classpath自动扫描的方式吧组件纳入spring容器中管理 @Componet 当组件不好归类的时候用 @Service 业务层 @Controller 控制层 @Repository DAO层 @Scope(“prototype”) 指定bean的作用域 @PostConstruct 指定bean的初始化方法（使用广泛） @Predestroy bean销毁前关闭资源spring 15 AOP技术 代理对象（权限管理）Proxy spring 16 CGlib的使用 当要代理的类没有继承接口的时候用 AOP中基本概念 spring 17 spring框架的AOP编程 1 基于xml配置的方式 2 基于注解方式 基于注解方式声明切面 //（*）任意返回值 包名加类名.（*）所有方法（..代表有无参数皆可） @Pointcut(&quot;excetion (* cn.hn.service.impl.PersonServiceBean.*(..))&quot;) spring 18 声明切面的各种注解 spring 19 使用xml配置切面 spring 20 表达式的介绍 spring 21 spring+JDBC组合开发 beans.xml引入外部的配置 文件 如果是在src目录下的话，该位置为：classpath:文件名.后缀 如果是在/WEB-INF/目录下的话，该位置为： /WEB-INF/文件名.后缀。spring 23 spring中的事务管理 什么时候回滚 Checked异常继承java.lang.Exception类。Unchecked异常继承自java.lang.RuntimeException类。 遇到RuntimeException的时候回滚 Exception不会回滚 事务传播属性（spring实现） 数据库的隔离级别（底层数据库实现）spring 24 使用xml配置事务 spring 25 spring+Hibernate+Struts整合开发spring 26 先完成spring+Hibernate的集成，能够操作数据库spring 27 然后完成struts的集成，能够通过action访问数据库spring 28 使用spring的依赖注入来获得PersonService 首先将action交给spring管理,在beans.xml中配置bean时要注意name属性的命名要和struts-config.xml中的action-mapping中的action的path属性命名一致。 然后在struts-config中配置一个请求控制器，来先根据action中的path属性值到spring容器中寻找beanspring 29 Hibernate二级缓存 hibernate5和之前的有点不同 hibernate.cache.use_second_level_cache=true hibernate.cache.use_query_cache=false hibernate.cache.region.factory_class=org.hibernate.cache.ehcache.EhCacheRegionFactory &lt;!-- hibernate3的二级缓存配置 hibernate.cache.provider_class=org.hibernate.cache.EhCacheProvider --&gt; spring 30 解决struts中文乱码问题 解决Hibernate因为session关闭导致的延迟加载异常spring 31 spring2.5+JPA+strutsspring 32 配置struts]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机]]></title>
    <url>%2F2018%2F07%2F08%2FJava%E8%99%9A%E6%8B%9F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[JVM相关知识 相关概念吞吐量吞吐量 = 运行用户代码时间 /（运行用户代码时间 + 垃圾收集时间）。 内核态是什么保护模式类加载过程 加载 验证 准备 解析 初始化 内存管理StackOverflow启动一个新的线程的时候,虚拟机会分配一个栈,java栈以栈帧的方式保存线程运行状态,然后线程调用方法的时候,虚拟机就会压入新的栈帧,只要方法没有返回,栈帧就会存在,当栈帧太多就会导致溢出. OutOfMemory 栈内存溢出 就是启动的线程太多,没有足够的空间分配java栈了.一个线程的java栈大小由-Xss参数设置 堆内存溢出 java堆中存放对象实例的空间不足,通过-Xmx设置最大值. 方法区和运行时常量池溢出 -XX:permSize=10M -XX:MaxPermSize=10M 本机直接内存溢出 -XX:MaxDirectMemorySize 内存模型根据某种特定的操作协议,对特定的内存或者高速缓存进行读写访问的过程抽象 内存模型为了保证共享内存的正确性（可见性、有序性、原子性），内存模型定义了共享内存系统中多线程程序读写操作行为的规范。通过这些规则来规范对内存的读写操作，从而保证指令执行的正确性。它与处理器有关、与缓存有关、与并发有关、与编译器也有关。他解决了CPU多级缓存、处理器优化、指令重排等导致的内存访问问题，保证了并发场景下的一致性、原子性和有序性。 java内存模型Java内存模型（Java Memory Model ,JMM）就是一种符合内存模型规范的，屏蔽了各种硬件和操作系统的访问差异的，保证了Java程序在各种平台下对内存的访问都能保证效果一致的机制及规范。 8中原子操作: lock:作用于主内存的变量,将一个变量标记为一条线程独占 unlock:主内存,施放线程独占的变量 read:主内存,将变量值从主内存传输到线程的工作内存,之后要使用load操作 load:作用于工作内存,将从主内存中read得到的变量值放入工作内存的变量副本中 use:工作内存,将工作内存中一个变量的值传递给执行引擎 assign:工作内存,将从执行引擎接收到的值赋值给工作内存中的变量 store:工作内存,将工作内存中的变量值从工作内存传送到主内存,供之后的write操作使用 write:主内存,包store操作从工作内存中取得的值放入主内存的变量中 垃圾回收(GC)分为堆(heap)和非堆(non-heap)简单说,堆就是给开发人员使用的代码可及的存放类实例和数组的内存区域;非堆就是除了堆之外的留给JVM自己使用的内存区域. 方法区的变动 (JDK1.8)移除了永久带,类元信息被放到metaSpace(本地化的内存?),一定程度上解决原来运行时加载大量类信息而引起的Full GC的问题,如反射,动态代理 将常量池(JDK1.7中就移了)和静态变量放到了java堆中 运行时数据区 程序计数器：线程私有。是一块较小的内存，是当前线程所执行的字节码的行号指示器。是Java虚拟机规范中唯一没有规定OOM（OutOfMemoryError）的区域。 Java栈：线程私有。生命周期和线程相同。是Java方法执行的内存模型。执行每个方法都会创建一个栈帧，用于存储局部变量和操作数（对象引用）。局部变量所需要的内存空间大小在编译期间完成分配。所以栈帧的大小不会改变。存在两种异常情况：若线程请求深度大于栈的深度，抛StackOverflowError。若栈在动态扩展时无法请求足够内存，抛OOM。 Java堆：所有线程共享。虚拟机启动时创建。存放对象实力和数组。所占内存最大。分为新生代（Young区），老年代（Old区）。新生代分Eden区，Servior区。Servior区又分为From space区和To Space区。Eden区和Servior区的内存比为8:1。 当扩展内存大于可用内存，抛OOM。 方法区：所有线程共享。用于存储已被虚拟机加载的类信息、常量、静态变量等数据。又称为非堆（Non–Heap）。方法区又称“永久代”(JDK1.8中已经移除)。GC很少在这个区域进行，但不代表不会回收。这个区域回收目标主要是针对常量池的回收和对类型的卸载。当内存申请大于实际可用内存，抛OOM。 本地方法栈：线程私有。与Java栈类似，但是不是为Java方法（字节码）服务，而是为本地非Java方法服务。也会抛StackOverflowError和OOM。 堆内存结构 JVM堆内存分为2块：Permanent Space 和 Heap Space Permanent 即 持久代（Permanent Generation），主要存放的是Java类定义信息，与垃圾收集器要收集的Java对象关系不大。 Heap = { Old + NEW(Eden, from, to) }，Old 即 年老代（Old Generation），New 即 年轻代（Young Generation）。年老代和年轻代的划分对垃圾收集影响比较大。 年轻代所有新生成的对象首先都是放在年轻代。年轻代的目标就是尽可能快速的收集掉那些生命周期短的对象。年轻代一般分3个区，1个Eden区，2个Survivor区（from 和 to）。大部分对象在Eden区中生成。当Eden区满时，还存活的对象将被复制到Survivor区（两个中的一个），当一个Survivor区满时，此区的存活对象将被复制到另外一个Survivor区，当另一个Survivor区也满了的时候，从前一个Survivor区复制过来的并且此时还存活的对象，将可能被复制到年老代。2个Survivor区是对称的，没有先后关系，所以同一个Survivor区中可能同时存在从Eden区复制过来对象，和从另一个Survivor区复制过来的对象；而复制到年老区的只有从另一个Survivor区过来的对象。而且，因为需要交换的原因，Survivor区至少有一个是空的。特殊的情况下，根据程序需要，Survivor区是可以配置为多个的（多于2个），这样可以增加对象在年轻代中的存在时间，减少被放到年老代的可能。针对年轻代的垃圾回收即 Young GC。 年老代在年轻代中经历了N次（可配置）垃圾回收后仍然存活的对象，就会被复制到年老代中。因此，可以认为年老代中存放的都是一些生命周期较长的对象。针对年老代的垃圾回收即 Full GC。 持久代用于存放静态类型数据，如 Java Class, Method 等。持久代对垃圾回收没有显著影响。但是有些应用可能动态生成或调用一些Class，例如 Hibernate CGLib 等，在这种时候往往需要设置一个比较大的持久代空间来存放这些运行过程中动态增加的类型。 什么时候回收触发GC的条件,JVM卸载类的判断条件:根据Eden区和From Space区的内存大小来决定。当内存大小不足时，则会启动GC线程并停止应用线程。 Minor GC(新生代GC)当Eden区满时，触发指发生在新生代的垃圾收集动作，因为Java对象大多都具备朝生夕灭的特性，所以Minor GC非常频繁，一般回收速度也比较快。 Full GC(老年代GC)也叫Major GC,速度一般会比Minor GC慢10倍以上。 调用System.gc时，系统建议执行Full GC，但是不必然执行 老年代空间不足 1.7之前的方法区(永久带)空间不足 通过Minor GC后进入老年代的平均大小大于老年代的可用内存 由Eden区、From Space区向To Space区复制时，对象大小大于To Space可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小 回收对象通过可达性分析法无法搜索到的对象和可以搜索到的对象。对于搜索不到的方法进行标记。 该类的所有实例都已经被回收,也就是java堆中不存在该类的任何实例 加载该类的ClassLoader已经被回收 该类对应的java.lang.Class对象没有在任何地方呗引用,无法通过反射在任何地方访问该类的方法 怎么回收对于可以搜索到的对象进行复制操作，对于搜索不到的对象，调用finalize()方法进行释放。当GC线程启动时，会通过可达性分析法把Eden区和From Space区的存活对象复制到To Space区，然后把Eden Space和From Space区的对象释放掉。当GC轮训扫描To Space区一定次数后，把依然存活的对象复制到老年代，然后释放To Space区的对象。 引用的分类 强引用 类似Object obj = new Object()这类,只要强引用在,永远不会被回收 软引用 有用但是非必须的,只有内存不够的时候会被回收,如果回收之后还是不够才会抛出内存溢出异常 弱引用 只能存活到下一次垃圾回收 虚引用 唯一目的就是在一个关联了虚引用的对象被回收的时候收到系统通知 判断对象是否存活的方法 引用计数法 当对象被引用的时候计数加一,不用的时候减一,缺点是无法解决对象之间相互循环引用的问题,比如两个对象中的成员变量相互引用的时候 可达性分析算法 当一个对象到GC Roots没有任何引用链相连的时候,就是该对象不可用 GC Roots包括:1. 虚拟机栈(栈帧中的本地变量表)中的引用对象 2. 方法区中的类的静态引用属性 3. 方法区中的常量引用的对象 4. 本地方法栈中JNI(即一般说的Native方法)引用的对象 回收方法区(HotSpot中的永久带)回收性价比比较低,主要回收弃用常量和无用的类 判断弃用常量 比如字面量”abc”,当没有String对象叫”abc”的时候 判断无用的类(同时满足以下三个条件) 当java堆中没有该类的实例 加载该类的ClassLoader被回收 该类的java.lang.Class对象没有在任何地方被引用,也就是无法在任何地方通过反射访问该类的方法 垃圾回收算法 标记清除算法 不足:标记和清除的效率不高,然后就是清除之后会产生大量不连续的内存碎片 复制算法 直接将内存一分为二,当其中一块内存用完了,将其中还存活的对象复制到另一块内存上,比较浪费空间. 复制收集算法(回收新生代) 将内存分为一块Eden空间和两块Survivor空间,8:1:1,回收时将Eden和Survivor中还存活的对象复制到另一块Survivor空间中,如果Survivor空间内存不够,就需要用到老年代 标记整理算法(回收老年代) 分代收集算法 一般将java堆分成新生代和老年代,再使用合适的回收算法 并行和并发并行（Parallel）：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行的，可能会交替执行），用户程序在继续运行。而垃圾收集程序运行在另一个CPU上。 垃圾收集器(书上列举了七种) 参考地址:https://crowhawk.github.io/2017/08/15/jvm_3/ 新生代收集器 Serial收集器 采用复制算法的新生代收集器。 是一个单线程收集器，只会使用一个CPU或一条收集线程去完成垃圾收集工作，在进行垃圾收集时，必须暂停其他所有的工作线程，直至Serial收集器收集结束为止（“Stop The World”）。 这项工作是由虚拟机在后台自动发起和自动完成的，在用户不可见的情况下把用户正常工作的线程全部停掉，这对很多应用来说是难以接受的。下图展示了Serial 收集器（老年代采用Serial Old收集器）的运行过程： 依然是HotSpot虚拟机运行在Client模式下的默认的新生代收集器。它也有着优于其他收集器的地方：简单而高效（与其他收集器的单线程相比），对于限定单个CPU的环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得更高的单线程收集效率。在用户的桌面应用场景中，分配给虚拟机管理的内存一般不会很大，收集几十兆甚至一两百兆的新生代（仅仅是新生代使用的内存，桌面应用基本不会再大了），停顿时间完全可以控制在几十毫秒最多一百毫秒以内，只要不频繁发生，这点停顿时间可以接收。所以，Serial收集器对于运行在Client模式下的虚拟机来说是一个很好的选择。 ParNew收集器 ParNew收集器就是Serial收集器的多线程版本，它也是一个新生代收集器。 除了使用多线程进行垃圾收集外，其余行为包括Serial收集器可用的所有控制参数、收集算法（复制算法）、Stop The World、对象分配规则、回收策略等与Serial收集器完全相同，两者共用了相当多的代码。ParNew收集器的工作过程如下图（老年代采用Serial Old收集器）： ParNew收集器除了使用多线程收集外，其他与Serial收集器相比并无太多创新之处，但它却是许多运行在Server模式下的虚拟机中首选的新生代收集器，其中有一个与性能无关的重要原因是，除了Serial收集器外，目前只有它能和CMS收集器（Concurrent Mark Sweep）配合工作。ParNew 收集器在单CPU的环境中绝对不会有比Serial收集器有更好的效果，甚至由于存在线程交互的开销，该收集器在通过超线程技术实现的两个CPU的环境中都不能百分之百地保证可以超越。在多CPU环境下，随着CPU的数量增加，它对于GC时系统资源的有效利用是很有好处的。它默认开启的收集线程数与CPU的数量相同，在CPU非常多的情况下可使用-XX:ParallerGCThreads参数设置。 Parallel Scavenge 收集器 并行的多线程新生代收集器，它也使用复制算法。 CMS等收集器的关注点是尽可能缩短垃圾收集时用户线程的停顿时间，而Parallel Scavenge收集器的目标是达到一个可控制的吞吐量（Throughput）。Parallel Scavenge收集器除了会显而易见地提供可以精确控制吞吐量的参数，还提供了一个参数-XX:+UseAdaptiveSizePolicy，这是一个开关参数，打开参数后，就不需要手工指定新生代的大小（-Xmn）、Eden和Survivor区的比例（-XX:SurvivorRatio）、晋升老年代对象年龄（-XX:PretenureSizeThreshold）等细节参数了，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量，这种方式称为GC自适应的调节策略（GC Ergonomics）。自适应调节策略也是Parallel Scavenge收集器与ParNew收集器的一个重要区别。另外值得注意的一点是，Parallel Scavenge收集器无法与CMS收集器配合使用，所以在JDK 1.6推出Parallel Old之前，如果新生代选择Parallel Scavenge收集器，老年代只有Serial Old收集器能与之配合使用。 年老代收集器 Serial Old收集器是Serial收集器的老年代版本，它同样是一个单线程收集器，使用“标记-整理”（Mark-Compact）算法。此收集器的主要意义也是在于给Client模式下的虚拟机使用。如果在Server模式下，它还有两大用途： 在JDK1.5 以及之前版本（Parallel Old诞生以前）中与Parallel Scavenge收集器搭配使用。 作为CMS收集器的后备预案，在并发收集发生Concurrent Mode Failure时使用。工作流程见新生代收集器Serial收集器. Parallel Old收集器 是Parallel Scavenge收集器的老年代版本，使用多线程和“标记-整理”算法。 在JDK 1.6中才开始提供的在此之前，如果新生代选择了Parallel Scavenge收集器，老年代除了Serial Old以外别无选择，所以在Parallel Old诞生以后，“吞吐量优先”收集器终于有了比较名副其实的应用组合，在注重吞吐量以及CPU资源敏感的场合，都可以优先考虑Parallel Scavenge加Parallel Old收集器。Parallel Old收集器的工作流程与Parallel Scavenge相同Parallel Scavenge/Parallel Old收集器配合使用的流程图： CMS收集器(Concurrent Mark Sweep) 是一种以获取最短回收停顿时间为目标的收集器，它非常符合那些集中在互联网站或者B/S系统的服务端上的Java应用，这些应用都非常重视服务的响应速度。 从名字上（“Mark Sweep”）就可以看出它是基于“标记-清除”算法实现的。CMS收集器工作的整个流程分为以下4个步骤： 初始标记（CMS initial mark）：仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，需要“Stop The World”。 并发标记（CMS concurrent mark）：进行GC Roots Tracing的过程，在整个过程中耗时最长,查找初始标记中找到的对象相关联的存活对象 重新标记（CMS remark）：为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短。此阶段也需要“Stop The World”。 并发清除（CMS concurrent sweep）由于整个过程中耗时最长的并发标记和并发清除过程收集器线程都可以与用户线程一起工作，所以，从总体上来说，CMS收集器的内存回收过程是与用户线程一起并发执行的。通过下图可以比较清楚地看到CMS收集器的运作步骤中并发和需要停顿的时间: 优点:并发收集、低停顿缺点: 对CPU资源非常敏感其实，面向并发设计的程序都对CPU资源比较敏感。在并发阶段，它虽然不会导致用户线程停顿，但会因为占用了一部分线程（或者说CPU资源）而导致应用程序变慢，总吞吐量会降低。CMS默认启动的回收线程数是（CPU数量+3）/4，也就是当CPU在4个以上时，并发回收时垃圾收集线程不少于25%的CPU资源，并且随着CPU数量的增加而下降。但是当CPU不足4个时（比如2个），CMS对用户程序的影响就可能变得很大，如果本来CPU负载就比较大，还要分出一半的运算能力去执行收集器线程，就可能导致用户程序的执行速度忽然降低了50%，其实也让人无法接受。 无法处理浮动垃圾（Floating Garbage）可能出现“Concurrent Mode Failure”失败而导致另一次Full GC的产生。由于CMS并发清理阶段用户线程还在运行着，伴随程序运行自然就还会有新的垃圾不断产生。这一部分垃圾出现在标记过程之后，CMS无法再当次收集中处理掉它们，只好留待下一次GC时再清理掉。这一部分垃圾就被称为“浮动垃圾”。也是由于在垃圾收集阶段用户线程还需要运行，那也就还需要预留有足够的内存空间给用户线程使用，因此CMS收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，需要预留一部分空间提供并发收集时的程序运作使用。 标记-清除算法导致的空间碎片CMS是一款基于“标记-清除”算法实现的收集器，这意味着收集结束时会有大量空间碎片产生。空间碎片过多时，将会给大对象分配带来很大麻烦，往往出现老年代空间剩余，但无法找到足够大连续空间来分配当前对象。 G1收集器(Garbage-First) G1收集器是当今收集器技术发展最前沿的成果之一，它是一款面向服务端应用的垃圾收集器，HotSpot开发团队赋予它的使命是（在比较长期的）未来可以替换掉JDK 1.5中发布的CMS收集器。与其他GC收集器相比，G1具备如下特点： 并行与并发G1能充分利用多CPU、多核环境下的硬件优势，使用多个CPU来缩短“Stop The World”停顿时间，部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让Java程序继续执行。 分代收集与其他收集器一样，分代概念在G1中依然得以保留。虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但它能够采用不同方式去处理新创建的对象和已存活一段时间、熬过多次GC的旧对象来获取更好的收集效果。 空间整合G1从整体来看是基于“标记-整理”算法实现的收集器，从局部（两个Region之间）上来看是基于“复制”算法实现的。这意味着G1运行期间不会产生内存空间碎片，收集后能提供规整的可用内存。此特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次GC。 可预测的停顿这是G1相对CMS的一大优势，降低停顿时间是G1和CMS共同的关注点，但G1除了降低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内，消耗在GC上的时间不得超过N毫秒，这几乎已经是实时Java（RTSJ）的垃圾收集器的特征了。 横跨整个堆内存在G1之前的其他收集器进行收集的范围都是整个新生代或者老生代，而G1不再是这样。G1在使用时，Java堆的内存布局与其他收集器有很大区别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，而都是一部分Region（不需要连续）的集合。每个region被标记成E,S,O,H,其中H代表Humongous,用来分配 大对象(当对象实例大小超过Region大小的一半) 建立可预测的时间模型G1收集器之所以能建立可预测的停顿时间模型，是因为它可以有计划地避免在整个Java堆中进行全区域的垃圾收集。G1跟踪各个Region里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的Region（这也就是Garbage-First名称的来由）。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了G1收集器在有限的时间内可以获取尽可能高的收集效率。 避免全堆扫描——Remembered SetG1把Java堆分为多个Region，就是“化整为零”。但是Region不可能是孤立的，一个对象分配在某个Region中，可以与整个Java堆任意的对象发生引用关系。在做可达性分析确定对象是否存活的时候，需要扫描整个Java堆才能保证准确性，这显然是对GC效率的极大伤害。为了避免全堆扫描的发生，虚拟机为G1中每个Region维护了一个与之对应的Remembered Set。虚拟机发现程序在对Reference类型的数据进行写操作时，会产生一个Write Barrier暂时中断写操作，检查Reference引用的对象是否处于不同的Region之中（在分代的例子中就是检查是否老年代中的对象引用了新生代中的对象），如果是，便通过CardTable把相关引用信息记录到被引用对象所属的Region的Remembered Set之中。当进行内存回收时，在GC根节点的枚举范围中加入Remembered Set即可保证不对全堆扫描也不会有遗漏。 GC的类型 young gc当所有的eden region填满,会回收存活对象到survivor region, mixed gc当young region到了一定的比例,会回收整个young region以及部分的old region full gc当mixed gc来不及,导致old region被填满,则会触发,就是serial old的单线程回收,要尽量避免 G1收集器运作的步骤如果不计算维护Remembered Set的操作，G1收集器的运作大致可划分为以下几个步骤： 初始标记（Initial Marking）仅仅只是标记一下GC Roots 能直接关联到的对象，并且修改TAMS（Nest Top Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可以的Region中创建对象，此阶段需要停顿线程，但耗时很短。 并发标记（Concurrent Marking）从GC Root 开始对堆中对象进行可达性分析，找到存活对象，此阶段耗时较长，但可与用户程序并发执行。 最终标记（Final Marking）为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程的Remembered Set Logs里面，最终标记阶段需要把Remembered Set Logs的数据合并到Remembered Set中，这阶段需要停顿线程，但是可并行执行。 筛选回收（Live Data Counting and Evacuation）首先对各个Region中的回收价值和成本进行排序，根据用户所期望的GC 停顿是时间来制定回收计划。此阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分Region，时间是用户可控制的，而且停顿用户线程将大幅度提高收集效率。通过下图可以比较清楚地看到G1收集器的运作步骤中并发和需要停顿的阶段（Safepoint处）： 垃圾收集器总结 JVM性能调优的大致步骤参考地址:https://blog.csdn.net/rickyit/article/details/53895060参考地址(强烈推荐):https://blog.csdn.net/kthq/article/details/8618052 年轻代大小选择 响应时间优先的应用 ：尽可能设大，直到接近系统的最低响应时间限制 （根据实际情况选择）。在此种情况下，年轻代收集发生的频率也是最小的。同时，减少到达年老代的对象。 吞吐量优先的应用 ：尽可能的设置大，可能到达Gbit的程度。因为对响应时间没有要求，垃圾收集可以并行进行，一般适合8CPU以上的应用。 年老代大小选择 响应时间优先的应用 ：年老代使用并发收集器，所以其大小需要小心设置，一般要考虑并发会话率 和会话持续时间 等一些参数。如果堆设置小了，可以会造成内存碎片、高回收频率以及应用暂停而使用传统的标记清除方式；如果堆大了，则需要较长的收集时间。最优化的方案，一般需要参考以下数据获得： 并发垃圾收集信息 持久代并发收集次数 传统GC信息 花在年轻代和年老代回收上的时间比例减少年轻代和年老代花费的时间，一般会提高应用的效率 吞吐量优先的应用 ：一般吞吐量优先的应用都有一个很大的年轻代和一个较小的年老代。原因是，这样可以尽可能回收掉大部分短期对象，减少中期的对象，而年老代尽存放长期存活对象。 较小堆引起的碎片问题 因为年老代的并发收集器使用标记、清除算法，所以不会对堆进行压缩。当收集器回收时，他会把相邻的空间进行合并，这样可以分配给较大的对象。但是，当堆空间较小时，运行一段时间以后，就会出现“碎片”，如果并发收集器找不到足够的空间，那么并发收集器将会停止，然后使用传统的标记、清除方式进行回收。如果出现“碎片”，可能需要进行如下配置：-XX:+UseCMSCompactAtFullCollection ：使用并发收集器时，开启对年老代的压缩。 -XX:CMSFullGCsBeforeCompaction=0 ：上面配置开启的情况下，这里设置多少次Full GC后，对年老代进行压缩 回收器选择JVM给了三种选择：串行收集器、并行收集器、并发收集器 ，但是串行收集器只适用于小数据量的情况，所以这里的选择主要针对并行收集器和并发收集器。默认情况下，JDK5.0以前都是使用串行收集器，如果想使用其他收集器需要在启动时加入相应参数。JDK5.0以后，JVM会根据当前系统配置进行判断。 吞吐量优先的 并行收集器如上文所述，并行收集器主要以到达一定的吞吐量为目标，适用于科学技术和后台处理等。典型配置 ：java -Xmx3800m -Xms3800m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:ParallelGCThreads=20-XX:+UseParallelGC ：选择垃圾收集器为并行收集器。 此配置仅对年轻代有效。即上述配置下，年轻代使用并发收集，而年老代仍旧使用串行收集。-XX:ParallelGCThreads=20 ：配置并行收集器的线程数，即：同时多少个线程一起进行垃圾回收。此值最好配置与处理器数目相等。java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:ParallelGCThreads=20 -XX:+UseParallelOldGC-XX:+UseParallelOldGC ：配置年老代垃圾收集方式为并行收集。JDK6.0支持对年老代并行收集。java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:MaxGCPauseMillis=100-XX:MaxGCPauseMillis=100 : 设置每次年轻代垃圾回收的最长时间，如果无法满足此时间，JVM会自动调整年轻代大小，以满足此值。java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:MaxGCPauseMillis=100 -XX:+UseAdaptiveSizePolicy-XX:+UseAdaptiveSizePolicy ：设置此选项后，并行收集器会自动选择年轻代区大小和相应的Survivor区比例，以达到目标系统规定的最低相应时间或者收集频率等，此值建议使用并行收集器时，一直打开。 响应时间优先的 并发收集器如上文所述，并发收集器主要是保证系统的响应时间，减少垃圾收集时的停顿时间。适用于应用服务器、电信领域等。典型配置 ：java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:ParallelGCThreads=20 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC-XX:+UseConcMarkSweepGC ：设置年老代为并发收集。测试中配置这个以后，-XX:NewRatio=4的配置失效了，原因不明。所以，此时年轻代大小最好用-Xmn设置。-XX:+UseParNewGC :设置年轻代为并行收集。可与CMS收集同时使用。JDK5.0以上，JVM会根据系统配置自行设置，所以无需再设置此值。java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseConcMarkSweepGC -XX:CMSFullGCsBeforeCompaction=5 -XX:+UseCMSCompactAtFullCollection-XX:CMSFullGCsBeforeCompaction ：由于并发收集器不对内存空间进行压缩、整理，所以运行一段时间以后会产生“碎片”，使得运行效率降低。此值设置运行多少次GC以后对内存空间进行压缩、整理。-XX:+UseCMSCompactAtFullCollection ：打开对年老代的压缩。可能会影响性能，但是可以消除碎片 堆大小的典型设置： 1. `java -Xmx3550m -Xms3550m -Xmn2g -Xss128k` 12345678-Xmx3550m ：设置JVM最大可用内存为3550M。-Xms3550m ：设置JVM初始内存为3550m。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。-Xmn2g ：设置年轻代大小为2G。整个堆大小 = 年轻代大小 + 年老代大小 + 持久代大小 。持久代一般固定大小为64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8。-Xss128k ：设置每个线程的堆栈大小。JDK5.0以后每个线程堆栈大小为1M，以前每个线程堆栈大小为256K。更具应用的线程所需内存大小进行调整。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右。 2. `java -Xmx3550m -Xms3550m -Xss128k -XX:NewRatio=4 -XX:SurvivorRatio=4 -XX:MaxPermSize=16m` 123456789-XX:MaxTenuringThreshold=0-XX:NewRatio=4 :设置年轻代（包括Eden和两个Survivor区）与年老代的比值（除去持久代）。设置为4，则年轻代与年老代所占比值为1：4，年轻代占整个堆栈的1/5-XX:SurvivorRatio=4 ：设置年轻代中Eden区与Survivor区的大小比值。设置为4，则两个Survivor区与一个Eden区的比值为2:4，一个Survivor区占整个年轻代的1/6-XX:MaxPermSize=16m :设置持久代大小为16m。-XX:MaxTenuringThreshold=0 ：设置垃圾最大年龄。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代 。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象再年轻代的存活时间 ，增加在年轻代即被回收的概论。 逃逸分析参考地址:http://www.hollischuang.com/archives/2398类实例和数组不一定都是分配在堆上了 概念逃逸分析的基本行为就是分析对象动态作用域：当一个对象在方法中被定义后，它可能被外部方法所引用，例如作为调用参数传递到其他地方中，称为方法逃逸。123456public static StringBuffer craeteStringBuffer(String s1, String s2) &#123; StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); return sb;&#125; StringBuffer sb是一个方法内部变量，上述代码中直接将sb返回，这样这个StringBuffer有可能被其他方法所改变，这样它的作用域就不只是在方法内部，虽然它是一个局部变量，称其逃逸到了方法外部。甚至还有可能被外部线程访问到，譬如赋值给类变量或可以在其他线程中访问的实例变量，称为线程逃逸。使用逃逸分析，编译器可以对代码做如下优化： 同步省略。如果一个对象被发现只能从一个线程被访问到，那么对于这个对象的操作可以不考虑同步。 将堆分配转化为栈分配。如果一个对象在子程序中被分配，要使指向该对象的指针永远不会逃逸，对象可能是栈分配的候选，而不是堆分配。 分离对象或标量替换。有的对象可能不需要作为一个连续的内存结构存在也可以被访问到，那么对象的部分（或全部）可以不存储在内存，而是存储在CPU寄存器中。-XX:+DoEscapeAnalysis ： 表示开启逃逸分析 -XX:-DoEscapeAnalysis ： 表示关闭逃逸分析 从jdk 1.7开始已经默认开始逃逸分析，如需关闭，需要指定-XX:-DoEscapeAnalysis 实验看以下代码:1234567891011121314151617181920212223public static void main(String[] args) &#123; long a1 = System.currentTimeMillis(); for (int i = 0; i &lt; 1000000; i++) &#123; alloc(); &#125; // 查看执行时间 long a2 = System.currentTimeMillis(); System.out.println("cost " + (a2 - a1) + " ms"); // 为了方便查看堆内存中对象个数，线程sleep try &#123; Thread.sleep(100000); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125;&#125;private static void alloc() &#123; User user = new User();&#125;static class User &#123;&#125; 关闭JIT编译器逃逸分析-Xmx4G -Xms4G -XX:-DoEscapeAnalysis -XX:+PrintGCDetails -XX:+HeapDumpOnOutOfMemoryError 然后在终端中查看堆中的对象数量:100万个 12345678910111213➜ ~ jps2809 StackAllocTest2810 Jps➜ ~ jmap -histo 2809 num #instances #bytes class name---------------------------------------------- 1: 524 87282184 [I 2: 1000000 16000000 StackAllocTest$User 3: 6806 2093136 [B 4: 8006 1320872 [C 5: 4188 100512 java.lang.String 6: 581 66304 java.lang.Class 打开逃逸分析,只有8万多-Xmx4G -Xms4G -XX:+DoEscapeAnalysis -XX:+PrintGCDetails -XX:+HeapDumpOnOutOfMemoryError 123456789101112131415➜ ~ jps7092858 Launcher2859 StackAllocTest2860 Jps➜ ~ jmap -histo 2859 num #instances #bytes class name---------------------------------------------- 1: 524 101944280 [I 2: 6806 2093136 [B 3: 83619 1337904 StackAllocTest$User 4: 8006 1320872 [C 5: 4188 100512 java.lang.String 6: 581 66304 java.lang.Class]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>Java</tag>
        <tag>线程安全</tag>
        <tag>虚拟机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式]]></title>
    <url>%2F2018%2F07%2F08%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[设计模式概念及应用 设计模式的介绍和应用场景工厂模式，观察者模式，模板方法模式，策略模式，职责链模式等等，通常会结合Spring和UML类图提问 介绍六大原则创建型模式单例模式保证内存中只存在一个对象:单例模式123456789101112//饿汉模式public class Singleton&#123; //类静态成员变量在类加载之后初始化的过程是线程安全的 private static final Singleton instance = new Singleton(); //构造函数必须是私有的 private Singleton()&#123;&#125; public static Singleton getInstance()&#123; return instance; &#125;&#125;//懒汉模式 懒汉模式的多线程问题123456789101112131415161718//饿汉模式public class Singleton&#123; private static final Singleton instance; //构造函数必须是私有的 private Singleton()&#123;&#125; public static Singleton getInstance()&#123; if(instance == null)&#123; synchronized(Singleton.class)&#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125;//懒汉模式 单例模式应用 安卓开发中使用过,MyApplication类,使用的单例模式,用来进行全局设置,如服务器地址,屏幕宽度高度等,一些初始化信息.需要全局共享资源的时候可以使用,频繁实例化然后又销毁的对象,有状态的工具类对象,日志,配置文件 JDK中实现了单例模式的,java.lang.reflect.Proxy类,Runtime类,线程池 工厂方法模式 简单工厂模式:就是根据传入不同的参数的来返回不同的类实例,这些类都继承了相同的接口,比如画图接口中有画图方法 工程方法模式:就是为每种类单独写一个工厂类 抽象工厂模式:就是定义一个抽象工厂来返回多个工厂,然后再有返回的每一种工厂返回对象实例 建造者模式 当遇到结构比较复杂的类的时候可以考虑使用 结构型模式代理模式代理模式 装饰模式装饰模式就是给被装饰的对象附加更多的功能,而且可以自由组合装饰模式的实现: 定义一个装饰类并实现要装饰的接口 将接口定义为成员变量 定义构造函数并将要装饰的接口作为参数传入 然后可以定义增强功能的类并实现要装饰的接口 使用的时候就是new一个要装饰的对象,然后new一个装饰类对象,将要装饰的对象作为构造参数,然后再new另外一个装饰类对象,将刚刚这个装饰类对象作为参数传入,以此类推 适配器模式责任链行为型模式命令模式模板方法模式策略模式就是当有几个类的方法相似的时候,比如两数的加减乘除,可以将方法抽取到一个接口中,然后分别实现,然后在一个Condition类中定义这些类的public final static对象 实例,使用的时候只要用类名取出对象调用相应的方法就行了 终结者模式观察者模式备忘录模式访问者模式java8 stream内部迭代的实现方法 回调函数参考地址:https://zhuanlan.zhihu.com/p/30052334https://blog.csdn.net/cangchen/article/details/44063359 回调函数是什么: 回调机制就是,按照约定的接口,把内部工作流的某个功能暴露给外部使用者,让外部使用者提供数据或者实现 回调函数是一个功能片段,什么时候调用由工作流决定(比如按钮),回调函数的实现者不能直接调用回调函数实现工作流的功能 应用场景: 安卓中的按键监听器,button.setOnClickListener(new OnClickListener{...}) 多线程的Runnable接口,自定义run方法实现接口,然后传入Thread,什么时候调用run()方法由工作流决定 等待网络任务的同时不能阻塞主线程任务的时候 当工作流中某个功能需要调用者来定义的时候 优点: 工作流中的某个功能可以在调用的时候来实现,灵活,比如安卓中的Button和imageButton会进行不同的业务]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日常小坑记录]]></title>
    <url>%2F2018%2F07%2F08%2F%E6%97%A5%E5%B8%B8%E5%B0%8F%E5%9D%91%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[日常琐碎的小问题 dockerdocker部署redis 使用自定义配置文件启动redis容器,立马就关闭,原因是参数中的daemonize设置成了yes spring配置整合Mybatis 配置通用Mapper插件的时候,父类Mapper要放在启动类扫面不到的地方,即不在@MapperScan(basePackages = &quot;com.hn.mapper&quot;)所在路径下报错:1Field userDao in com.hn.service.impl.UserServiceImpl required a bean of type &apos;com.hn.dao.UserDao&apos; that could not be found. 1234567@SpringBootApplication@MapperScan(basePackages = "com.hn.mapper")public class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; mac终端git中文显示的问题 可以在终端中执行以下命令解决 git config --global core.quotepath false 或者oh-my-zsh的话 打开oh-my-zsh配置文件 ~/.zshrc 在文件最后面添加如下代码： 12export LC_ALL=en_US.UTF-8export LANG=en_US.UTF-8 重启下终端（terminal）或输入 source ~/.zshrc]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快捷键]]></title>
    <url>%2F2018%2F07%2F08%2F%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[各种工具常用快捷键 vim基本操作 u:撤销上一次操作 U:撤销当前行的所有修改 Ctrl+r:重做 dd:删除一行 D 删除到行尾 ctrl+r:对撤消的撤消 ：e!:放弃更改，然后相当于重新打开 移动 b、3b、w、3w:向前\后移动几个单词，标点也算一个单词。 $：移动到行尾 3$：移动到3行后的行尾 ^:移动到行首 +：移到下一行的行首 -： 移到上一行的行首 33G：跳转到33行 此时按``可以返回到原来行 ctrl+b\f 向上\下滚动一屏 这个比较实用，记住。 q!:不保存退出 wq:保存退出 复制粘贴 复制 v进入可视模式，ma做一个标记,然后移动到相应行，y是复制,d是剪切。 粘贴 p:粘贴在游标之后 P:粘贴在游标之前 vim中Nyy可以复制光标后的N行。有时我们不容易得出行数，这时可以用做标记的方法来制定复制范围： 1. 在开始行上输入ma作一个标记a 2. 移动到结束行，输入y’a会复制当前行到标记a之间的文本。d’a会删除。 vimium sublime搜索侧边栏文件cmd p 搜索文本内容cmd r 替换字符串alt cmd f Evernote Developer Token 1S=s65:U=10754e3:E=165d76a10de:C=15e7fb8e3e8:P=1cd:A=en-devtoken:V=2:H=723743689a4e85e72255c4a4168034b1 NoteStore URL: 1https://app.yinxiang.com/shard/s65/notestore 快捷键设置 12345[ &#123; "keys": ["ctrl+e", "ctrl+o"], "command": "open_evernote_note" &#125;, &#123; "keys": ["ctrl+s"], "command": "save_evernote_note", "context": [&#123;"key": "evernote_note"&#125;, &#123;"key": "evernote_has_guid"&#125;] &#125;, &#123; "keys": ["ctrl+s"], "command": "send_to_evernote", "context": [&#123;"key": "evernote_note"&#125;, &#123;"key": "evernote_has_guid", "operator": "equal", "operand": false&#125;] &#125;]]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>快捷键</tag>
        <tag>效率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实践出真知]]></title>
    <url>%2F2018%2F07%2F08%2F%E5%AE%9E%E8%B7%B5%E5%87%BA%E7%9C%9F%E7%9F%A5%2F</url>
    <content type="text"><![CDATA[实践过程中踩过的坑. tomcat访问404可能的问题 tomcat/webapps中明明有项目文件夹,但是通过浏览器就是访问不到文件夹中的网页,有可能是因为配置文件中的mysql连接失败,项目启动失败导致无法访问所有文件. web.xml中配置有问题的话有可能会导致tomcat启动不起来]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iBase4J部署笔记]]></title>
    <url>%2F2018%2F07%2F08%2FiBase4J%E9%83%A8%E7%BD%B2%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[这是摘要 安装配置zookeeper安装配置Nginx安装配置Redis 安装路径 /usr/local/redis 各种命令 启动服务器 进入/usr/local/bin,执行命令redis-server,或者redis-server /etc/redis.conf 启动客户端 进入/usr/local/bin,执行命令redis-cli 关闭服务器 ps -u jim(替换成你的用户名) -o pid,rss,command | grep redis-server,或者kill redisr然后按tab键会自动转换成pid.]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>ibase4j</tag>
        <tag>nginx</tag>
        <tag>redis</tag>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[db2部署教程]]></title>
    <url>%2F2018%2F07%2F08%2Fdb2%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[这是摘要 Install DB2DB2 v9.7 fp5 is recommended. Follow the command steps of installing DB2 is ok. Here is a sample script which can directly install DB2 successfully.One thing to make sure is set DB2 type to be Oracle compatible. To support sql like “select from between 1 and 100”, Oracle compatible mode is required.During DB2 installation, one user account “db2inst1” is setup and we need set its password, for example, set it as “passw0rd”. login as root12mkdir /root/db2setupcd /root/db2setup download v9.7fp5_linuxx64_ese.tar.gz from ftp1234wget ftp://ningbo:sterling11@9.115.47.13/DB2/licenses/v9.7fp5_linuxx64_ese.tar.gztar -zxf v9.7fp5_linuxx64_ese.tar.gzcd ese./db2_install -f NOTSAMP # choose No when whether use different directory choose ESE as the version of DB2 Blow steps is to create instanceadd group role123groupadd dasadm1groupadd db2fadm1groupadd db2iadm1 add user( the passw0rd may not correct setted by this cmd, if so, just use passwd $username to change password )123456useradd -m -d /home/db2inst1 -g db2iadm1 db2inst1 -p passw0rduseradd -m -d /home/db2fenc1 -g db2fadm1 db2fenc1 -p passw0rduseradd -m -d /home/dasusr1 -g dasadm1 dasusr1 -p passw0rdpasswd db2inst1passwd db2fenc1passwd dasusr1 create db2 user &amp; instance123cd /opt/ibm/db2/V9.7/instance./db2icrt -u db2fenc1 db2inst1su - db2inst1 set database connection type and port123456db2set DB2COMM=tcpipdb2set DB2CODEPAGE=1208db2 update database manager configuration using svcename 50000db2startdb2stop activate the license12345678wget ftp://ningbo:sterling11@9.115.47.13/DB2/licenses/v9.7fp5_linuxx64_ese_c.tar.gztar -zxf v9.7fp5_linuxx64_ese_c.tar.gzcd ese_c/db2/license/db2licm -ldb2licm -a db2ese_c.licdb2startdb2stop login to root userset DB2 management server1234cd /opt/ibm/db2/V9.7/instance./dascrt -u dasusr1 su - dasusr1db2admin start]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>db2</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[左程云算法课进阶班]]></title>
    <url>%2F2018%2F07%2F08%2F%E5%B7%A6%E7%A8%8B%E4%BA%91%E7%AE%97%E6%B3%95%E8%AF%BE%E8%BF%9B%E9%98%B6%E7%8F%AD%2F</url>
    <content type="text"><![CDATA[左程云算法课进阶班 代码地址:https://gitee.com/ninghuang/swordTOOffer/tree/master/src/cn/hn/algoriththm 二叉搜索树套路(进阶5) 选择一个节点进行可能性分析: 最大子树来自左子树中 最大子树来自右子树中 这个节点就是二叉搜索树(判断信息需要左右子树中的最大二叉搜索树的头结点,左最大值,右最小值) 查找一棵树中的最大二叉搜索子树 返回树中最大最小值(% asset_img 子树套路返回树中最大最小值.png 子树套路返回树中最大最小值) 树中两个节点的最长距离(进阶5) 可能性分析:假设当前节点为head 如果head来,那么当前节点的来_活跃度就是所有子节点不来的活跃度总和 如果head不来,那么当前节点的不来_活跃度就是子节点来或者不来活跃度中大的那个的总和 二叉平衡搜索树(进阶4-1)所谓二叉搜索树就是树中每一个节点的左子树都比自己小,右子树都比自己大平衡性指的是左右子树的规模差不多 搜索节点 插入节点 删除节点 没有左孩子或者右孩子直接拿另一边的子树顶上来 孩子双全拿右孩子的最左节点顶上来,最左节点如果有右子树,连到最左节点的父节点上 调整平衡性 基本操作只有左旋和右旋,在一个节点上发现不平衡时,右旋就是这个节点左子树不动,往右边的父节点变成右节点,该节点的右子树变成父节点的左子树 常见的二叉平衡搜索树 AVL树严格的平衡搜索二叉树,左右子树层数差不超过一层 SB树(Size Balance Tree)当前节点为根节点的子树,节点数量不能超过他的叔叔节点作为根节点的子树的节点数 红黑树 节点是红色或黑色。 根是黑色。 所有叶子都是黑色（叶子是NIL节点）。 每个红色节点必须有两个黑色的子节点。（从每个叶子到根的所有路径上不能有两个连续的红色节点。） 从任一节点到其每个叶子的所有简单路径都包含相同数目的黑色节点。这些约束确保了红黑树的关键特性：从根到叶子的最长的可能路径不多于最短的可能路径的两倍长。结果是这个树大致上是平衡的。因为操作比如插入、删除和查找某个值的最坏情况时间都要求与树的高度成比例，这个在高度上的理论上限允许红黑树在最坏情况下都是高效的，而不同于普通的二叉查找树。要知道为什么这些性质确保了这个结果，注意到性质4导致了路径不能有两个毗连的红色节点就足够了。最短的可能路径都是黑色节点，最长的可能路径有交替的红色和黑色节点。因为根据性质5所有最长的路径都有相同数目的黑色节点，这就表明了没有路径能多于任何其他路径的两倍长。 跳表(进阶6)SkipListIterator(不同于红黑树的非树状设计) 二叉平衡搜索树应用大楼轮廓(进阶4-1) Morris遍历(进阶3-1) 遍历二叉树的时间复杂度为还是O(N),但是额外空间复杂度为O(1) 单调栈结构(进阶3-1) 求一个数左右两边最近的比自己大或者小的数 给定一个数组,希望在O(N)时间内,获得数组中每一个数字的左右离这个数字最近的大于这个数字的数 将数组中的数依次压入栈,要求下大上小,当当前要进栈的数字a大于当前栈顶的数字b时,将栈顶的数字b弹出,此时当前数字a就是b右边最近的比b大的数,b弹出之后栈顶的数字就是b左边满足要求的数 碰到一样大的数字将下标放在一起 单调栈的应用(进阶3-1)烽火台(进阶3-1) 求最大子矩阵的大小 cn.hn.advanced.MaximalRectangle.java求矩阵中的最大子矩阵 从第0行开始,到最后一行,求出以每一行为底的最大子矩阵 构造数组的maxTree cn.hn.advanced.MaxTree.java 解法一 先建一个大根堆 变成树 解法二 使用单调栈找出每个数字左右的最近最大数信息 根据信息串联 窗口结构(进阶2) MaxOfWindowUpdate.java 窗口滑过数组,返回每次窗口中的最大值(窗口最大值更新结构) 准备一个双端队列, 队列加数逻辑:往队列尾部加元素下标,当前加的元素要是大于等于之前的元素,之前的元素的下标弹出 队列减数逻辑:窗口形成之后随着左边界移动从队列中移除对应下标,队列头为当前窗口最大值 窗口结构的应用(进阶2) cn.hn.advanced.AllLessNumSubArray用于维持一个窗口中的最值 求一个数组子数组的最大最小差值&lt;=num的个数 一个子数组如果满足条件,那么这个子数组的所有子数组满足条件 一个子数组如果不满足条件,那么包含这个子数组的数组一定不满足条件 准备一个窗口最大值更新结构,一个窗口最小值更新结构 L从0位置开始,R一直往右扩大,直到窗口中的(最大值-最小值)&gt;num停,假设这个时候在X位置,那么以0位置开头的满足条件的子数组个数为(X+1)个 L移动到1位置,R继续往右,直到不满足条件,重复上述步骤 时间复杂度就是遍历整个数组,O(N) BFPRT算法(进阶2) cn.hn.basic.BFPRT.java在一个无序数组中找到第k大的数字,时间复杂度O(n) 解法1: 使用partition函数,时间复杂度长期期望O(N),取决于分组情况,也就是partition中的pivot值选取 解法2:主要是保证了pivot值的有效 将数组分成五个数一组(余数一组),每组取中位数,组成的数组再分组取中位数直到最后返回一个数(O(1)+O(N)+O(N)+T(N/5)) 假设一次分组取中位数的过程为T(N),则第二次为T(N/5) 根据这个数字将数组分成左中右,判断k在不在中间(O(N)) 根据判断选择左边或者右边继续以上步骤(T(7N/10)) 时间复杂度为:T(N) = T(N/5)+T(7N/10)+O(N)12345678910111213//在begin和end范围上求,数组中第k小的数public static int select(int[] arr, int begin, int end, int i) &#123; if (begin == end) return arr[begin]; int pivot = medianOfMedians(arr, begin, end);//求中位数的中位数 int[] pivotRange = partition(arr, begin, end, pivot);//使用partition函数求中间的范围 if (i &gt;= pivotRange[0] &amp;&amp; i &lt;= pivotRange[1]) &#123;//是否命中 return arr[i]; &#125; else if (i &lt; pivotRange[0]) &#123;//如果没命中,选一个方向继续找 return select(arr, begin, pivotRange[0] - 1, i); &#125; else &#123; return select(arr, pivotRange[1] + 1, end, i); &#125;&#125; KMP(进阶1) cn.hn.algoriththm.KMP.java用来求解一个大字符串str1中是否包含小字符串str2 相关概念 next数组:保存的是,字符串中,每一个字符之前的字符串中,最大的前缀和后缀匹配的长度.比如abcdabce,字符e的最长前缀和最长后缀的匹配长度为3,当前缀长度取3,后缀长度取3,这时前后缀都是abc,所以定义字符e的最长前缀和最长后缀的匹配长度为3,也就是next数组中,字符e所在位置的值为3. 应用的时候求的是短字符串的next数组,数组第一个元素为-1,第二个为0. next数组的求解方式 从左到右求解,根据之前一个字符的结果加速计算 求字符a的前缀长度时,根据b的前缀长度,只需判断x位置是否等于字符b 如果不相等,继续尝试字符c的前缀12345678910111213141516171819public int[] getNext(String str) &#123; char[] chars = str.toCharArray(); int[] next = new int[chars.length]; next[0] = -1; next[1] = 0; int pCur = 2;//指向要求最长前后缀匹配长度的字符 int pn = 0;//当字符不匹配是用来左移的指针 while (pCur &lt; next.length) &#123; //当前字符的前一个字符是否和之前最长前缀的后一个字符相等 if (chars[pCur - 1] == chars[pn]) &#123; next[pCur++] = ++pn; &#125; else if (pn &gt; 0) &#123; //将pn移到之前一个最长前缀 pn = next[pn]; &#125; else &#123; //pn已经指向第一个字符了还是不匹配 next[pCur++] = 0; &#125; &#125; return next;&#125; 加速原理 str1的i到x-1位置和str2的0到y-1位置都匹配,但是到了下一个位置就不匹配了 此时根据next数组可以得知str2中y位置的最长前缀和最长后缀匹配长度length 根据length,将str2的最长前缀,移动到最长后缀的位置(codeing的时候通过左移str2中的指针实现),从z位置继续尝试匹配 如果length为0,则从str1中x位置的下一个位置和str2的0位置从新开始尝试匹配 为什么流程正确 假设str1中从k开始能和str2匹配上, 但是因为str1中,从j开始已经是最长后缀,所以1中的假设不成立123456789101112131415161718public int isContain(String str1, String str2) &#123; char[] chars1 = str1.toCharArray(); char[] chars2 = str2.toCharArray(); int i1 = 0; int i2 = 0; int[] next = getNext(str2);//获取next数组 while (i1 &lt; chars1.length &amp;&amp; i2 &lt; chars2.length) &#123; if (chars1[i1] == chars2[i2]) &#123;//字符匹配,指针一起右移 i1++; i2++; &#125; else if (next[i2] == -1) &#123;//字符没配上,并且str2中的指针已经来到0位置,还是配不上,str1指针右移,从新开始尝试匹配 i1++; &#125; else &#123;//根据前缀长度,左移str2的指针,继续尝试 i2 = next[i2]; &#125; &#125; return i2 == next.length ? i1 - i2 : -1;&#125; KMP算法应用 cn.hn.basic.KMP_ShortestHaveTwice.java有一个原始串abcdeab,在它的后面加上自己的子串,是的得到的新字符串中包含两个原始串,且开头位置不一样,求最短的那个加法,这里的答案是abcdeabcdeab 求原始串的next数组,多求一位 根据next数组最后一位求解,只要在后面加上除了最长前缀的部分即可 cn.hn.basic.KMP_T1SubtreeEqualsT2.java求一棵树是否是另一棵树的子树 将两棵树序列化,然后用KMP算法计算 Manacher算法(进阶1) 求解一个字符串中的最大回文子串长度 为了偶回文和奇回文都能用,要加# 回文半径数组 最右回文右边界R,此时的中心为C 可能性 当前字符不在回文右边界R里,暴力破解 当前字符在回文右边界R内 当前字符位置i关于C的对称点i’的回文半径在L内,那么i的回文半径等于i’的回文半径 要是回文半径在L外头,那么i的回文半径就是R 要是回文半径在L上,i的回文半径需要尝试 manacher应用在一个字符串后面添加字符,让它变成回文,求最短 就是求每一个字符的回文半径,当有一个字符的回文半径R达到最右边的时候,此时的L到开头位置就是要加的字符]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[左程云算法课基础班]]></title>
    <url>%2F2018%2F07%2F08%2F%E5%B7%A6%E7%A8%8B%E4%BA%91%E7%AE%97%E6%B3%95%E8%AF%BE%E5%9F%BA%E7%A1%80%E7%8F%AD%2F</url>
    <content type="text"><![CDATA[左程云算法课程基础班笔记. 经典算法打印一个序列的所有子序列 对于每一个元素做两种选择进行递归 打印 不打印 下标到最后以后,进行输出12345678910111213//打印所有的子序列 public static void printAllSubSeq(char[] arr, int index, String res) &#123; //终止条件:下标到头,打印 if (index == arr.length) &#123; System.out.println(res); return; &#125; //两种选择,加上当前字符,往下走 printAllSubSeq(arr, index + 1, res + arr[index]); //不加当字符 printAllSubSeq(arr, index + 1, res); &#125; 打印数组全排列12345678910111213141516//打印数组的全排列 //尝试start位置开始的子数组的全排列 public static void printAllFullSeq(char[] arr, int start) &#123; //终止条件 if (arr.length == start) &#123; System.out.println(arr); return; &#125; //需要闯进去原数组的拷贝,不然交换之后,原数组无法遍历到 char[] arr2 = Arrays.copyOf(arr, arr.length); //将start之后每一个位置的数,放到start位置上,然后尝试start位置之后的数组的全排列 for (int i = start; i &lt; arr.length; i++) &#123; swap(arr2, i, start); printAllFullSeq(arr2, start + 1); &#125; &#125; 奶牛问题 cn/hn/basic/Cow.java 一开始有N只牛,牛不会死,每只牛从生下来第三年之后开始生小牛,问n年之后有多少只牛 f(n) = f(n-1) + f(n-3)123456789101112//返回year年之后的牛public static int howManyCows(int num, int years) &#123; if (years &lt; 1)&#123; return 0; &#125; if (years &lt; 4) &#123; //一开始的N只牛都可以生小牛 return num * years; &#125; //第n年的牛等于去年的牛,加上三年前的牛 return howManyCows(num, years - 1) + howManyCows(num, years - 3);&#125; 子数组的和能否为aim给定一个数组,数组中的数任意挑选,能否累加出aim值 递归过程,每来到一个位置,选择加上当前值,或者加上0 终止条件是来到最后的位置加完123456public boolean isSum(int[] arr, int aim, int index, int sum) &#123; if (index == arr.length) return aim == sum; //选择加上当前值或者不加 return isSum(arr, aim, index + 1, sum + 0) || isSum(arr, aim, index + 1, sum + arr[index]);&#125; 子数组的和能否为aim改dp因为每一个index和sum可以唯一确定一个返回值,所以可以改成dp 获得矩阵中的最短路径和(递归改dp) 子过程就是当前位置的值加上,当前位置选择的向右走还是向下走的路径的和. 1234567891011121314151617//返回从(x,y)位置开始到右下角的最短路径和 public static int getMinSum(int[][] arr, int x, int y) &#123; //位置来到了右下角 if (x == arr[0].length - 1 &amp;&amp; y == arr.length - 1) return arr[x][y]; //位置处于最右边,只能往下 if (x == arr[0].length - 1) return arr[x][y] + getMinSum(arr, x, y + 1); //位置处于最下边,只能往右 if (y == arr.length - 1) return arr[x][y] + getMinSum(arr, x + 1, y); //位置处于中间,可能往下也可能往右 int right = getMinSum(arr, x + 1, y); int down = getMinSum(arr, x, y + 1); //选择往下走和往右走短的那个 return arr[x][y] + Math.min(right, down); &#125; 改成dp就是将重复计算的值缓存因为根据参数(x,y)能够唯一确定返回值,也就是无后效型,因此可以改dp 12345678910111213141516171819//通过二维转换数组,改成dp public static int getMinPathDP(int[][] matrix) &#123; int row = matrix.length - 1; int col = matrix[0].length - 1; int[][] dp = matrix; //dp数组中最后一行上的值等于matrix数组中该位置的值加上dp数组中该位置右边的值 for (int x = col - 1; x &gt;= 0; x--) dp[row][x] = matrix[row][x] + dp[row][x + 1]; //dp数组中最后一列的值等于matrix数组中该位置的值加上dp数组中该位置下方的值 for (int y = row -1 ; y &gt;= 0; y--) dp[y][col] = matrix[y][col] + dp[y + 1][col]; //dp数组中中间位置的值等于matrix数组中该位置的值加上dp数组中该位置下方和右边的值 for (int x = col - 1;x &gt;= 0;x --) &#123; for (int y = row - 1;y &gt;= 0; y--) &#123; dp[x][y] = matrix[x][y] + Math.min(dp[x + 1][y], dp[x][y + 1]); &#125; &#125; return dp[0][0]; &#125; 汉诺塔问题小的只能压在大的上面,一开始N层都在from上,最后都移动到to上(N层,一开始都在from上),递归子过程如下: 将1到N-1层从from移到help上 将第N层从from移到to上 将1到N-1层从help移到to上123456789public void process(int n,String from, String to, String help)&#123; if(n == 1)&#123; sout("move" + 1 + " from " + from + " to " + to); &#125;else&#123; process(n-1, from, help, to); sout("move" + n + " from " + from + " to " + to); process(n-1, help, to, from); &#125;&#125; 递归和动态规划 可以改成动态规划的递归要满足无后效性,就是子过程的结果不受之前决策的影响 排时间表 cn/hn/basic/BestArrange.java 贪心策略:按照结束时间早来选择项目 按照结束时间排序,结束早的排在前面 遍历数组,如果当前时间小于项目开始时间,开始项目 计数加一,当前时间改成该项目结束时间 求最大项目收益 cn/hn/basic/IPO.java (贪心策略) k个项目,每一个项目有花费和收益,有初始启动基金,求能做的项目做完之后的收益 将所有的项目,根据项目的花费建一个小根堆 循环k次, 每次,从小根堆中取出所有花费比启动资金小的项目(如果有的话),然后按照收益大的加入大根堆 只要大根堆不为空就一直做, 分金条哈夫曼编码 cn/hn/basic/LessMoney.java (贪心策略) 用所有的数构建一个小根堆,每次取出最小的两个加起来,放回堆,再次调整成小根堆,重复,将每次的和累加起来就是最小代价 12345678910111213141516public static int lessMoney(int[] arr) &#123; //优先级队列是基于堆实现,大小取决于Comparator PriorityQueue&lt;Integer&gt; pQ = new PriorityQueue&lt;&gt;(); for (int i = 0; i &lt; arr.length; i++) &#123; pQ.add(arr[i]); &#125; int sum = 0; int cur = 0; //队列不空则取出最小的两个相加,然后在放回堆中 while (pQ.size() &gt; 1) &#123; cur = pQ.poll() + pQ.poll(); sum += cur; pQ.add(cur); &#125; return sum;&#125; 证明贪心策略(实际笔试或者面试的时候不用纠结,用对数器来验证) 证明比较器有传递性 字典序 当长度相等时候,从高位到低位依次比较如”abc”先于”bac”,”abc”先于”abd” 长度不同的时候,先将短的补0(或者ASCII值最小的字符),然后比较如”ab”和”abc”比较,先将”ab”补位成”ab0”,”ab0”先于”abc” 拼接字符串字典序最小比如将”b”,”ba”拼接,返回字典序最小的,应该是”bab”,而不是”bba”错误的思路:并不是将所有字符串排序之后,按照从小到大拼起来 排序策略如果s1+s2 &lt;= s2+s1,那么s1放在前面,按照这个创建比较器Comparator 排序拼接 证明 123456Arrays.sort(strs, new Comparator&lt;String&gt;() &#123; @Override public int compare(String s1, String s2) &#123; return (s1 + s2).compareTo(s2 + s1); &#125;&#125;); 前缀树 cn/hn/basic/TrieTree.java 往树中插入字符串 可以查询树中有没有插入过某个字符串 可以查询插入过几次字符串 可以查询有没有以某个字符串开头的字符串,或者结尾的字符串1234567891011121314151617181920//前缀树的节点结构public static class TrieNode &#123; public int path; //表示当前节点被经过几次 public int end; //表示有多少个字符串是以当前节点结尾的 //代表一种选择,这里可以是某一个字符,经典实现就只将选择放在边上,放在节点上实现繁琐 //如果其中一个元素指向null,说明这种选择不存在 public TrieNode[] nexts;// public HashMap&lt;Character,TrieNode&gt; nexts; //也可以用map实现 public TrieNode() &#123; path = 0; end = 0; nexts = new TrieNode[26];//以26个字母为例 &#125;&#125;public class Trie&#123; private TrieNode root;//root节点上path和end为0 public Trie()&#123;root = new TireNode();&#125; //每插入一个字符创,这个分支上的TrieNode的path和end就更新一次 pubilc void insert(String word)&#123;&#125;&#125; 并查集 cn/hn/basic/UnionFind.java 判断两个集合是不是同一集合(这里的集合结构是集合中的点最终指向一个点,这个点是指向自己的,叫集合代表),就是判断集合代表是否相同 合并两个集合,就是将数量少的集合的集合代表,挂在数量多的集合的集合代表上 优化并查集:就是每次查找一个点属于哪个集合代表的过程中,把查找路径上的点都直接改成指向集合代表,即扁平化,优化下一次的查找效率,当查找次数多了之后,查找效率就会逼近O(1) 可以用来并行解决矩阵中岛的个数的问题(用来分块计算连成一片的问题特别合适,有效避免重复计算) 矩阵中岛的数量 不用并行处理的解法:遍历,碰到1后,count++,之后进入递归,将当前的1和相邻的所有1都变成2,12345678910111213141516171819202122232425262728public static int countIslands(int[][] m) &#123; if (m == null || m[0] == null) &#123; return 0; &#125; int N = m.length; int M = m[0].length; int res = 0; for (int i = 0; i &lt; N; i++) &#123; for (int j = 0; j &lt; M; j++) &#123; if (m[i][j] == 1) &#123; res++; infect(m, i, j, N, M); &#125; &#125; &#125; return res;&#125;public static void infect(int[][] m, int i, int j, int N, int M) &#123; if (i &lt; 0 || i &gt;= N || j &lt; 0 || j &gt;= M || m[i][j] != 1) &#123; return; &#125; m[i][j] = 2; infect(m, i + 1, j, N, M); infect(m, i - 1, j, N, M); infect(m, i, j + 1, N, M); infect(m, i, j - 1, N, M);&#125; 一致性哈希 经典服务器抗压处理 后台有m台机器,前端请求输入参数key,查询对应的value, 存(key,value)的时候计算key哈希值,然后模m,决定放在那一台后台机器上, 请求的时候用同样的hash函数计算哈希值,向对应的机器请求 缺点就是和hashMap扩容一样,增加后台的机器的时候需要重新计算所有的hash值 优化的分配结构 将hash域做成一个环, 将m台机器的IP(也可以是其他能代表机器的)经过hash之后分配到环上 存key的时候,计算key的hash值,将(key,value)存到环上第一个大于等于该hash值的机器上 扩容的时候只要将扩容机器hash值大的后一个机器上的数据迁移部分过来就行,扩容代价小 问题是分配机器的时候环上的hash域不能达到负载均衡 虚拟节点 每一台机器分配1000个虚拟节点,hash之后分配到环上,这样就基本分布均衡了 大文件处理比如有一个100T的文件,存储在分布式存储系统上,文件内容就是每一行一个字符串,现在给你1000台机器,要求打印文件中重复的行 读取每一行,算出hash值,然后模1000,分配到0~999中的一台机器上,重复的行会在同一台机器上 黑名单查询(布隆过滤器)里有100亿条url,需要判断一条url是否在黑名单里 传统解法就是多几台机器处理 使用布隆过滤器,和多个hash函数, 原来640G的空间,转化成布隆过滤器存储(失误率0.0001,样本量100亿,其中一个样本对应一个bit),只需要16G(左程云版)10亿字节约为1G,100亿条url,每条url经过hash函数处理映射成8byte的数据,而哈希表的存储效率为50%,所以100亿条的大小约为:100亿*8byte*2=160G使用布隆过滤器,假设一个样本占一位,存储效率100%,100亿*1bit/8=1.25G,布隆过滤器有一定的误杀率,但是黑名单中的url一定会被过滤 设计一种结构可以不重复保存key值,并且可以随机返回key值,时间复杂度为O(1) 使用两个HashMap,一个保存key:index,另一个保存index:key, 每次取值的时候根据map的size来random出随机的index 每次删除的时候拿map中的最后一个来填补删除的位置,保证index的连续性 制造哈哈函数 通过一个现有的hash’函数得到hash值,然后设高八位为a,低八位为b,那么a+b##i为新的独立hash值,其中i为系数 求完全二叉树的节点个数时间复杂度小于O(N) tip:满二叉树的节点个数为2^h-1 (其中h为二叉树高度) 求二叉树左边界的高度 右子树的高度有没有到最后一层,到了左子树就是满的 没到的话右子树就是满的 递归求另外半边子树的节点个数 判断二叉搜索树(进阶6)(binary search tree)(任何一个节点的左子树都小,右子树都大) 中序遍历递增的数 中序遍历非递归版本中,在输出值的地方判断是不是比上一次的值大 判断完全二叉树(进阶6)(complete binary tree)(就是每一层都怼满) 二叉树按层遍历,如果一个节点有右节点,但是没有左节点,直接返回false 一个节点只有左节点,或者左右节点都没有,那么该节点后面的节点都要没有左右节点 判断平衡二叉树(balance binary tree)(树中任意一个子树的左右子树高度差不超过1)(树形DP) 递归函数,参数为当前节点 定义一个类ResultData,属性为是否平衡isBalance和高度height 判断节点是否为空,为空返回new ResultData(true,0) 判断当前节点的左节点返回的结果是不是平衡,不是直接返回结果 判断当前节点的右节点返回的结果是不是平衡,不是直接返回结果 判断当前节点左右节点返回结果的高度差是否满足,满足则返回左右结果中最大高度+1作为高度 二叉树的序列化和反序列化 序列化 先序遍历,用_分割,用#作为空占位符 反序列化 先用_将字符串分割成队列 用递归实现的前序遍历重新构建树 按层打印二叉树 使用队列存每一层从左到右的节点,只要队列不空,就一直打印 寻找二叉树后继节点(也可以想一下前驱节点),所谓后继节点就是中序遍历中的后一个节点 判断给定节点是否有右子树,有的话后继节点就是右子树的最左节点 没有右子树的话,判断给定节点是否为其父节点的左节点,是的话父节点就是后继节点;不是的话一直往上找,直到某个节点是其父节点的左节点,该父节点就是后继节点 二叉树的遍历先序,中序,后序遍历,递归和非递归方式(TreeNode.java) 递归方式就是打印语句放在第一行,中间,还是最后 非递归方式 先序遍历 准备一个栈,push head节点,然后循环(弹出最上面的,打印,每次按照先右子节点,后左子节点的顺序压入栈) 中序遍历 准备一个栈,如果当前节点不为空或者栈不为空, 一直压入左节点, 如果当前节点为空,从栈中弹出节点打印,并将当前节点指向打印节点的右节点 后序遍历 准备一个栈1,push head节点,然后循环(弹出最上面的,存入栈2,然后按照先左节点,后右节点入栈) 最后依次弹出栈2的节点 反转单向链表之字形打印思路: 指针a往右走,到底之后往下 指针b往下走,到底之后往右 按照ab之间的连线打印数字 判断链表有没有环并返回入环节点(IsRingNodeList.java) 解法一: 遍历链表,使用Set保存出现过的节点,然后返回重复出现的 时间N,空间N 解法二: 快指针一次两步,慢指针一次一步,在环里相遇后快指针回到headNode, 快指针,慢指针同时一次走一步前进,相遇的地方就是入环节点 时间N,空间1 判断两条单向链表相交返回第一个相交的节点(IsNodeListIntersect.java) 一条有环,一条无环,不可能相交 两条无环的链表 遍历两条链表,记录长度和最后一个节点 判断最后一个节点是否是同一个节点,不是的话不可能相交 计算长度差,假设是d,长的链表先从头走d步,然后两个一起遍历,第一个相等的节点就是相交 两个都有环 两个不相交 分别找到入环节点,其中一个入环节点开始遍历,与另一个入环节点比较,要是回到入环节点的时候还没有相等,说明不相交 拓扑2 入环节点相等,然后以入环节点为终点,相当于找无环链表相交的节点 拓扑3 入环节点不相等,但是遍历过程中找到另一个入环节点,返回任意一个入环节点 tip计算mid 使用L+(R-L)/2比(L+R)/2安全,因为不用担心下标越界,也可以写成位运算L+(R-L)&gt;&gt;1 为什么:在计算不能整除的负数下标和的时候可能会越界,不过一般用不到 递归函数复杂度计算master公式 能够匹配公式:T(N) = a*T(N/b) + O(N^d)的都能按照以下式子估算: loga(b) &gt; d -&gt; O(N^loga(b)) loga(b) = d -&gt; O(N^d * logN) loga(b) &lt; d -&gt; O(N^d) 比较器继承Comparator&lt;Student&gt;接口,实现compare(Student s1,Student s2)方法 返回负数:第一个参数放前面 返回正数:第一个参数放后面 返回0:相等 系统提供的堆1. PriorityQueue(实质上是一个堆结构),构造的时候传入比较器 非基于比较的排序1. 桶排序 2. 计数排序 3. 基数排序 各种排序算法以及优缺点 参考地址:https://blog.csdn.net/speedme/article/details/23021467 工程上的综合排序 普通类型(int,double,short等):快速排序 自定义的类:归并排序 长度很小的数组(小于60?):插入排序 因为常数项小 O(N^2)的三个排序冒泡排序 bubbleSort(与数据状况无关),可以稳定 * 每一趟遍历将最大的或者最小的交换到一边,下一次就遍历就不经过它了,时间复杂度`O(N^2)`,额外空间复杂度`O(1)` 插入排序 insertionSort(和选择排序的区别就在于,当数组有序的时候遍历一次就行了,也就是和数据状况有关,可以稳定) * 从第二个数字开始到最后一个数字,每次将这个数字与之前的数字作比较,大(或者小)就和前面的数字交换,直到不符合条件,时间复杂度最好`O(n)`,最差`O(n^2)`,空间`O(1)` * 希尔排序是插入排序的优化,采用分组插入 参考地址:https://www.cnblogs.com/chengxiao/p/6104371.html 选择排序 selectionSort(工程上几乎不用,与数据状况无关,不稳定) * 每一次遍历选择最大或者最小的数,和这一次遍历开头的数交换,时间复杂度`O(n^2)`,空间复杂度`O(1)` O(N*logN)的三个排序归并排序 mergeSort (可以稳定) 递归,将数组分成两部分,分别排好序然后在合并(通过外排),时间复杂度O(N*logN),额外空间复杂度O(n) 算法步骤: 归并主函数:void mergeSort(int[] arr),调用mergeProcessor(arr,0,arr.length-1) 递归调用函数:void mergeProcessor(int[] arr,int L, int R),当L==R的时候返回,计算mid,调用mergeProcessor(arr, L, mid),mergeProcesser(arr, mid + 1, R),调用merge(arr, L, mid, R), 合并函数:void merge(int[] arr, int L, int mid, int R),新建int helper[R-L+1](可以生成一个全局的,就不用一直生成销毁),将两部分数组按顺序填入 快速排序 quickSort (不稳定) 快速排序的主要思想是,在数组中随机找到一个数字作为基数,然后将大于这个数的放到它一边,小于它的数放在另外一边,然后左右再次进行相同的操作,直到排序完成.因为等于基数的元素也会被移动位置,所以快排是不稳定的. 经典快排的缺点:总是拿最后一个数去作为pivot,这样就会和数据状况很有关,最坏的情况就是有序数列排序的话时间复杂度就是O(N^2) 时间复杂度O(N*logN),额外空间复杂度O(logN) 堆排序 heepSort (堆就是完全二叉树,不稳定) 满二叉树:就是最后一层的叶节点全部补齐(是特殊的完全二叉树) 完全二叉树:如果最后一层没有全部补齐,那么需要从左到右依次补齐. 数组转换成完全二叉树: 当前节点的叶节点:左:2*i+1,右:2*i+2,越界则说明没有 当前节点的父节点:(i-1)/2 大根堆:每一棵子树的最大值都是这课子树的头部 将一个数组排成大根堆(heapInsert)的时间复杂度:O(N) = log1 + log2 + log3 + ... + log(N-1) heapIfY:如果大根堆中有一个数字变小了,将对重新调整成大根堆 排序问题的补充: 归并排序的额外空间复杂度可以变成O(1),但是很难,”归并排序 内部缓存法” 快排可以做到稳定,但是很难,”01 stable sort” 题目:一个数组,奇数放左边,偶数放右边,要求原始相对次序不变,时间复杂度O(N),额外空间复杂度O(1) 常见算法题小和问题 (LittleSum.java) 在一个数组中,每一个数左边比这个数小的数累加起来,叫做这个数组的小和,求一个数组的小和. 思路:这个过程就相当于找每一个数右边有几个数比当前数大,然后将当前数x上个数,然后累加,这个过程可以在归并排序的过程中进行计算在merge的过程中,在左边数组和右边数组比较的同时计算小和,只要将左边数组比右边小的数字直接乘上右边数组中比他大的数字的个数 为什么能够加速:因为每次左右数组比较的时候,当左边的数字比右边小的时候,就可以根据右边数组的下标直接得到右边数组有几个比左边数组这个数大 逆序对问题 类似于小和问题,在归并的过程中直接就可以将逆序对输出 荷兰国旗问题 (HollandNationFlag.java) 给定一个数组arr,一个数num,将小于,等于,大于num的数字放在左中右,要求额外空间复杂度O(1)(意思就是不用辅助数组),时间复杂度O(N)(遍历数组的次数固定) 思路:就是快排的partition函数改一下,smaller从左往右,bigger从右往左,因为还有一个==要比较,相当于小于区域推着等于区域往前走,这个修改后的partition函数可以用来优化之前的快快排 找中位数 有一个流一直在输出数,可以用一个数组接受,然后随时取得数组的中位数 思路:用一个大根堆和一个小根堆,大根堆存放较小的一半数字,小根底存放较大的一半数字,每次接收数字后放到数字较少的数组,然后调整堆,那么堆顶的数字有一个是中位数. 找最大差值 (MaxGap.java) 给定一个数组,求排序之后的相邻两个数字之间的差值的最大值,要求时间复杂度O(N): 思路:由于时间复杂度要求,所以不能用普通的排序,可以使用桶排序 按照数组中最大值和最小值的差值确定每个桶的存放数字范围,最大值放在len位置,最小值放在0位置,所以桶的数量为len+1,必有一个空桶 index = (int)((num - min) * len / (max - min))确定一个数应该放在哪里 每个同种只存放这个桶的最大值最小值以及同种是否有数, 遍历桶,求当前桶的最小值和上一个非空桶的最大值的差值,(前提是有非空桶,不然每个桶中最大最小的差值也要加入比较) 两个队列实现栈(StackByQueue.java) data队列和help队列 需要pop的时候先将data队列除了最后一个数的其他数字poll到help队列中,返回data中最后一个数 交换data和help的引用 两个栈实现队列 (QueueByStack.java) push栈和pop栈 只要pop栈中有元素,就不能从push栈中倒元素到pop 倒元素要一次全部倒完 判断回文链表 (IsPalindrome.java)比如1221,12321 解法一: 将链表依次弹入栈,然后遍历链表同时依次弹出栈,比较,时间复杂度O(N),额外空间O(N) 解法二: 定义两个指针,一个快指针,一个慢指针,通过他们找到链表中点,记住中点和最后一个节点 将中点指向null,中点之后的节点逆序 然后比较两侧的节点,最后要还原节点指向 时间复杂度O(N),额外空间复杂度O(1) 回旋打印数组思路:先确定左上角和右下角,然后打印边框,分成4个边打印,然后缩小左上角和右下角 链表的partition (PartitionNodeList.java)给定一个链表和一个数字num,将链表中大于num的放右边,等于num的放中间,小于num的放左边 解法一: 将链表的节点放在数组中,写一个partition函数排好,再串成链表 时间复杂度O(N),额外空间O(N) 解法二: 准备三个节点,less,equal,more 遍历链表,将less指向第一个小于num的,equal指向第一个等于num的,more指向第一个大于num的 时间复杂度O(N),额外空间O(1) 再次遍历链表,相应的节点分配到三个部分 拷贝链表 (CopyRandomNodeList.java)链表的每一个节点除了包含nextNode指针信息,还有一个randomNode指针,实现一个方法,返回拷贝出来的链表的头指针 解法一: 使用一个Map&lt;Node, Node&gt;保存原节点,复制节点的信息,然后通过遍历原链表,来确定复制节点的nextNode和randomNode应该指向哪里 时间N,空间N 解法二: 将原链表的每一个节点的指向它的复制节点,复制节点指向原来的下一个节点 复制节点的random指针指向的就是原节点的random指针指向节点的下一个节点 分离链表1234567891011121314151617181920212223public Node normalWay(Node head) &#123; Map&lt;Node, Node&gt; nodeMap = new HashMap&lt;&gt;(); Node currentNode = head; while (currentNode != null) &#123;//当前节点作为key,当前节点的复制节点作为value //如果不做映射的话无法确定复制节点的指向 nodeMap.put(currentNode, new Node(currentNode.getVal())); currentNode = currentNode.getNextNode(); &#125; currentNode = head; Node copyOfNextNode;//当前节点的下一个节点 Node copyOfRandomNode;//当前节点的随机节点 Node copyOfCurrentNode;//当前节点 while (currentNode != null) &#123; //根据源节点的指向,设置复制节点的指向 copyOfNextNode = nodeMap.get(currentNode.getNextNode()); copyOfRandomNode = nodeMap.get(currentNode.getRandomNode()); copyOfCurrentNode = nodeMap.get(currentNode); copyOfCurrentNode.setNextNode(copyOfNextNode); copyOfCurrentNode.setRandomNode(copyOfRandomNode); currentNode = currentNode.getNextNode(); &#125; return nodeMap.get(head); &#125; 123456789101112131415161718192021222324252627282930public Node betterWay(Node head) &#123; Node currentNode = head; //当前节点 Node copyNode; //当前节点的拷贝节点 Node tempNode; //拷贝节点的下一个节点 while (currentNode != null) &#123; //将1 -&gt; 2 -&gt; 3 转换成 1 -&gt; 1' -&gt; 2 -&gt; 2' -&gt; 3 -&gt; 3' -&gt; null copyNode = new Node(currentNode.getVal()); tempNode = currentNode.getNextNode(); currentNode.setNextNode(copyNode); copyNode.setNextNode(tempNode); currentNode = currentNode.getNextNode(); &#125; currentNode = head; //回到头结点 //复制节点的randomNode就是当前节点的randomNode的下一个节点(即它的复制节点) while (currentNode != null) &#123; copyNode = currentNode.getNextNode(); copyNode.setRandomNode(currentNode.getRandomNode().getNextNode()); currentNode = copyNode.getNextNode(); &#125; Node ret = head.getNextNode(); //返回拷贝列表的头节点 currentNode = head; while (currentNode != null) &#123; //分离复制链表和原链表 copyNode = currentNode.getNextNode(); //当前节点的下一个节点 tempNode = copyNode.getNextNode(); //拷贝节点的下一个节点 currentNode.setNextNode(tempNode); //当前节点指向拷贝节点的下一个节点 //如果拷贝节点的下一个节点为空,说明到头了 copyNode.setNextNode(tempNode == null ? null : tempNode.getNextNode()); currentNode = tempNode; //当前节点移动到拷贝节点的下一个节点 &#125; return ret; &#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法]]></title>
    <url>%2F2018%2F07%2F07%2F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[常见面试笔试算法思路代码,一些思维题 算法堆建大根堆 方式1 当堆的大小确定,从下往上调整,时间复杂度O(N) 1234567891011121314151617181920//当前节点i的父节点是(i-1)/2//tip 从下往上,从右往左,从最后一个非叶子节点开始向下比较,尝试和儿子节点交换//时间复杂度O(N)private static void buildMaxHeap(int[] arr) &#123; for (int i = (arr.length - 1) / 2; i &gt;= 0; i--) &#123; int j = i; int sonLeft = (j * 2) + 1; while (sonLeft &lt; arr.length) &#123; int maxIndex = sonLeft + 1 &lt; arr.length &amp;&amp; arr[sonLeft + 1] &gt; arr[sonLeft] ? sonLeft + 1 : sonLeft; maxIndex = arr[maxIndex] &gt; arr[j] ? maxIndex : j; if (maxIndex == j) &#123; break; &#125; SwapUtil.swapInt(arr, j, maxIndex); j = maxIndex; sonLeft = j * 2 + 1; &#125; &#125;&#125; 方式2 当堆的大小不确定,需要一边插入节点一边调整,时间复杂度O(NlogN) 123456789101112131415//时间复杂度O(N*logN))public static void buildMaxHeap2(int[] arr) &#123; //从上往下遍历每一个节点,并不断往上比较,尝试和父节点进行交换 for (int i = 0; i &lt; arr.length; i++) &#123; maxHeapInsert(arr, i); &#125;&#125;//这种构建大根堆的方式实际上应用于堆的大小没有确定时,需要一边插入节点一边调整public static void maxHeapInsert(int[] arr, int j) &#123; while (arr[j] &gt; arr[(j - 1) / 2]) &#123; SwapUtil.swapInt(arr, j, (j - 1) / 2); j = (j - 1) / 2; &#125;&#125; 使用JDK实现的堆 12345//使用优先级队列实现大根堆PriorityQueue&lt;Integer&gt; maxHeap = new PriorityQueue&lt;&gt;((a, b) -&gt; b - a);//使用优先级队列实现小根堆PriorityQueue&lt;Integer&gt; minHeap = new PriorityQueue&lt;&gt;(Comparator.comparing(Integer::valueOf));PriorityQueue&lt;Integer&gt; maxHeap = new PriorityQueue&lt;&gt;((a, b) -&gt; a - b); 调整大根堆123456789101112131415161718192021222324252627//大根堆中从0位置开始,到end位置结束,index位置的数字变化了,重新调整大根堆private static void heapIfIndex(int[] arr, int index, int end) &#123; int father = (index - 1) / 2; if (father &gt;= 0 &amp;&amp; arr[index] &gt; arr[father]) &#123; //变大且大于父节点的情况 while (father &gt;= 0 &amp;&amp; arr[index] &gt; arr[father]) &#123; SwapUtil.swapInt(arr, index, father); index = father; father = (index - 1) / 2; &#125; &#125; else &#123; //变小或者不变 int sonLeft = index * 2 + 1; //当左儿子没有越界 while (sonLeft &lt;= end) &#123; //比较左右儿子 int maxIndex = sonLeft + 1 &lt;= end &amp;&amp; arr[sonLeft + 1] &gt; arr[sonLeft] ? sonLeft + 1 : sonLeft; //比较儿子中大的和父亲 maxIndex = arr[maxIndex] &gt; arr[index] ? maxIndex : index; if (maxIndex == index) &#123; break; &#125; SwapUtil.swapInt(arr, index, maxIndex); index = maxIndex; sonLeft = 2 * index + 1; &#125; &#125;&#125; 快排 平均时间复杂度:O(nlogn)最坏时间复杂度:O(n^2)空间复杂度:O(nlogn) partition函数12345678910111213141516public static int[] partition2(int[] arr, int start, int end) &#123; //某一状态下,smaller指针代表小于基准的数字的范围,bigger代表大于基准的数字的范围 int smaller = start - 1;//smaller代表小于的范围 int bigger = end;//bigger代表大于的范围 while (start &lt; bigger) &#123; if (arr[start] &lt; arr[end]) &#123;//当start指针发现小于基准的数字,交换到smaller指针处,向前移动smaller指针和start指针 SwapUtil.swapInt(arr, ++smaller, start++); &#125; else if (arr[start] &gt; arr[end]) &#123;//当start指针发现大于基准的数字,和bigger交换,往回移动bigger指针,因为不确定bigger指针指向的数字,所以start指针不移动,接着判断 SwapUtil.swapInt(arr, --bigger, start); &#125; else &#123;//当start指针发现相等的数字,直接移动start指针 start++; &#125; &#125; SwapUtil.swapInt(arr, bigger, end);//因为用最后一个数作比较,所以要和bigger中最小的数字交换 return new int[]&#123;smaller + 1, bigger&#125;;//返回的是最后基准的下标&#125; 递归主体12345678910public static void quickSort2(int[] arr, int start, int end) &#123; if (start &lt; end &amp;&amp; start &gt;= 0 &amp;&amp; end &lt; arr.length) &#123; SwapUtil.swapInt(arr, start + (int) (Math.random() * (end - start + 1)), end);//随机选择数组中的一个数放到数组最后作为基准 int[] pivot = partition2(arr, start, end);//返回的处理后基准两侧的下标 quickSort2(arr, start, pivot[0] - 1); quickSort2(arr, pivot[1] + 1, end); &#125; else &#123; throw new RuntimeException("illegal"); &#125;&#125; 归并排序 时间复杂度O(NlogN)额外空间复杂度O(N) 递归主体12345678910public static void mergeProcessor(int[] arr, int start, int end) &#123; if (start == end) &#123; return; &#125; int mid = (start + end) / 2; mergeProcessor(arr, start, mid); mergeProcessor(arr, mid + 1, end); merge(arr, start, mid, end);&#125; merge过程12345678910111213141516171819202122232425262728public static void merge(int[] arr, int start, int mid, int end) &#123; int[] helper = new int[end - start + 1]; int count = 0; int left = start; int right = mid + 1; //要合并的两个数组依次从第一个数开始比较,小的放在helper先放进 while (left &lt;= mid &amp;&amp; right &lt;= end) &#123; if (arr[left] &lt;= arr[right]) &#123; helper[count++] = arr[left++]; &#125; else &#123; helper[count++] = arr[right++]; &#125; &#125; //将剩下的数字放进helper数组 if (left &gt; mid) &#123; while (right &lt;= end) &#123; helper[count++] = arr[right++]; &#125; &#125; else &#123; while (left &lt;= mid) &#123; helper[count++] = arr[left++]; &#125; &#125; //将helper数组拷贝回arr数组 for (int i = 0; i &lt; helper.length; i++) &#123; arr[start + i] = helper[i]; &#125;&#125; 冒泡时间复杂度O(N^2),额外空间复杂度O(1) 插入时间复杂度最好O(n),最差O(n^2),空间O(1)和选择排序的区别就在于,当数组有序的时候遍历一次就行了,也就是和数据状况有关,可以稳定从第二个数字开始到最后一个数字,每次将这个数字与之前的数字作比较,大(或者小)就和前面的数字交换,直到不符合条件, 选择时间复杂度O(n^2),空间复杂度O(1)每一次遍历选择最大或者最小的数,和这一次遍历开头的数交换, 二分查找1234567891011121314151617public static int bSearch(int[] arr, int aim) &#123; Arrays.sort(arr); int left = 0; int right = arr.length - 1; int mid = (left + right) / 2; while (left &lt;= right) &#123; if (aim &gt; arr[mid]) &#123; left = mid + 1; &#125; else if (aim &lt; arr[mid]) &#123; right = mid - 1; &#125; else &#123; return mid; &#125; mid = (left + right) / 2; &#125; return -1;&#125; 大数据量查找给40亿个不重复的无序的unsigned int整数,判断一个数是否在这40亿个数之中. 用将近16g内存,将所有数字采用bitmap存储到大的hash表中,时间复杂度O(N),查找需要O(1) 将40亿个数用二进制表示,以最高位判断,0分一波,1分一波,然后次高位,相当于二分.(存疑,待考证) 链表删除节点单链表反转判断环找交点思维题三门问题有三个门进行抽奖,其中一个有大奖并且主持人知道是哪个,观众选择一个后,主持人从剩下的两个淘汰掉一个不是大奖的,然后问观众是否改变自己的选择,问中奖概率是佛一样 量水问题5升水和3升水量出4升水 BFS123456789101112131415161718192021public static void bfs(Node node) &#123; if (node == null) &#123; return; &#125; Queue&lt;Node&gt; queue = new LinkedList&lt;&gt;(); HashSet&lt;Node&gt; map = new HashSet&lt;&gt;(); queue.add(node); map.add(node); //队列不空,则将队列中最前面的取出进行处理,类似于按层遍历二叉树 while (!queue.isEmpty()) &#123; Node cur = queue.poll(); System.out.println(cur.value); for (Node next : cur.nexts) &#123; //排除之前搜索过的节点 if (!map.contains(next)) &#123; map.add(next); queue.add(next); &#125; &#125; &#125; &#125; DFS123456789101112131415161718192021222324public static void dfs(Node node) &#123; if (node == null) &#123; return; &#125; Stack&lt;Node&gt; stack = new Stack&lt;&gt;(); HashSet&lt;Node&gt; set = new HashSet&lt;&gt;(); stack.add(node); set.add(node); System.out.println(node.value); //只要栈不空,弹出最上面的进行处理, while (!stack.isEmpty()) &#123; Node cur = stack.pop(); for (Node next : cur.nexts) &#123; //之前搜索过的节点排除 if (!set.contains(next)) &#123; stack.push(cur); stack.push(next); set.add(next); System.out.println(next.value); break; &#125; &#125; &#125; &#125; java技巧List转换成数组12]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>思维题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开发工具及环境部署]]></title>
    <url>%2F2018%2F07%2F07%2F%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E5%8F%8A%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[这是摘要]]></content>
      <categories>
        <category>笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[计算机网络]]></title>
    <url>%2F2018%2F07%2F07%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[计算机网络相关知识 网络编程 参考地址:https://www.javazhiyin.com/12939.html cookie和sessionCookie用cookie写一个历史浏览记录 session用session实现购物记录 session的原理 设置session的有效期 浏览器禁用Cookie后servlet共享数据导致的问题:url重写session的常见应用 登录 防止表单重复提交 前台javaScript 后台表单带随机数 实现一次性校验码实现带验证码的登录 过滤器filterCrossFilter1234567891011public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException &#123; HttpServletResponse response= (HttpServletResponse) servletResponse; String origin= servletRequest.getRemoteHost()+":"+servletRequest.getRemotePort(); response.setHeader("Access-Control-Allow-Origin", "*"); response.setHeader("Access-Control-Allow-Headers", "Origin, X-Requested-With, Content-Type, Accept"); response.setHeader("Access-Control-Allow-Methods","POST,GET,OPTIONS,DELETE"); /* response.setHeader("Access-Control-Allow-Methods","POST,GET,OPTIONS,DELETE"); response.setHeader("Access-Control-Max-Age","3600"); response.setHeader("Access-Control-Allow-Credentials","true");*/ filterChain.doFilter(servletRequest,servletResponse); &#125; HtmlFilter(防止XSS攻击)参考地址:http://blog.csdn.net/qq924862077/article/details/620535771234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import javax.servlet.*;import javax.servlet.http.HttpServletResponse;@WebFilter("/HtmlFilter")public class HtmlFilter implements Filter &#123; public void doFilter(ServletRequest req, ServletResponse resp, FilterChain chain) throws IOException, ServletException &#123; HttpServletRequest request = (HttpServletRequest) req; HttpServletResponse response = (HttpServletResponse) resp; MyHtmlRequest htmlRequest = new MyHtmlRequest(request); chain.doFilter(htmlRequest, response); &#125; class MyHtmlRequest extends HttpServletRequestWrapper&#123; public MyHtmlRequest(HttpServletRequest request) &#123; super(request); &#125; @Override public String getParameter(String name) &#123; String value = super.getParameter(name); if(value == null) return null; return filter(value); &#125; public String filter(String message) &#123; if (message == null) return (null); char content[] = new char[message.length()]; message.getChars(0, message.length(), content, 0); StringBuilder result = new StringBuilder(content.length + 50); for (int i = 0; i &lt; content.length; i++) &#123; switch (content[i]) &#123; case '&lt;': result.append("&amp;lt;"); break; case '&gt;': result.append("&amp;gt;"); break; case '&amp;': result.append("&amp;amp;"); break; case '"': result.append("&amp;quot;"); break; default: result.append(content[i]); &#125; &#125; return (result.toString()); &#125; &#125;&#125; CharactorFilter12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import javax.servlet.*;import javax.servlet.http.HttpServletResponse;/** * Servlet Filter implementation class CharacterEncodingFilter */@WebFilter("/CharacterEncodingFilter")public class CharacterEncodingFilter implements Filter &#123; private String charset ; private String defaultCharset = "UTF-8"; public void doFilter(ServletRequest req, ServletResponse resp, FilterChain chain) throws IOException, ServletException &#123; HttpServletRequest request = (HttpServletRequest) req; HttpServletResponse response = (HttpServletResponse) resp; request.setCharacterEncoding(charset); response.setCharacterEncoding(charset); response.setContentType("text/html;charset=" + charset); request.setAttribute("charset", charset); chain.doFilter(new MyRequest(request), response); &#125; public void init(FilterConfig fConfig) throws ServletException &#123; String charset = fConfig.getInitParameter("charset"); if(charset == null)&#123; charset = defaultCharset; &#125; this.charset = charset; &#125;&#125;class MyRequest extends HttpServletRequestWrapper&#123; private HttpServletRequest request; public MyRequest(HttpServletRequest request) &#123; super(request); this.request = request; &#125; @Override public String getParameter(String name) &#123; String value = request.getParameter(name); if(value == null) return null; if(!request.getMethod().equals("get"))&#123; return value; &#125; try &#123; value = new String(value.getBytes("ios8859-1"),request.getParameter("charset")); &#125; catch (UnsupportedEncodingException e) &#123; throw new RuntimeException(e); &#125; return value; &#125;&#125;]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[脚本及Linux知识]]></title>
    <url>%2F2018%2F07%2F07%2F%E8%84%9A%E6%9C%AC%E5%8F%8ALinux%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[这是摘要 shell脚本wget使用 从有数字规律的网址抓取网页并保存在当前目录？假设网址为 http://test/0.xml，其中这个数字可以递增到100。(网易笔试题)1234for((i=0;i&lt;100;++i));dowget http://test/$i.xml; done shell加法 source命令 替换文本文件 换行符格式 管道那么，一种思路就是把你tail输出的东西再做一次包装处理，这个很符合linux管道处理的思想。以高亮Log中的ERROR为例，你可以这样：tail -f xxx.log | perl -pe &#39;s/(ERROR)/\e[1;31m$1\e[0m/g&#39;其中，xxx.log是你要跟踪的文件。这里假设了你的Linux的PATH中有perl。perl在这里干的事情，就是通过命令行的方式进行动态的替换ERROR字符串的操作，替换过程中，主要使用了Linux的console_codes的语法结构。（具体关于console_codes的细节，可以通过man console_codes进行了解）这里，\e主要进行转移说明。 正则表达式匹配数字 匹配1~1000^([1-9][0-9]{0,4}|1000)$ centos常用命令查找文件whereis：whereis命令用来定位指令的二进制程序、源代码文件和man手册页等相关文件的路径。whereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。 locate：locate命令和slocate命令都用来查找文件或目录。locate命令其实是find -name的另一种写法，但是要比后者快得多，原因在于它不搜索具体目录，而是搜索一个数据库/var/lib/locatedb，这个数据库中含有本地所有文件信息。Linux系统自动创建这个数据库，并且每天自动更新一次，所以使用locate命令查不到最新变动过的文件。为了避免这种情况，可以在使用locate之前，先使用updatedb命令，手动更新数据库。 which：which命令用于查找并显示给定命令的绝对路径，环境变量PATH中保存了查找命令时需要遍历的目录。which指令会在环境变量$PATH设置的目录里查找符合条件的文件。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。 type：type命令用来显示指定命令的类型，判断给出的指令是内部指令还是外部指令。命令类型：1234567891011alias：别名。keyword：关键字，Shell保留字。function：函数，Shell函数。builtin：内建命令，Shell内建命令。file：文件，磁盘文件，外部命令。unfound：没有找到。-t：输出“file”、“alias”或者“builtin”，分别表示给定的指令为“外部指令”、“命令别名”或者“内部指令”；-p：如果给出的指令为外部指令，则显示其绝对路径；-a：在环境变量“PATH”指定的路径中，显示给定指令的信息，包括命令别名。[root@localhost ~]# type date date is /bin/date [root@localhost ~]# type mysql mysql is /usr/bin/mysql因为显示的是路径，可以理解为找到了这个文件（个人理解）。 find：参考地址:https://www.cnblogs.com/ay-a/p/8017419.htmlfind命令用来在指定目录下查找文件。任何位于参数之前的字符串都将被视为欲查找的目录名。如果使用该命令时，不设置任何参数，则find命令将在当前目录下查找子目录与文件。并且将查找到的子目录和文件全部进行显示。find -name &quot;dxt-core&quot; -type dfind基本用法12345678910111213141516171819find 如不加任何参数，表示查找当前路径下的所有文件和目录find -print 将结果打印到标准输出find /data/log 指定路劲查找find / -name "abc.txt" 在系统中查找 abc.txt 如果执行完毕没有找到，则说明系统中不存在该文件find 还支持正则表达式查找find /data/logs -mame "*.log" -type f -printf 查找符合指定字符串的文件find . -name "[0-9]" -type f 查找以数字开头的文件find / -mtime -1 |head 查找系统内最近24小时修改过的文件find / -mmin -15|head 查找系统内最近15 分钟修改过的文件find 使用 type 选项可以查找特定的文件类型，常见类型如下 b 块设备文件 d 目录 c 字符设备文件 p 管道文件 l 符号链接文件 f 普通文件find . -type d 查找当前路径中的所有目录find . -type f 查找当前路径中的所有文件find . -type l 查找当前路径中的所有符号链接文件 查看内存使用free -mh 查看硬盘使用可以查看一级文件夹大小、使用比例、档案系统及其挂入点，但对文件却无能为力df -h 查看文件夹大小可以查看文件及文件夹的大小du -hdu -h --max-depth 1 bin/Mdroid 强制杀死进程kill -s 9 PID 修改文件权限可执行文件chmod 755 filename1234567sudo chmod -（代表类型）×××（所有者）×××（组用户）×××（其他用户）rwx-rwx-rwxsudo chmod 600 ××× （只有所有者有读和写的权限）sudo chmod 644 ××× （所有者有读和写的权限，组用户只有读的权限）sudo chmod 700 ××× （只有所有者有读和写以及执行的权限）sudo chmod 666 ××× （每个人都有读和写的权限）sudo chmod 777 ××× （每个人都有读和写以及执行的权限） grep命令获取本机内网ipifconfig eth0|grep &quot;inet addr&quot;|awk &#39;{print $2}&#39;|awk -F : &#39;{print $2}&#39;参考地址:https://www.cnblogs.com/kerrycode/archive/2015/06/16/4581030.html 常用参数123456789-d skip 忽略子目录-l pattern files 查询多文件时只列出匹配的文件名-v 显示不包括匹配文本的所有行-c 只输出匹配行的计数-i 不区分大小写(只适用于单字符)-h 查询多文件时不显示文件名-n 显示匹配行及行号-s 不显示不存在或无匹配文本的错误信息-v 显示不包括匹配文件的所有行 awk命令命令查找日志 查找访问日志如 tcp 0 0 127.0.0.1:8652 127.0.0.1:40192 TIME_WAIT格式文件,IP最多的前五个IPawk ‘{print $5}’ apache.log|cut -d:-f1|sort|uniq -c|sort -nr|head -n 5 查看进程ps -ef|grep tomcat 查看端口情况netstat -pan|grep 10991234567//常用参数:-a 显示所有连接和监听的端口-b 显示包含于创建每个连接或监听端口的可执行组件。在某些情况下已知可执行组件-e 显示以太网统计信息。此选项可以与 -s选项组合使用。-n 以数字形式显示地址和端口号。-o 显示与每个连接相关的所属进程 ID。-p proto 显示 proto 指定的协议的连接；proto 可以是 修改密码passwd username 添加用户连接远程主机其他接口ssh root@ip -p port 查看系统版本信息uname -a filename.zip的解压unzip filename.zip 解压文件tar -zxvf filename.tar.gz12345678910111213141516171819//其中zxvf含义分别如下-c: 建立压缩档案-x：解压-t：查看内容-r：向压缩归档文件末尾追加文件-u：更新原压缩包中的文件//这五个是独立的命令，压缩解压都要用到其中一个，可以和别的命令连用但只能用其中一个。下面的参数是根据需要在压缩或解压档案时可选的。-z：有gzip属性的-j：有bz2属性的-Z：有compress属性的-v：显示所有过程-O：将文件解开到标准输出//下面的参数-f是必须的-f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。//事实上, 从1.15版本开始tar就可以自动识别压缩的格式,故不需人为区分压缩格式就能正确解压tar -xvf filename.tar.gztar -xvf filename.tar.bz2tar -xvf filename.tar.xztar -xvf filename.tar.Z 解压文件到指定文件夹tar -zxvf jdk-8u161-linux-i586.tar.gz -C java centos7配置环境配置java环境 先将jdk的压缩包上传到服务器scp jdk.tar.gz root@xx.xxx.xxx.xxx:/usr/local/java/ 然后使用tar -zxvf jdk.tar.gz解压 然后配置vim /etc/profile,添加以下内容到底部 1234JAVA_HOME=/usr/local/java/jdk PATH=$JAVA_HOME/bin:$PATH CLASSPATH=$JAVA_HOME/jre/lib/ext:$JAVA_HOME/lib/tools.jar export PATH JAVA_HOME CLASSPATH 然后source /etc/profile 查看所有端口netstat -ntlp 查看端口的开启情况netstat 查看指定端口的占用情况netstat -lnp|grep 8080 检测远程主机端口是否打开telnet 120.79.202.146 8080 centos7启动firewalld配置防火墙 确定打开firewalldsystemctl restart firewalld 查看firewalld状态,版本等firewall-cmd --state 查看已经打开的portfirewall-cmd --zone=dmz --list-ports 开启端口firewall-cmd --zone=public --add-port=1099/tcp --permanent 1234命令含义：–zone #作用域–add-port=80/tcp #添加端口，格式为：端口/通讯协议–permanent #永久生效，没有此参数重启后失效 重启防火墙 123firewall-cmd --reload #重启firewallsystemctl stop firewalld.service #停止firewallsystemctl disable firewalld.service #禁止firewall开机启动 注意阿里云服务器还要在控制中心添加端口规则 将idea web项目部署到远程tomcat,以及开机启动tomcat 配置tomcat/bin/catalina.sh文件,首行加入 123456789export CATALINA_OPTS="-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=1099 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Djava.rmi.server.hostname=10.82.82.248"export JAVA_OPTS="-Dcom.sun.management.jmxremote=-Dcom.sun.management.jmxremote.port=1099-Dcom.sun.management.jmxremote.ssl=false-Dcom.sun.management.jmxremote.authenticate=false" 启动tomcatsh ./catalina.sh run &gt; /dev/null 2&gt;&amp;1 &amp;其中&gt; /dev/null 2&gt;&amp;1 &amp;是Linux中的命令：把标准输出和出错处理都放到回收站，这样就免得一大堆输出占领你的屏幕。 在用jps命令： javahome: /usr/lib/jvm/java-8-oracle 设置tomcat开机启动,将startup.sh修改 12345export JAVA_HOME=/usr/local/java/jdk1.8export CLASSPATH=$CLASSPATH:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:. export PATH=$JAVA_HOME/bin:$PATH export CATALINA_HOME=/usr/local/tomcat /usr/local/tomcat/bin/catalina.sh start 在/etc/rc.d/rc.local中加入/usr/local/tomcat/bin/startup.sh docker开机自动启动systemctl enable docker.service 使用iptables管理防火墙vi /etc/sysconfig/iptables配置端口service iptables restart重启防火墙systemctl restart iptables.service重启防火墙/etc/init.d/iptables restart重启防火墙systemctl enable iptables.service设置防火墙卡机启动 tomcat配置idea远程debug centos环境,在startup.sh首行输入 1declare -x CATALINA_OPTS="-server -Xdebug -Xnoagent -Djava.compiler=NONE -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=5005" 在Ubuntu环境中,startup.sh首行输入 1CATALINA_OPTS="-server -Xdebug -Xnoagent -Djava.compiler=NONE -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=5005" idea远程部署 配置tomcat/bin/catalina.sh文件 12345678910export CATALINA_OPTS="-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=1099 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Djava.rmi.server.hostname=10.82.82.248"export JAVA_OPTS="-Dcom.sun.management.jmxremote=-Dcom.sun.management.jmxremote.port=1099-Dcom.sun.management.jmxremote.ssl=false-Dcom.sun.management.jmxremote.authenticate=false" 启动tomcat 1./catalina.sh run &gt; /dev/null 2&gt;&amp;1 &amp; 其中“ &gt; /dev/null 2&gt;&amp;1 &amp;”是Linux中的命令：把标准输出和出错处理都放到回收站，这样就免得一大堆输出占领你的屏幕。 在用jps命令： javahome: /usr/lib/jvm/java-8-oracle 设置tomcat开机启动,将startup.sh修改 12345export JAVA_HOME=/usr/lib/jvm/java-8-oracle export CLASSPATH=$CLASSPATH:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:. export PATH=$JAVA_HOME/bin:$PATH export CATALINA_HOME=/usr/local/tomcat /usr/local/tomcat/bin/catalina.sh start 在/etc/rc.d/rc.local中加入/usr/local/tomcat/bin/startup.sh]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库]]></title>
    <url>%2F2018%2F07%2F07%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[数据库相关知识 分库分表分库分表 读写分离 参考地址:https://blog.csdn.net/justdb/article/details/17331569 概念 what读写分离，基本的原理是让主数据库处理事务性增、改、删操作（INSERT、UPDATE、DELETE），而从数据库处理SELECT查询操作。数据库复制被用来把事务性操作导致的变更同步到集群中的从数据库。 why所以读写分离，解决的是，数据库的写入，影响了查询的效率。 when数据库不一定要读写分离，如果程序使用数据库较多时，而更新少，查询多的情况下会考虑使用，利用数据库 主从同步 。可以减少数据库压力，提高性能。当然，数据库也有其它优化方案。memcache 或是 表折分，或是搜索引擎。都是解决方法。 提高性能的原因 物理服务器增加，负荷增加 主从只负责各自的写和读，极大程度的缓解X锁和S锁争用 从库可配置myisam引擎，提升查询性能以及节约系统开销 从库同步主库的数据和主库直接写还是有区别的，通过主库发送来的binlog恢复数据，但是，最重要区别在于主库向从库发送binlog是异步的，从库恢复数据也是异步的 读写分离适用与读远大于写的场景，如果只有一台服务器，当select很多时，update和delete会被这些select访问中的数据堵塞，等待select结束，并发性能不高。 对于写和读比例相近的应用，应该部署双主相互复制 可以在从库启动是增加一些参数来提高其读的性能，例如–skip-innodb、–skip-bdb、–low-priority-updates以及–delay-key-write=ALL。当然这些设置也是需要根据具体业务需求来定得，不一定能用上 分摊读取。假如我们有1主3从，不考虑上述1中提到的从库单方面设置，假设现在1分钟内有10条写入，150条读取。那么，1主3从相当于共计40条写入，而读取总数没变，因此平均下来每台服务器承担了10条写入和50条读取（主库不承担读取操作）。因此，虽然写入没变，但是读取大大分摊了，提高了系统性能。另外，当读取被分摊后，又间接提高了写入的性能。所以，总体性能提高了，说白了就是拿机器和带宽换性能。MySQL官方文档中有相关演算公式：官方文档 见6.9FAQ之“MySQL复制能够何时和多大程度提高系统性能” MySQL复制另外一大功能是增加冗余，提高可用性，当一台数据库服务器宕机后能通过调整另外一台从库来以最快的速度恢复服务，因此不能光看性能，也就是说1主1从也是可以的。 Mysql主从同步方案慢查询优化:分析sql方法参考地址:https://blog.csdn.net/u012990533/article/details/45643509explain select * from table where table.id = 1运行上面的sql语句后你会看到，下面的表头信息：table | type | possible_keys | key | key_len | ref | rows | Extra table显示这一行的数据是关于哪张表的 type这是重要的列，显示连接使用了何种类型。从最好到最差的连接类型为const、eq_reg、ref、range、indexhe和ALL说明：不同连接类型的解释（按照效率高低的顺序排序） system：表只有一行：system表。这是const连接类型的特殊情况。 const ：表中的一个记录的最大值能够匹配这个查询（索引可以是主键或惟一索引）。因为只有一行，这个值实际就是常数，因为MYSQL先读这个值然后把它当做常数来对待。 eq_ref：在连接中，MYSQL在查询时，从前面的表中，对每一个记录的联合都从表中读取一个记录，它在查询使用了索引为主键或惟一键的全部时使用。 ref：这个连接类型只有在查询使用了不是惟一或主键的键或者是这些类型的部分（比如，利用最左边前缀）时发生。对于之前的表的每一个行联合，全部记录都将从表中读出。这个类型严重依赖于根据索引匹配的记录多少—越少越好。 range：这个连接类型使用索引返回一个范围中的行，比如使用&gt;或&lt;查找东西时发生的情况。 index：这个连接类型对前面的表中的每一个记录联合进行完全扫描（比ALL更好，因为索引一般小于表数据）。 ALL：这个连接类型对于前面的每一个记录联合进行完全扫描，这一般比较糟糕，应该尽量避免。 possible_keys显示可能应用在这张表中的索引。如果为空，没有可能的索引。可以为相关的域从WHERE语句中选择一个合适的语句 key实际使用的索引。如果为NULL，则没有使用索引。很少的情况下，MYSQL会选择优化不足的索引。这种情况下，可以在SELECT语句中使用USE INDEX（indexname）来强制使用一个索引或者用IGNORE INDEX（indexname）来强制MYSQL忽略索引 key_len使用的索引的长度。在不损失精确性的情况下，长度越短越好 ref显示索引的哪一列被使用了，如果可能的话，是一个常数 rowsMYSQL认为必须检查的用来返回请求数据的行数 Extra关于MYSQL如何解析查询的额外信息。将在表4.3中讨论，但这里可以看到的坏的例子是Using temporary和Using filesort，意思MYSQL根本不能使用索引，结果是检索会很慢说明：extra列返回的描述的意义 Distinct ：一旦mysql找到了与行相联合匹配的行，就不再搜索了。 Not exists ：mysql优化了LEFT JOIN，一旦它找到了匹配LEFT JOIN标准的行，就不再搜索了。 Range checked for each Record（index map:#） ：没有找到理想的索引，因此对从前面表中来的每一个行组合，mysql检查使用哪个索引，并用它来从表中返回行。这是使用索引的最慢的连接之一。 Using filesort ：看到这个的时候，查询就需要优化了。mysql需要进行额外的步骤来发现如何对返回的行排序。它根据连接类型以及存储排序键值和匹配条件的全部行的行指针来排序全部行。 Using index ：列数据是从仅仅使用了索引中的信息而没有读取实际的行动的表返回的，这发生在对表的全部的请求列都是同一个索引的部分的时候。 Using temporary ：看到这个的时候，查询需要优化了。这里，mysql需要创建一个临时表来存储结果，这通常发生在对不同的列集进行ORDER BY上，而不是GROUP BY上。 Where used ：使用了WHERE从句来限制哪些行将与下一张表匹配或者是返回给用户。如果不想返回表中的全部行，并且连接类型ALL或index，这就会发生，或者是查询有问题。 优化方法参考地址:https://tech.meituan.com/mysql-index.html 先运行看看是否真的很慢，注意设置SQL_NO_CACHE where条件单表查，锁定最小返回记录表。这句话的意思是把查询语句的where都应用到表中返回的记录数最小的表开始查起，单表每个字段分别查询，看哪个字段的区分度最高 explain查看执行计划，是否与1预期一致（从锁定记录较少的表开始查询）比如explain select * from test1 where id=1; order by limit 形式的sql语句让排序的表优先查 了解业务方使用场景 加索引时参照建索引的几大原则 观察结果，不符合预期继续从0分析 完整的对象访问名称Oracle参考地址:https://blog.csdn.net/KimSoft/article/details/4627520]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>优化</tag>
        <tag>读写分离</tag>
        <tag>主备方案</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开源框架]]></title>
    <url>%2F2018%2F07%2F07%2F%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[开源框架学习 kafkakafka入门 kafka启动参数 调整JVM堆大小kafka-server-start.shexport KAFKA_HEAP_OPTS=&quot;-Xmx256M -Xms128M&quot; SpringSpringMVC 分布式框架Redis持久化原理kafka消息队列MyBatis数据关联动态映射事务关联log4jweb.xml中的配置12345&lt;!--配置log4j--&gt;&lt;context-param&gt; &lt;param-name&gt;log4jConfigLocation&lt;/param-name&gt; &lt;param-value&gt;/WEB-INF/log4j.properties&lt;/param-value&gt;&lt;/context-param&gt; 代码中的使用123456789private static final String LOG_NAME = "hn_log";private static final String LOG_FORMATTER = "%s : %s.%s - %s";private static final String LOG_TAG = "hnexia";Logger logger = Logger.getLogger(LOG_NAME);...String result = ret.toString();String location = "in logAspect =======================================================================================================";logger.info(String.format(LOG_FORMATTER, location + LOG_TAG, className, methodName, result)); log4j.properties1234567891011121314151617181920212223242526272829303132333435### set log levels ###log4j.rootLogger = debug,stdout,D,I,E# 控制台输出的配置，所有日志输出，都会显示在控制台log4j.appender.stdout = org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.Target = System.outlog4j.appender.stdout.layout = org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern = [%-5p] %d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; method:%l%n%m%n# DEBUG级别及以上级别的日志，会写到F://logs/log.log文件中，文件不存在的时候会自动创建#log4j.appender.D = org.apache.log4j.DailyRollingFileAppender#log4j.appender.D.File = /usr/local/tomcat/webapps/art-education-o2o/logs/debug.log#log4j.appender.D.Append = true#log4j.appender.D.Threshold = DEBUG#log4j.appender.D.layout = org.apache.log4j.PatternLayout#log4j.appender.D.layout.ConversionPattern = %-d&#123;yyyy-MM-dd HH:mm:ss&#125; [%p] %m%n# INFO，会写到F://logs/log.log文件中，文件不存在的时候会自动创建log4j.appender.I = org.apache.log4j.DailyRollingFileAppenderlog4j.appender.I.File = /usr/local/tomcat/webapps/art-education-o2o/logs/info.loglog4j.appender.I.Append = truelog4j.appender.I.Threshold = INFOlog4j.appender.I.layout = org.apache.log4j.PatternLayoutlog4j.appender.I.layout.ConversionPattern = %-d&#123;yyyy-MM-dd HH:mm:ss&#125; [%p] %m%n# ERROR级别及以上级别的日志，会写到F://logs/error.log文件中，文件不存在的时候会自动创建log4j.appender.E = org.apache.log4j.DailyRollingFileAppenderlog4j.appender.E.File =/usr/local/tomcat/webapps/art-education-o2o/logs/error.loglog4j.appender.E.Append = truelog4j.appender.E.Threshold = ERRORlog4j.appender.E.layout = org.apache.log4j.PatternLayoutlog4j.appender.E.layout.ConversionPattern = %-d&#123;yyyy-MM-dd HH:mm:ss&#125; [%p] %m%n# 指定com.neusoft包下的所有类的等级为DEBUG。可以把com.neusoft改为自己项目所用的包名。log4j.logger.com.neusoft =DEBUG]]></content>
      <categories>
        <category>框架</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>kafka</tag>
        <tag>springMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目实践积累]]></title>
    <url>%2F2018%2F07%2F07%2F%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5%E7%A7%AF%E7%B4%AF%2F</url>
    <content type="text"><![CDATA[做过的项目,实际问题的解决思路 实际问题订单异常检测电商平台每日订单异常检测,服务端代码思路 网易邮箱抽奖网易邮箱活动,3.4亿用户,只允许8千万人参加,最后在八千万人中产生一二三等奖,有一台物理机和一台数据库服务器,怎么设计 ##]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础知识]]></title>
    <url>%2F2018%2F07%2F07%2FJava%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[Java基础知识 java基础 +++面向对象常用类及数据结构 Java常用的数据结构中，请描述Vector, ArrayList, LinkedList的不同场景下的性能差别 不考虑同步的情况下 随机访问数据,ArrayList对象要远优于LinkedList对象； 进行插入或者删除操作，LinkedList对象要远优于ArrayList对象； 考虑同步的时候只能使用Vector,而Vector和ArrayList相比增加了同步操作,性能上比不上ArrayList Object ==和equals的区别在Object类中equals()方法其实等效于==,比较的是两个对象的内存地址是否相等123public boolean equals(Object obj) &#123; return (this == obj);&#125; 如果重写equals()方法,那么它和==就不一定等效了 hashCode()和equals() hashCode的存在主要是用于查找的快捷性，如Hashtable，HashMap等，hashCode是用来在散列存储结构中确定对象的存储地址的； 如果两个对象相同，就是适用于equals(java.lang.Object) 方法，那么这两个对象的hashCode一定要相同； 如果对象的equals方法被重写，那么对象的hashCode也尽量重写，并且产生hashCode使用的对象，一定要和equals方法中使用的一致，否则就会违反上面提到的第2点； 两个对象的hashCode相同，并不一定表示两个对象就相同，也就是不一定适用于equals(java.lang.Object) 方法，只能够说明这两个对象在散列存储结构中，如Hashtable，他们“存放在同一个篮子里”。总结一下:hashCode()方法就是用于决定元素存放在结构中(如HashMap等)的位置,然后方便查找的,而在hash值冲突的时候,就需要用到equals()方法来判断到底要找的是哪一个.所以重写了equals()方法,一般还要重写hashCode()方法, String,StringBuilder和StringBuffer String对象的每次赋值都相当于重新生成一个新对象,所以String对象频繁变化会导致内存中无引用的字符串多了以后,会引起JVM的GC StringBuilder底层是基于char数组实现的,append()方法基于System.arraycopy()方法实现 StringBuffer就是线程安全的StringBuilder AVL树十分严格的平衡二叉搜索树,树中任何一个节点的两棵子树高度差不超过112345678910public class AVLTree&lt;T extends Comparable&lt;T&gt;&gt; &#123; private AVLTreeNode&lt;T&gt; mRoot; // 根结点 // AVL树的节点(内部类) class AVLTreeNode&lt;T extends Comparable&lt;T&gt;&gt; &#123; T key; // 关键字(键值) int height; // 高度 AVLTreeNode&lt;T&gt; left; // 左孩子 AVLTreeNode&lt;T&gt; right; // 右孩子 ......&#125; 大根堆 集合 哈希算法基于哈希算法在信息安全中主要应用在 文件校验 数字签名 鉴权协议 一致性Hash算法参考地址:http://blog.csdn.net/cywosp/article/details/23397179一致性hash算法提出了在动态变化的Cache环境中，判定哈希算法好坏的四个定义: 平衡性 单调性 分散性 负载 异常 反射访问私有变量和私有方法 调用类的getDeclaredFields()和getDeclaredMethods()方法获得相应的field和method 调用method和field的setAccessible()方法允许访问私有成员属性 field调用get(实例)方法,获得对应field的值 method调用invoke()方法执行私有方法 泛型注意点 泛型方法和泛型类的&lt;&gt;互不相干 泛型擦除如果泛型参数没有指定上限(如),那类型擦除之后T就被替换成Object,否则就是上限,如String 类型擦除也是为什么泛型类和泛型方法中不接受基本数据类型的原因 IO/NIONIO的使用 简介参考地址:https://blog.csdn.net/shimiso/article/details/24990499同步和异步说的是消息的通知机制,阻塞和非阻塞说的是线程的状态 同步阻塞io服务器实现是一个连接创建一个线程,直到连接断开这个线程都被这个连接占用指的就是传统的Java IO,线程发起io后会阻塞,等到io结束之后才会恢复同步阻塞时序图 同步非阻塞io服务器实现是一个请求创建一个线程,或者说多个连接对应一个线程,由于线程数量小于连接数量,所以IO操作不能够阻塞Java中的NIO(Non-blocking IO)是这种模型,线程发起io后可以去干其他事情,但是另外起一个线程去监听或者轮询io状态(是否有可读数据,或者是否可以写入);传统io中,比如read()操作,当没有数据可读的时候线程就会阻塞;但是在NIO中,没有数据可读的时候read()就会返回立即0 有一个专门的线程来处理所有的IO事件,并负责分发 事件触发机制:事件到来的时候触发,而不是同步监视事件 线程通讯：线程之间通过 wait,notify 等方式通讯。保证每次上下文切换都是有意义的。减少无谓的线程切换。 异步非阻塞io服务器实现是一个有效请求创建一个线程Java中的AIO((Asynchronous IO))是这种模型,线程发起io后可以直接干其他事前,系统会监听io状态,IO完成后会通知线程 文件 文件是怎么在磁盘上存储的 Jdk8特性 +++ lambda表达式函数式编程streamFP并发处理高并发 举例子,代码实现 多线程 +++线程 线程池 CyclicBarrierJ.U.Cfork/join设计模式 Servlet容器 ++ 什么是servlet容器 用户想web server请求的只能是静态页面,如果web server想要返回动态页面,就需要servlet来产生.所谓的servlet其实就是Java程序,servlet容器就是装这些java程序的,负责servlet程序的创建,执行和销毁,因此javax.servlet中的Servlet接口定义了是哪个方法: init()在Servlet生命周期初始化阶段调用,传递一个实现了javax.servlet.ServletConfig接口的对象以获取web application中的初始化参数 service()接收一个请求都会调用一次Service()方法,每个请求的处理都在独立的线程中进行,Service()方法判断请求的类型并转发给相应的Servlet进行处理 destory() webserver和Servlet容器处理一个请求的过程 Web服务器接收到HTTP请求 Web服务器将请求转发给servlet容器 如果容器中不存在所需的servlet，容器就会检索servlet，并将其加载到容器的地址空间中 容器调用servlet的init()方法对servlet进行初始化（该方法只会在servlet第一次被载入时调用） 容器调用servlet的service()方法来处理HTTP请求，即，读取请求中的数据，创建一个响应。servlet会被保留在容器的地址空间中，继续处理其他的HTTP请求 Web服务器将动态生成的结果返回到正确的地址。 tomcattomcat Servlet 是不是线程安全的:单例模式,当客户端第一次请求该servlet的时候会根据web.xml实例化servlet,之后客户端再访问该servlet使用的就是同一实例,是不是线程安全看servlet的实现,如果servlet的内部属性会被多线程改变,那就是不安全的. jetty调优技巧 ++实践积累坑(容易犯错的细节)细节12345678910111213141516171819&lt;!-- 代码段 --&gt;int i = 0;i = i++;&lt;!-- 结果i还是0 --&gt;/*代码段*/int i = 0;i = ++ i;&lt;!-- 结果i是1 --&gt;&lt;!-- 基本数值类型转换问题 --&gt;int a = 9;int b = 4;double c = a/b;&lt;!-- 结果是2.0 --&gt;&lt;!-- 控制sout的小数输出位数 --&gt;sout("%.2f",doubleNum);&lt;!-- 结果输出的double数字会保留两位小数 --&gt; 值调用和引用调用关于java中的值调用和引用调用(java核心技术p120)引用调用是引用地址的拷贝,虽然能够实际影响到引用对象的状态,但是一个方法无法让一个对象引用一个新的对象,比如:12345public static void swap(Person p1,Person p2)&#123; Person temp = p1; p1 = p2; p2 = temp;&#125; 以上代码是无法实现交换功能的. ##关于new对象的位置很有讲究,如果在循环的外面new的话,相当于add进list中的都是同一个对象123456789101112131415161718List&lt;String[]&gt; list = new ArrayList&lt;&gt;(); //关于课程信息的一行数据 //关于课程的所有字段 String[] fieldNames = &#123;DbHelper.COURSE_NAME, DbHelper.START_DATE, DbHelper.END_DATE, DbHelper.DAY_OF_WEEK, DbHelper.START_TIME, DbHelper.END_TIME, DbHelper.TEACHER_NAME, DbHelper.ADDRESS,DbHelper.COURSE_ID&#125;; //从数据库取出的一行数据 String[] fieldValues ;//千万不能在这里new ,不然后面add进list的全是这个地址= new String[fieldNames.length]; Cursor cursor = db.query(DbHelper.TABLE_NAME_COURSE, null, null, null, null, null, null); if (cursor.moveToFirst()) &#123; do &#123; fieldValues = new String[fieldNames.length]; for(int i = 0; i &lt; fieldNames.length; i ++) &#123; fieldValues[i] = cursor.getString(cursor.getColumnIndex(fieldNames[i])); &#125; Log.d("hn", fieldValues.toString()); System.out.println(fieldValues.toString()); list.add(fieldValues); &#125; while (cursor.moveToNext()); &#125; cursor.close();]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker常用命令]]></title>
    <url>%2F2018%2F07%2F07%2Fdocker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[docker常用命令 部署tomcat 搜索tomcat镜像docker search tomcat 获取镜像docker pull tomcat 启动容器docker run --name tomcat8091 -p 8092:8080 -v $PWD/temp:/usr/local/tomcat/temp -d docker.io/tomcat 访问端口号对应的ip,验证服务是否启动 部署redis 搜索拉取 启动容器docker run --name redis6380 -p 6380:6379 -v $PWD/data:/data -v $PWD/data/redis.config:/data/redis.config -d docker.io/redis redis-server /data/redis.config参数说明:redis-server redis.config:在容器执行redis-server启动命令，以redis.config为配置文件,最好通过自定义的配置文件启动,因为默认配置文件是找不到的,无法修改 进入容器查看redis状态docker exec -it redis redis-cli 关于容器的使用查看docker日志docker logs container_name 创建并启动容器docker run --name tomcat_test -p 8090:8080 -v $PWD/webapps:/usr/local/tomcat/webapps -d tomcat参数说明:-name 为容器命名-p 8090:8080：将宿主机的8090端口映射到容器的8080端口(冒号后面的是容器的端口)-v $PWD/test:/usr/local/tomcat/webapps/test：将主机中当前目录下的test挂载到容器的/test 修改已经创建的容器的端口映射 参考地址:https://blog.csdn.net/wesleyflagon/article/details/78961990 启动容器docker start containerID 停止所有容器docker stop $(docker ps -q) 删除容器docker rm determined_swanson 删除镜像(删除镜像之前要先删除镜像的容器)docker rmi image_name 通过push备份容器12docker commit -p container_name container-backupdocker login username:mending 12docker tag container_name mending/container-backupdocker push mending/container-backup 通过save保存到本地docker save -o ~/container-backup.tar container-backup 通过主机命令行进入master容器123docker exec -it mysql_test bash[root@bogon ~]# docker exec -it mysql-master bashroot@1651d1cab219:/# 容器开机自动启动方法 如果容器没有创建sudo docker run --restart=always ..... 如果容器已经创建docker update --restart=always container_name no – 容器退出时不要自动重启。这个是默认值。 on-failure[:max-retries] – 只在容器以非0状态码退出时重启。可选的，可以退出docker daemon尝试重启容器的次数。 always – 不管退出状态码是什么始终重启容器。当指定always时，docker daemon将无限次数地重启容器。容器也会在daemon启动时尝试重启，不管容器当时的状态如何。 unless-stopped – 不管退出状态码是什么始终重启容器，不过当daemon启动时，如果容器之前已经为停止状态，不要尝试启动它。 创建mysql容器sudo docker run -d -p 3306:3306 --name mysql_test -e MYSQL_ROOT_PASSWORD=root -v /mysql/datadir:/var/lib/mysql -v /mysql/conf:/etc/mysql/conf.d docker.io/mysql:latest参数说明: -e MYSQL_ROOT_PASSWORD=password ：指定root密码-v /mysql/datadir:/var/lib/mysql ：指定数据库本地存储路径，如果系统没有关闭SELinux，会启动失败，原因是本地目录不允许挂载到容器，需要先执行chcon -Rt svirt_sandbox_file_t /mysql/datadir-v /mysql/conf:/etc/mysql/conf.d ：指定使用自定义的mysql配置文件启动数据库，比如在该路径下创建一个my-config.cnf docker中配置Oracle 创建Oracle容器docker run --name oracle_test -d -p 49160:22 -p 49191:1521 -e ORACLE_ALLOW_REMOTE=true daocloud.io/ihypo/oracle-xe-11g 进入oracle容器docker exec -it 806ebe7f5231 /bin/bash ssh进入oracle容器ssh root@localhost -p 49160 容器中登录Oracle数据库su - oracle sqlplus system/oracle 创建用户create user HZYXY_BI identified by HZYXY_BI;说明:by 后面是密码 授予权限grant resource,connect to HZYXY_BI; 切换用户登录conn HZYXY_BI/HZYXY_BI; 容器中存放数据库文件的目录/u01/app/oracle/oradata/XE/create user shyhex identified by shyhex default tablespace shyhex]]></content>
      <categories>
        <category>运维相关</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
</search>
